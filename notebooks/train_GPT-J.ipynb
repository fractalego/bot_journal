{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import custom_fwd, custom_bwd\n",
    "\n",
    "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenBNBLinear(nn.Module):\n",
    "    def __init__(self, weight, absmax, code, bias=None):\n",
    "        assert isinstance(bias, nn.Parameter) or bias is None\n",
    "        super().__init__()\n",
    "        self.out_features, self.in_features = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    "        self.bias = bias\n",
    " \n",
    "    def forward(self, input):\n",
    "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output\n",
    " \n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
    "        return cls(weights_int8, *state, linear.bias)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
    " \n",
    " \n",
    "class DequantizeAndLinear(torch.autograd.Function): \n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
    "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
    "        ctx._has_bias = bias is not None\n",
    "        return F.linear(input, weights_deq, bias)\n",
    " \n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
    "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
    "        # grad_output: [*batch, out_features]\n",
    "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
    "        grad_input = grad_output @ weights_deq\n",
    "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
    "        return grad_input, None, None, None, grad_bias\n",
    " \n",
    " \n",
    "class FrozenBNBEmbedding(nn.Module):\n",
    "    def __init__(self, weight, absmax, code):\n",
    "        super().__init__()\n",
    "        self.num_embeddings, self.embedding_dim = weight.shape\n",
    "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
    "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
    "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
    "        self.adapter = None\n",
    " \n",
    "    def forward(self, input, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            # note: both quantuized weights and input indices are *not* differentiable\n",
    "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
    "            output = F.embedding(input, weight_deq, **kwargs)\n",
    "        if self.adapter:\n",
    "            output += self.adapter(input)\n",
    "        return output \n",
    " \n",
    "    @classmethod\n",
    "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
    "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
    "        return cls(weights_int8, *state)\n",
    " \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
    " \n",
    " \n",
    "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
    "    assert chunk_size % 4096 == 0\n",
    "    code = None\n",
    "    chunks = []\n",
    "    absmaxes = []\n",
    "    flat_tensor = matrix.view(-1)\n",
    "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
    "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
    "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
    "        chunks.append(quantized_chunk)\n",
    "        absmaxes.append(absmax_chunk)\n",
    " \n",
    "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
    "    absmax = torch.cat(absmaxes)\n",
    "    return matrix_i8, (absmax, code)\n",
    " \n",
    " \n",
    "def convert_to_int8(model):\n",
    "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
    "    for module in list(model.modules()):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                print(name, child)\n",
    "                setattr( \n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBLinear(\n",
    "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                        bias=child.bias,\n",
    "                    ),\n",
    "                )\n",
    "            elif isinstance(child, nn.Embedding):\n",
    "                setattr(\n",
    "                    module,\n",
    "                    name,\n",
    "                    FrozenBNBEmbedding(\n",
    "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
    "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
    "                        code=torch.zeros(256),\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        convert_to_int8(self.attn)\n",
    "        convert_to_int8(self.mlp)\n",
    "\n",
    "\n",
    "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "        \n",
    "\n",
    "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        convert_to_int8(self)\n",
    "\n",
    "\n",
    "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdd2ccb868e481596abbd375ffc796c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77ac062a25f4cd991507001ea391498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
      "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
      "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
     ]
    }
   ],
   "source": [
    "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "_ = gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861408"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "_ = gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A garbled conversation is transcribed as follows:\n",
      "Left: THIS IS MIKE FROM QB EINSURANCE HOW CAN I HELP YOU\n",
      "Right: NOT TO BAD NOT TO BAD HOW ARE YOU\n",
      "Left: ALL IS GOOD DO YOU HAVE A POLICY NUMBER\n",
      "\n",
      "Please rewrite the conversation in lower case:\n",
      "\n",
      "THIS IS MIKE FROM QB EINSURANCE HOW CAN I HELP YOU\n",
      "NOT TO BAD NOT TO BAD HOW ARE YOU\n",
      "ALL IS GOOD DO YOU HAVE A POLICY NUMBER\n",
      "\n",
      "The following is a garbled conversation:\n",
      "CPU times: user 8.66 s, sys: 15.6 ms, total: 8.67 s\n",
      "Wall time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = f\"\"\"\n",
    "A garbled conversation is transcribed as follows:\n",
    "Left: THIS IS MIKE FROM QB EINSURANCE HOW CAN I HELP YOU\n",
    "Right: NOT TO BAD NOT TO BAD HOW ARE YOU\n",
    "Left: ALL IS GOOD DO YOU HAVE A POLICY NUMBER\n",
    "\n",
    "Please correct the conversation and write it below, in lower case and adding punctuation:\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt')\n",
    "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "out = gpt.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 50, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person is trying to decide if a statement makes sense. \n",
      "\n",
      "Prior examples are:\n",
      "* Does \"skyscrapers are part of shopping list\" make sense? No.\n",
      "* Does \"batteries go well with fish\" make sense? No.\n",
      "* Does \"Pasta is an Italian dish\" make sense? Yes.\n",
      "* Does \"The earth is flat\" make sense? No.\n",
      "\n",
      "\n",
      "Using the examples above, does \"The earth is flat\" make sense?\n",
      "\n",
      "Yes.\n",
      "\n",
      "CPU times: user 1 s, sys: 0 ns, total: 1 s\n",
      "Wall time: 1e+03 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "statement = \"The earth is flat\"\n",
    "text = f\"\"\"\n",
    "A person is trying to decide if a statement makes sense. \n",
    "\n",
    "Prior examples are:\n",
    "* Does \"skyscrapers are part of shopping list\" make sense? No.\n",
    "* Does \"batteries go well with fish\" make sense? No.\n",
    "* Does \"Pasta is an Italian dish\" make sense? Yes.\n",
    "* Does \"The earth is flat\" make sense? No.\n",
    "\n",
    "\n",
    "Using the examples above, does \"{statement}\" make sense?\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = tokenizer(text, return_tensors='pt')\n",
    "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
    "out = gpt.generate(**prompt, max_length=prompt['input_ids'].shape[1] + 5, do_sample=False)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list = json.load(open('../data/creak_dev_list.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    for item in tqdm(data):\n",
    "        expected = get_answer_from_text(item)\n",
    "        predicted = ''\n",
    "        try:\n",
    "            predicted = generate_answer(model, item)\n",
    "        except (IndexError, RuntimeError) as e:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if expected == predicted:\n",
    "            tp += 1\n",
    "        if expected == 'N' and predicted == 'Y':\n",
    "            fp += 1\n",
    "        if expected == 'Y' and predicted == 'N':\n",
    "            fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1:', f1)\n",
    "    print('Skipped:', skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question(text):\n",
    "    _claim_yn = 'The claim makes sense:\\n'\n",
    "    return text[:text.find(_claim_yn) + len(_claim_yn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question_for_generation(text):\n",
    "    _claim_yn = 'The claim makes sense:\\n'\n",
    "    _new_claim_yn = 'Does the claim make sense:\\n'\n",
    "    text = text.replace(_claim_yn, _new_claim_yn)\n",
    "    pos = text.find(_new_claim_yn) + len(_new_claim_yn)\n",
    "    return text[:pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The claim makes sense:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text_for_generation(text):\n",
    "    _claim_yn = 'The claim makes sense:\\n'\n",
    "    _new_claim_yn = 'Does the claim make sense:\\n'\n",
    "    text = text.replace(_claim_yn, _new_claim_yn)\n",
    "    pos = text.find(_new_claim_yn) + len(_new_claim_yn)\n",
    "    return text[pos + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, text):\n",
    "    prompt = get_text_up_to_question_for_generation(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 2\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_from_text_for_generation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_answer(model, text):\n",
    "    prompt = get_text_up_to_question_for_generation(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 20\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    score = model(output, labels=output)[0]\n",
    "    out_text = tokenizer.decode(output[0][tokens_length:], skip_special_tokens=True)\n",
    "\n",
    "    return out_text, float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
       "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
       "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gpt\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The claim is:\n",
      "Peter Sellers spent his life as a single bachelor.\n",
      "\n",
      "The claim makes sense:\n",
      "Nope.\n",
      "\n",
      "Because:\n",
      "He was married four times with three children.\n"
     ]
    }
   ],
   "source": [
    "print(dev_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.59 s, sys: 0 ns, total: 3.59 s\n",
      "Wall time: 3.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"\\nYes, it does.\\n\\nNo, it doesn't.\\n\\nI think it does\", 2.367492914199829)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generate_full_answer(model, dev_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad266c0b9e344c0baef2ebebe2bd0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.47237880496054113\n",
      "Recall: 0.9976190476190476\n",
      "F1: 0.6411629686304514\n",
      "Skipped: 0\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"skyscrapers can be part of a shopping list\"\n",
    "\n",
    "text = f\"\"\"\n",
    "The claim is:\n",
    "{story}\n",
    "\n",
    "The claim makes sense:\n",
    "\"\"\"[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = '\"please add pasta\" is more of a statement than an item of a shopping list.'\n",
    "\n",
    "text = f\"\"\"\n",
    "The claim is:\n",
    "{story}\n",
    "\n",
    "The claim makes sense:\n",
    "\"\"\"[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The claim is:\n",
      "skyscrapers can be part of a shopping list\n",
      "\n",
      "The claim makes sense:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yes, skyscrapers can be part of a shopping list.\n",
      "\n",
      "The claim is:\n",
      "skyscrapers can be part of a shopping list\n",
      "\n",
      "The claim makes sense (yes or no):\n",
      "\n",
      "Yes, skyscrapers can be part of a shopping list.\n",
      "\n",
      "The claim is:\n",
      "skyscrapers can\n"
     ]
    }
   ],
   "source": [
    "print(generate_full_answer(model, text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
