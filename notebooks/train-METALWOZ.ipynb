{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/roles.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed 0 out of 42066\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_trimmed = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "        total_trimmed += 1\n",
    "    data.append(tokens)\n",
    "print(f'Trimmed {total_trimmed} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs, labels=inputs)[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 42066/42066 [32:03<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.7470427921034496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 42066/42066 [31:16<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.6445429053407125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 42066/42066 [31:16<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.6045077501759035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 42066/42066 [31:15<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.5830690292268248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 42066/42066 [31:16<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 0.5589288786221563\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(5):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'rules_small' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('rules_small' + str(4))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt, num):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    generated_ids = model.generate(tokens.to(\"cuda\"), max_length=_length, num_beams=num, num_return_sequences=num)\n",
    "    generated_sentences = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sentences = []\n",
    "    for index, item in enumerate(generated_sentences):\n",
    "        output = generated_sentences[index]\n",
    "        offset = len(prompt)\n",
    "        start = offset + 1\n",
    "        end = output.find('\\n', start)\n",
    "        sentences.append(item[start: end])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text(sentence):\n",
    "    return f\"\"\"\n",
    "The following utterance:\n",
    "{sentence}\n",
    "\n",
    "defines this type of role:\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['You are a bot that agrees with everything the user says.',\n",
       " 'You are a bot designed to agree with everything the user says, but you are not designed to agree with everything the user says.',\n",
       " 'It is a bot designed to agree with everything the user says.',\n",
       " 'You are a bot designed to agree with what the user says.',\n",
       " 'You are a bot designed to agree with everything the user says.',\n",
       " 'A bot designed to agree with everything the user says.',\n",
       " 'You are a bot designed to agree with what the user says.',\n",
       " 'You are a bot designed to agree to everything the user says.',\n",
       " 'You are a bot that agrees with everything the user says.',\n",
       " 'You are a bot that agrees with everything the user says.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = create_text(\"The user says 'Oh'\")\n",
    "generate_answer(model, prompt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 ms, sys: 13.7 ms, total: 39 ms\n",
      "Wall time: 38.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'George.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "The user says: \"My ship is called George\".\n",
    "\n",
    "Discussion:\n",
    "Q: Who is speaking?\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "generate_answer_with_typical_decoding(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Waggard.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "The Wag speaks: \"My ship is called George\".\n",
    "\n",
    "Discussion:\n",
    "Q: Who is speaking?\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "generate_answer_with_typical_decoding(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 44])\n",
      "41\n",
      "torch.Size([1, 3])\n",
      "CPU times: user 4.47 s, sys: 0 ns, total: 4.47 s\n",
      "Wall time: 4.47 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The Wag', 1.748390108346939)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "generate_answer_and_get_confidence(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_greedy(model, prompt, max_length= 50):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + max_length > 1024:\n",
    "        return ''\n",
    "    \n",
    "    while tokens.shape[-1] < tokens_length + max_length:\n",
    "        new_tokens = model(tokens.cuda())\n",
    "        pred_ids = torch.argmax(new_tokens.logits, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "        last_output = tokenizer.decode(last_token, skip_special_tokens=True)\n",
    "        if last_output == '\\n':\n",
    "            break\n",
    "        \n",
    "    generated_output = tokens[:, tokens_length:]\n",
    "    output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    end = output.find('\\n')\n",
    "    return output[:end].replace('A: ', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 ms, sys: 0 ns, total: 22.2 ms\n",
      "Wall time: 22 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Wag'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "generate_answer_greedy(model, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute prob of specific sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_probability_given_prompt(model, prompt, sequence):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    sequence_tokens = tokenizer.encode(sequence, return_tensors='pt')[0]\n",
    "    token_index = 0\n",
    "    total_prob = 1\n",
    "    while token_index < len(sequence_tokens):\n",
    "        new_tokens = model(tokens.cuda())\n",
    "        last_distribution = new_tokens.logits[:, -1].cpu().detach()\n",
    "        probs = torch.nn.functional.softmax(last_distribution)\n",
    "        total_prob *= probs[0][sequence_tokens[token_index]]\n",
    "        token_index += 1\n",
    "        \n",
    "        pred_ids = torch.argmax(new_tokens.logits, dim=-1)\n",
    "        last_token = pred_ids[:, -1].cpu().detach()\n",
    "        tokens = torch.cat([tokens, torch.tensor([[last_token]])], dim=-1)\n",
    "\n",
    "            \n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "The user speaks: \"My ship is called George\".\n",
    "\n",
    "Discussion:\n",
    "Q: Who is the user speaking to?\n",
    "A: \n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5747/1036420236.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = torch.nn.functional.softmax(last_distribution)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.8406e-08)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_probability_given_prompt(model, prompt, \"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'George'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer_greedy(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
