{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/qa_train_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list = json.load(open('../data/qa_dev_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_small' + str(6))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = \"cuda\"\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokens, length):\n",
    "    return model.generate(\n",
    "        tokens.to(device),\n",
    "        max_length=tokens.shape[1] + length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt, topk=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        text = prompt\n",
    "        start = len(prompt)\n",
    "        while \"\\n\" not in text[start:]:\n",
    "            output = inference(model, tokens, length)\n",
    "            decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            text += decoded[len(text):]\n",
    "            tokens = output\n",
    "\n",
    "    end = text.find('\\n', start)\n",
    "    return text[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_answers(model, prompt, num_replicas=10):\n",
    "    model.eval()\n",
    "    \n",
    "    answers_and_scores = [(\"\", 1) for _ in range(num_replicas*num_replicas)]\n",
    "    \n",
    "    texts = [prompt] * num_replicas\n",
    "    start = len(prompt)\n",
    "    with torch.no_grad():\n",
    "        while any(\"\\n\" not in text[start:] for text in texts):\n",
    "            tokens = tokenizer.batch_encode_plus(texts, return_tensors='pt').input_ids\n",
    "            tokens = tokens.to(device)\n",
    "            output = model(tokens)\n",
    "            for answer_index in range(num_replicas):\n",
    "                probs = torch.softmax(output.logits[answer_index][-1], dim=-1)\n",
    "                indices = torch.topk(probs, k=num_replicas).indices\n",
    "                for output_index, token_index in enumerate(indices):\n",
    "                    p = probs[token_index]\n",
    "                    total_index = answer_index * num_replicas + output_index\n",
    "                    new_tokens = tokens[answer_index].tolist() + [int(token_index)]\n",
    "                    decoded_text = tokenizer.decode(new_tokens)\n",
    "                    new_score =  answers_and_scores[total_index][1] * float(p)\n",
    "                    answers_and_scores[total_index] = decoded_text, new_score\n",
    "            \n",
    "            new_answers_and_scores = []\n",
    "            already_answered = set()\n",
    "            for answer, score in answers_and_scores:\n",
    "                if answer in already_answered:\n",
    "                    continue\n",
    "                \n",
    "                new_answers_and_scores.append((answer, score))\n",
    "                already_answered.add(answer)\n",
    "            \n",
    "            new_answers_and_scores = sorted(new_answers_and_scores, key=lambda x: -x[1])\n",
    "            texts = [new_answers_and_scores[index][0] for index in range(num_replicas)]\n",
    "            for i in range(num_replicas):\n",
    "                for j in range(num_replicas):\n",
    "                    answers_and_scores[i * j] = new_answers_and_scores[j]\n",
    "\n",
    "    scores = []\n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', start)\n",
    "        texts[index] = texts[index][start:end].split(':')[-1].strip()\n",
    "        scores.append(answers_and_scores[index][1])\n",
    "\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.7 ms, sys: 0 ns, total: 69.7 ms\n",
      "Wall time: 69.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "answers, scores = generate_all_answers(model, prompt, num_replicas=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['st. George', 'St. George', 'Sir George', 'George']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29918041831489056,\n",
       " 0.29318870276476167,\n",
       " 0.29318870276476167,\n",
       " 0.29318870276476167]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers_with_dropout(model, prompt, num_replicas=25, length=5):\n",
    "    model.train()\n",
    "    outputs = []\n",
    "    start = len(prompt)\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        texts = [prompt] * num_replicas\n",
    "        while any(\"\\n\" not in text[start:] for text in texts):\n",
    "            if tokens.shape[1] + length > 1024:\n",
    "                break\n",
    "            \n",
    "            output = model.generate(\n",
    "                tokens.cuda(),\n",
    "                max_length=tokens.shape[1] + length,\n",
    "                pad_token_id=50256\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            tokens = output\n",
    "            \n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', start)\n",
    "        texts[index] = texts[index][start:end].split(':')[-1].strip()\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.2 ms, sys: 0 ns, total: 79.2 ms\n",
      "Wall time: 78.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['St. George',\n",
       " 'St. George',\n",
       " 'st. George',\n",
       " \"George's\",\n",
       " 'George',\n",
       " 'St. George',\n",
       " 'st. George',\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'st. George',\n",
       " 'st. George',\n",
       " 'St. George',\n",
       " 'st. George',\n",
       " 'St. George',\n",
       " 'St. George']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers(model, prompt, num_replicas, length=100):\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    tokens_length = tokens.input_ids.shape[1]\n",
    "    if tokens_length + length > 1024:\n",
    "        return ''\n",
    "    generated_ids = model.generate(**tokens,\n",
    "                                   num_beams=num_replicas,\n",
    "                                   num_return_sequences=num_replicas,\n",
    "                                   max_length=tokens_length + length,\n",
    "                                  )\n",
    "    generated_sentences = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sentences = []\n",
    "    for index, item in enumerate(generated_sentences):\n",
    "        output = generated_sentences[index]\n",
    "        offset = len(prompt)\n",
    "        start = offset + 1\n",
    "        end = min(output.find('\\n', start), output.find('Q:', start))\n",
    "        sentences.append(item[start: end])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(0))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 19.9 ms, total: 1.55 s\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_answer_prompt(story, question):\n",
    "    return f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = get_answer_prompt(story, question)\n",
    "#answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)\n",
    "answers = generate_multiple_answers(model, prompt, num_replicas=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Dict, List\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Entailer:\n",
    "    def __init__(self):\n",
    "        model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\n",
    "            _device\n",
    "        )\n",
    "\n",
    "    def get_relation(self, premise: str, hypothesis: str) -> Dict[str, float]:\n",
    "        with torch.no_grad():\n",
    "            encodings = self._tokenizer(\n",
    "                premise, hypothesis, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            output = self._model(encodings[\"input_ids\"].to(_device))\n",
    "            prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "            label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            prediction = {name: float(pred) for pred, name in zip(prediction, label_names)}\n",
    "            return prediction\n",
    "\n",
    "    def entails(self, premise: str, hypothesis: str, threshold=0.5) -> bool:\n",
    "        prediction = self.get_relation(premise, hypothesis)\n",
    "        if prediction[\"entailment\"] > threshold:\n",
    "            return True\n",
    "        \n",
    "        return prediction[\"entailment\"] > threshold\n",
    "\n",
    "    def batch_entails(self, premise: List[str], \n",
    "                      hypothesis: List[str], \n",
    "                      threshold: float = 0.5, \n",
    "                      return_scores: bool = False) -> bool:\n",
    "        with torch.no_grad():\n",
    "            encodings = self._tokenizer.batch_encode_plus([lhs + \" [SEP] \" + rhs \n",
    "                                                           for lhs, rhs in zip(premise, hypothesis)], \n",
    "                                                          return_tensors=\"pt\", \n",
    "                                                          padding=True\n",
    "            )\n",
    "            output = self._model(**encodings.to(_device))\n",
    "            prediction = torch.softmax(output[\"logits\"], dim=-1)\n",
    "            if not return_scores:\n",
    "                return (prediction[:, 0] > threshold).tolist()\n",
    "            \n",
    "            return prediction[:, 0].tolist()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alce/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entailer = Entailer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9715461134910583, 0.001479353872127831]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailer.batch_entails([\"my name is alberto\", \"my name is John\"], [\"Alberto is the name\", \"Alberto is speaking\"], return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discussion_from_text(text, last_n=None):\n",
    "    trigger = \"Discussion:\\n\"\n",
    "    start = text.find(trigger) + len(trigger)\n",
    "    end = text.rfind(\"\\n\")\n",
    "    text = text[start:end]\n",
    "    if last_n:        \n",
    "        chunks = text.split(\"\\nQ:\")\n",
    "        text = \"Q:\" + \"\\nQ:\".join(chunks[-last_n:])\n",
    "        return text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = get_discussion_from_text(dev_list[0], last_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt_from_dialogue_and_answer(dialogue, answer):\n",
    "        return f\"\"\"\n",
    "Discussion:\n",
    "{dialogue.strip()}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: Where did Cotton's mother put her to clean the paint off?\n",
      "A: a bucket of water\n",
      "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
      "A: licked her face\n",
      "Q: Did they want Cotton to change the color of her fur?\n",
      "A: No\n",
      "\n",
      "Statement:\n"
     ]
    }
   ],
   "source": [
    "print(get_statement_prompt_from_dialogue_and_answer(dialogue, answer=\"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt_from_question_and_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def is_yes_no_question(text):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return False\n",
    "\n",
    "    text = re.sub(\"^Are\", \"are\", text)\n",
    "    text = re.sub(\"^Am\", \"am\", text)\n",
    "\n",
    "    word_and_pos_list = pos_tag(word_tokenize(text))\n",
    "    first_tag = word_and_pos_list[0][1]\n",
    "    if first_tag in [\"VBZ\", \"VBD\", \"VBP\", \"MD\"]:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_yes_no_question(\"am I right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers_from_multiple_prompts(model, prompts, length=5):\n",
    "    model.eval()\n",
    "    starts = [len(prompt) for prompt in prompts]\n",
    "    num_replicas = len(prompts)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tokens = tokenizer.batch_encode_plus(prompts, padding=True, return_tensors='pt').input_ids\n",
    "        texts = prompts\n",
    "        while any(\"\\n\" not in text[start + 1:] for text, start in zip(texts, starts)):\n",
    "            if tokens.shape[1] + length > 1024:\n",
    "                break\n",
    "            \n",
    "            output = model.generate(\n",
    "                tokens.cuda(),\n",
    "                max_length=tokens.shape[1] + length,\n",
    "                pad_token_id=50256\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            tokens = output\n",
    "\n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', starts[index] + 1)\n",
    "        texts[index] = texts[index][starts[index]:end].split(':')[-1].strip()\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_from_question_and_answer(question, answer):\n",
    "    text = f\"\"\"\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()\n",
    "    return generate_answer(statement_model, text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 30.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_dict = {}\n",
    "for answer in answers:\n",
    "    count_dict.setdefault(answer, 0)\n",
    "    count_dict[answer] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Score on Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(0))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\"\n",
    "prompt = get_answer_prompt(story, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_answer_prompt = '\\nA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_answer_prompt)\n",
    "\n",
    "def get_answer_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_answer_prompt))\n",
    "    return text[pos + len(_answer_prompt):end]\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_with_entailment(model, statement_model, prompt, story, dialogue):\n",
    "    answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)\n",
    "    #answers, _ = generate_all_answers(model, prompt, num_replicas=10)\n",
    "    statements = []\n",
    "    \n",
    "    valid_answers = set()\n",
    "    repetitions = 0\n",
    "    while not valid_answers and repetitions < 2:\n",
    "        count_dict = {}\n",
    "        for answer in answers:\n",
    "            count_dict.setdefault(answer, 0)\n",
    "            count_dict[answer] += 1\n",
    "        \n",
    "        answers = count_dict.keys()\n",
    "        \n",
    "        statement_prompts = []\n",
    "        for answer in answers:\n",
    "            statement_prompts.append(get_statement_prompt_from_dialogue_and_answer(dialogue, answer))\n",
    "        \n",
    "        statements = generate_answers_from_multiple_prompts(statement_model, statement_prompts)\n",
    "        entailment_predictions = entailer.batch_entails([story] * len(statements), statements, return_scores=True)\n",
    "        for answer, answer_score in zip(answers, entailment_predictions):\n",
    "            if answer_score > 0.5:\n",
    "                valid_answers.add((answer, answer_score))\n",
    "                \n",
    "        repetitions += 1\n",
    "            \n",
    "    if not valid_answers:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    ranked_answers = [(answer[0], count_dict[answer[0]]*answer[1]) for answer in valid_answers]\n",
    "    # ranked_answers = [(answer[0], answer[1]) for answer in valid_answers]\n",
    "    return sorted(ranked_answers, key=lambda x: -x[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "number = 1\n",
    "\n",
    "small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                     max_num_questions=3,\n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "prediction = get_answer_with_entailment(model, \n",
    "               statement_model,\n",
    "               small_text,\n",
    "               dev_dict['data'][index][\"story\"],\n",
    "               get_discussion_from_text(small_text, last_n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Discussion:\\nQ:Q: Who is at the door?\\nA: An elderly Chinese lady and a little boy\\nQ: Is she carrying something?\\nA: Her mommy's\\n\\nStatement:\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statement_prompt_from_dialogue_and_answer(get_discussion_from_text(small_text, last_n=3), \"Her mommy's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the text below two people are discussing a story.\n",
      "\n",
      "Story:\n",
      "My doorbell rings. On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
      "\n",
      "I know this lady. It is not her first visit. She is the boy's grandmother, and her daughter bought the house next door last October. \n",
      "\n",
      "Her daughter, Nicole, speaks fluent English. But she is now in Shanghai, and her parents are here with the little boy. Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients. \n",
      "\n",
      "I know what is inside the bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake. This has become an almost-daily practice. \n",
      "\n",
      "Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello. Once, she brought an iPad as well as the food. She pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go with her to her house. Then, she handed the iPad to her husband and almost immediately I found myself looking at Nicole in Shanghai and discussing her mother's cooking and salt intake. Instantly, tears welled in my eyes. \n",
      "\n",
      "\"Your mother just can't be bringing me meals like this all the time,\" I insisted. \"I can hardly do dishes in return.\" \n",
      "\n",
      "\"Oh, no, Lucy.\" Nicole said. \"Mum doesn't like western food. Don't worry about it; she has to cook for the three of them anyway, and she wants to do it.\" \n",
      "\n",
      "The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
      "\n",
      "I am now working on some more Chinese words--it's the least I can do after such display of kindness. \n",
      "\n",
      "\"Thank you\" is, of course, the first one. Somehow, it seems inadequate.\n",
      "\n",
      "Discussion:\n",
      "Q: Who is at the door?\n",
      "A: An elderly Chinese lady and a little boy\n",
      "Q: Is she carrying something?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alce/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "    unknown_predictions = []\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:100]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=3,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            try:\n",
    "                prediction = get_answer_with_entailment(model, \n",
    "                               statement_model,\n",
    "                               small_text,\n",
    "                               dev_dict['data'][index][\"story\"],\n",
    "                               get_discussion_from_text(small_text, last_n=2))\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if not prediction or prediction == \"unknown\":\n",
    "                unknown_predictions.append({\n",
    "                        'index': index,\n",
    "                        'number' : number,\n",
    "                        'label': label, \n",
    "                        'prediction': prediction})\n",
    "                continue\n",
    "        \n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                    false_positives.append(prediction)\n",
    "                \n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif entailer.entails(label, prediction):\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif entailer.entails(label, prediction):\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    wrong_predictions.append({\n",
    "                        'index': index,\n",
    "                        'number' : number,\n",
    "                        'label': label, \n",
    "                        'prediction': prediction})\n",
    "                    \n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, unknown_predictions, false_positives, total_number_of_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▎                                   | 15/100 [03:43<19:17, 13.62s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1411 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████████| 100/100 [35:58<00:00, 21.58s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy, wrong_predictions, unknown_predictions, false_positives, total_number_of_questions = compute_accuracy_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1313, 0.7616146230007617)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknown_predictions), total_number_of_questions, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first 10, last 3 questions:\n",
    "\n",
    "    without replicas: 0.770\n",
    "\n",
    "    15 replicas, 1 repetition, using whole dialogue: 0.768595041322314\n",
    "            num unknown_predictions = 20\n",
    "            answered_questions = 121\n",
    "            \n",
    "    15 replicas, 1 repetition, using whole dialogue: 0.7768595041322314\n",
    "            num unknown_predictions = 20\n",
    "            answered_questions = 121\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue: 0.8211382113821138\n",
    "            num unknown_predictions = 20\n",
    "            answered_questions = 121\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue: 0.7768595041322314\n",
    "            num unknown_predictions = 20\n",
    "            answered_questions = 121\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue: 0.8211382113821138\n",
    "            num unknown_predictions = 18\n",
    "            answered_questions = 123\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue: 0.8048780487804879\n",
    "            num unknown_predictions = 18\n",
    "            answered_questions = 123\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue, only scores (not count): 0.7154471544715447\n",
    "            num unknown_predictions = 18\n",
    "            answered_questions = 123\n",
    "            \n",
    "    15 replicas, 2 repetitions, using whole dialogue: 0.7868852459016393\n",
    "            num unknown_predictions = 19\n",
    "            answered_questions = 122\n",
    "            \n",
    "    15 replicas, 4 repetitions, using whole dialogue: 0.79\n",
    "            num unknown_predictions = 18\n",
    "            answered_questions = 123\n",
    "    15 replicas, 10 repetitions, using whole dialogue: 0.76\n",
    "            num unknown_predictions = 19\n",
    "            answered_questions = 122\n",
    "    \n",
    "    Using unique beam search (deterministic, only one repetition):\n",
    "    - 10 replicas, 1 repetition, last 2 question for statements: 0.6923076923076923\n",
    "    \n",
    " \n",
    " # first 100, last 3 questions:\n",
    "     15 replicas, 2 repetitions, using whole dialogue: 0.7616146230007617\n",
    "            num unknown_predictions = 265\n",
    "            answered_questions = 1313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some improvement with entailer. This is encouraging!!\n",
    "\n",
    "\n",
    "### TODO:\n",
    "    * Use different dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
