{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb85ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5e0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/adversarial_claims_train.json'))\n",
    "dev_list = json.load(open('../data/adversarial_claims_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5613ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_correct_answer_prompt = '\\nCA: '\n",
    "_wrong_answer_prompt = '\\nWA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_correct_answer_prompt)\n",
    "\n",
    "def get_correct_answer_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_correct_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_correct_answer_prompt))\n",
    "    return text[pos + len(_correct_answer_prompt):end]\n",
    "\n",
    "def get_wrong_answer_number(text, number):\n",
    "    pos = text.find(_wrong_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_wrong_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_wrong_answer_prompt))\n",
    "    return text[pos + len(_wrong_answer_prompt):end]\n",
    "\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_description_from_text(text):\n",
    "    start_prompt = 'Story:'\n",
    "    end_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):text.find(end_prompt)]\n",
    "\n",
    "def get_discussion_from_text(text):\n",
    "    start_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):].strip()\n",
    "\n",
    "def get_statement_prompt_from_text(full_text, number, max_questions=5):\n",
    "    text = 'Discussion:\\n'\n",
    "    questions_and_answers_list = get_discussion_from_text(full_text).split('\\n')\n",
    "    start = max(0, (number + 1 - max_questions) * 3)\n",
    "    end = (number + 1) * 3\n",
    "    questions_and_answers_list = questions_and_answers_list[start:end]\n",
    "    text += '\\n'.join(questions_and_answers_list)\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd1e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statement_from_dialogue(statement_model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = statement_model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2facc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a252ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f11a400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(2))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29ee6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_statement_prompt_from_text(full_text, number, max_questions=5):    \n",
    "    text = 'Discussion:\\n'\n",
    "    start = max(0, number - max_questions)\n",
    "    end = number + 1\n",
    "    for index in range(start, end):\n",
    "        text += f'Q: {get_question_number(full_text, index).capitalize()}\\n'\n",
    "        text += f'A: {get_correct_answer_number(full_text, index).replace(\".\", \"\").capitalize()}\\n'\n",
    "\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d83b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refuting_statement_prompt_from_text(full_text, number, max_questions=5):    \n",
    "    text = 'Discussion:\\n'\n",
    "    start = max(0, number - max_questions)\n",
    "    end = number + 1\n",
    "    for index in range(start, end):\n",
    "        text += f'Q: {get_question_number(full_text, index).capitalize()}\\n'\n",
    "        if index != number:\n",
    "            text += f'A: {get_correct_answer_number(full_text, index).capitalize()}\\n'\n",
    "    text += f'A: {get_wrong_answer_number(full_text, index).replace(\".\", \"\").capitalize()}\\n'\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cec7be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supporting_claim_from_questions(text, number):\n",
    "    description = get_description_from_text(text)\n",
    "    statement_prompt = get_correct_statement_prompt_from_text(text, number)\n",
    "    statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "    return create_claim_from_description_and_dialogue(description, statement) + 'Yes.'\n",
    "\n",
    "def get_refuting_claim_from_questions(text, number):\n",
    "    description = get_description_from_text(text)\n",
    "    statement_prompt = get_refuting_statement_prompt_from_text(text, number)\n",
    "    statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "    return create_claim_from_description_and_dialogue(description, statement) + 'Nope.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52b2e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claim_from_description_and_dialogue(description, dialogue):\n",
    "    if not dialogue:\n",
    "        return ''\n",
    "    if dialogue[-1] == '.':\n",
    "        dialogue = dialogue[:-1]    \n",
    "    text = 'Evidence:\\n'\n",
    "    text += description.replace('\\n\\n', '\\n') + '\\n\\n'\n",
    "    text += 'Claim:\\n'\n",
    "    text += dialogue + '\\n\\n'\n",
    "    text += 'The evidence supports the claim:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f10afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_supporting_claims(text_list):\n",
    "    claims = []\n",
    "    for text in tqdm(text_list):\n",
    "        for number in range(get_answers_number(text)):\n",
    "            claims.append(get_supporting_claim_from_questions(text, number))\n",
    "    return claims\n",
    "\n",
    "def get_refuting_claims(text_list):\n",
    "    claims = []\n",
    "    for text in tqdm(text_list):\n",
    "        for number in range(get_answers_number(text)):\n",
    "            claims.append(get_refuting_claim_from_questions(text, number))\n",
    "    return claims        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c2c9dd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3968/3968 [38:49<00:00,  1.70it/s]  \n",
      "100%|██████████| 3968/3968 [39:02<00:00,  1.69it/s]  \n",
      "100%|██████████| 327/327 [03:26<00:00,  1.59it/s]\n",
      "100%|██████████| 327/327 [03:26<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "supporting_texts = get_supporting_claims(train_list)\n",
    "refuting_texts = get_refuting_claims(train_list)\n",
    "\n",
    "dev_supporting_texts = get_supporting_claims(dev_list)\n",
    "dev_refuting_texts = get_refuting_claims(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76e0fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "\n",
      "CHAPTER XXIV. THE INTERRUPTED MASS \n",
      "The morning of that Wednesday of Corpus Christi, fateful to all concerned in this chronicle, dawned misty and grey, and the air was chilled by the wind that blew from the sea. The chapel bell tinkled out its summons, and the garrison trooped faithfully to Mass. \n",
      "Presently came Monna Valentina, followed by her ladies, her pages, and lastly, Peppe, wearing under his thin mask of piety an air of eager anxiety and unrest. Valentina was very pale, and round her eyes there were dark circles that told of sleeplessness, and as she bowed her head in prayer, her ladies observed that tears were falling on the illuminated Mass-book over which she bent. And now came Fra Domenico from the sacristy in the white chasuble that the Church ordains for the Corpus Christi feast, followed by a page in a clerkly gown of black, and the Mass commenced. \n",
      "There were absent only from the gathering Gonzaga and Fortemani, besides a sentry and the three prisoners. Francesco and his two followers. \n",
      "Gonzaga had presented himself to Valentina with the plausible tale that, as the events of which Fanfulla's letter had given them knowledge might lead Gian Maria at any moment to desperate measures, it might be well that he should reinforce the single man-at-arms patrolling the walls. Valentina, little recking now whether the castle held or fell, and still less such trifles as Gonzaga's attendance at Mass, had assented without heeding the import of what he said. \n",
      "And so, his face drawn and his body quivering with the excitement of what he was about to do, Gonzaga had repaired to the ramparts so soon as he had seen them all safely into chapel. The sentinel was that same clerkly youth Aventano, who had read to the soldiers that letter Gian Maria had sent Gonzaga. This the courtier accepted as a good omen. If a man there was among the soldiery at Roccaleone with whom he deemed that he had an account to settle, that man was Aventano. \n",
      "\n",
      "\n",
      "Claim:\n",
      "The garrison was in the group\n",
      "\n",
      "The evidence supports the claim:\n",
      "Nope.\n"
     ]
    }
   ],
   "source": [
    "print(refuting_texts[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a6b1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6862"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refuting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5607f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(supporting_texts, open('../data/adv_supporting.json', 'w'))\n",
    "json.dump(refuting_texts, open('../data/adv_refuting.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f59d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dev_supporting_texts, open('../data/adv_dev_supporting.json', 'w'))\n",
    "json.dump(dev_refuting_texts, open('../data/adv_dev_refuting.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068574c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
