{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d041322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8380e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba651de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a34f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))\n",
    "dev_list = json.load(open('../data/qa_dev_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f10956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\\'s horses slept. But Cotton wasn\\'t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton\\'s mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer\\'s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\\n\"What are you doing, Cotton?!\" \\n\\n\"I only wanted to be more like you\". \\n\\nCotton\\'s mommy rubbed her face on Cotton\\'s and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton\\'s mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton\\'s fur was all all dry. \\n\\n\"Don\\'t ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn\\'t want that!\" \\n\\nThen Cotton thought, \"I change my mind. I like being special\".'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dict['data'][0]['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a0dbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81d06c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-d88cfd312026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                      last_question=False)\n\u001b[1;32m      7\u001b[0m \u001b[0mfirst_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_first_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multiple_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-da76bdb9cafe>\u001b[0m in \u001b[0;36mgenerate_multiple_answers\u001b[0;34m(model, prompt, num_replicas)\u001b[0m\n\u001b[1;32m     12\u001b[0m              \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m              \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m              \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         )\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_specific_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m             )\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, encoder_outputs, attention_mask, use_cache, model_specific_kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             )\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             )\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 170\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \"\"\"\n\u001b[1;32m   2048\u001b[0m     return torch.layer_norm(input, normalized_shape, weight, bias, eps,\n\u001b[0;32m-> 2049\u001b[0;31m                             torch.backends.cudnn.enabled)\n\u001b[0m\u001b[1;32m   2050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc=0\n",
    "number = 1\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "first_answer = generate_first_answer(model, small_text)\n",
    "answers = generate_multiple_answers(model, small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_from_data_item(item):\n",
    "    return item['story']\n",
    "\n",
    "def get_dialogue_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = ''\n",
    "    text += ' '.join([q['input_text'] + ' ' + a['input_text'] + '.'\n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '?'.join(text.split('?')[:-1]) + '?'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claim_from_description_and_dialogue(description, dialogue):\n",
    "    if dialogue[-1] == '.':\n",
    "        dialogue = dialogue[:-1]    \n",
    "    text = 'Evidence:\\n'\n",
    "    text += description.replace('\\n\\n', '\\n') + '\\n\\n'\n",
    "    text += 'Claim:\\n'\n",
    "    text += dialogue + '\\n\\n'\n",
    "    text += 'The evidence supports the claim:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_answer(description, dialogue, first_answer, answers):\n",
    "    return first_answer\n",
    "    best_perplexity = 1e6\n",
    "    \n",
    "    current_dialogue = dialogue + ' ' + first_answer\n",
    "    text = create_claim_from_description_and_dialogue(description, current_dialogue)\n",
    "    y_or_n, perplexity = generate_answer(fact_checking_model, text)\n",
    "    if y_or_n == 'Y':\n",
    "        return first_answer, best_perplexity\n",
    "    \n",
    "    best_answer = ''\n",
    "    for answer in answers:\n",
    "        current_dialogue = dialogue + ' ' + answer\n",
    "        text = create_claim_from_description_and_dialogue(description, current_dialogue)\n",
    "        y_or_n, perplexity = generate_answer(fact_checking_model, text)\n",
    "        if perplexity < best_perplexity and y_or_n == 'Y':\n",
    "            best_perplexity = perplexity\n",
    "            best_answer = answer\n",
    "    if not best_answer:\n",
    "        best_answer = first_answer\n",
    "    return best_answer, best_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa7cc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "number = 6\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bdf939f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white', 'White']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('White', 1000000.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = 0\n",
    "number = 0\n",
    "description = get_description_from_data_item(dev_dict['data'][doc])\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "first_answer = generate_first_answer(model, small_text)\n",
    "answers = generate_multiple_answers(model, small_text)\n",
    "dialogue = get_dialogue_from_data_item(dev_dict['data'][doc],\n",
    "                                       max_num_questions=5, \n",
    "                                       question_number=number,\n",
    "                                       last_question=False)\n",
    "print(answers)\n",
    "select_answer(description, dialogue, first_answer, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370e942",
   "metadata": {},
   "source": [
    "# Computing accuracy after fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d188c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_original_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "        total_questions = get_answers_number(text)\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=5,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            #description = get_description_from_data_item(dev_dict['data'][index])\n",
    "            #dialogue = get_dialogue_from_data_item(dev_dict['data'][index],\n",
    "            #                           max_num_questions=5, \n",
    "            #                           question_number=number,\n",
    "            #                           last_question=False)\n",
    "            first_answer = generate_first_answer(model, small_text)\n",
    "            #answers = generate_multiple_answers(model, small_text)\n",
    "            prediction, _ = select_answer(description, dialogue, first_answer, [])\n",
    "            if not prediction:\n",
    "                print('NO PREDICTION!!')\n",
    "                continue\n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                    false_positives.append(prediction)\n",
    "                \n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                else:\n",
    "                    wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82fe9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "        total_questions = get_answers_number(text)\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=5,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            description = get_description_from_data_item(dev_dict['data'][index])\n",
    "            dialogue = get_dialogue_from_data_item(dev_dict['data'][index],\n",
    "                                       max_num_questions=5, \n",
    "                                       question_number=number,\n",
    "                                       last_question=False)\n",
    "            first_answer = generate_first_answer(model, small_text)\n",
    "            answers = generate_multiple_answers(model, small_text)\n",
    "            prediction, _ = select_answer(description, dialogue, first_answer, answers)\n",
    "            if not prediction:\n",
    "                print('NO PREDICTION!!')\n",
    "                continue\n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                    false_positives.append(prediction)\n",
    "                \n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                else:\n",
    "                    wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2676fc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:53<00:00, 41.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7021276595744681,\n",
       " [{'label': 'paint herself like them', 'prediction': 'she painted herself'},\n",
       "  {'label': \"the farmer's\", 'prediction': 'her mommy'},\n",
       "  {'label': 'the farmer', 'prediction': 'her mommy'},\n",
       "  {'label': \"the old farmer's\", 'prediction': 'her mommy'},\n",
       "  {'label': 'rubbed her face', 'prediction': 'laughed'},\n",
       "  {'label': 'started laughing', 'prediction': 'laughed'},\n",
       "  {'label': 'they started laughing', 'prediction': 'laughed'},\n",
       "  {'label': 'dropped her into a big bucket of water',\n",
       "   'prediction': 'a bucket of water'},\n",
       "  {'label': 'a big bucket of water', 'prediction': 'a bucket of water'},\n",
       "  {'label': 'the bottle', 'prediction': 'It was hard and clear'},\n",
       "  {'label': 'a bottle', 'prediction': 'It was hard and clear'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'no', 'prediction': 'Yes'},\n",
       "  {'label': \"They took the note to Asta's papa\",\n",
       "   'prediction': \"took it to Asta's papa\"},\n",
       "  {'label': 'unknown', 'prediction': \"took it to Asta's papa\"},\n",
       "  {'label': \"They took the note to Asta's papa\",\n",
       "   'prediction': \"took it to Asta's papa\"},\n",
       "  {'label': 'unknown', 'prediction': 'Yes'},\n",
       "  {'label': 'an elderly Chinese lady', 'prediction': 'a Chinese lady'},\n",
       "  {'label': 'An elderly Chinese lady and a little boy',\n",
       "   'prediction': 'a Chinese lady'},\n",
       "  {'label': 'elderly Chinese lady', 'prediction': 'a Chinese lady'},\n",
       "  {'label': 'Yes', 'prediction': 'No'},\n",
       "  {'label': 'yes', 'prediction': 'No'},\n",
       "  {'label': 'his neighbor', 'prediction': 'Nicole'},\n",
       "  {'label': 'her daughter bought the house next door', 'prediction': 'Nicole'},\n",
       "  {'label': 'house next door', 'prediction': 'Shanghai'},\n",
       "  {'label': 'next door', 'prediction': 'Shanghai'},\n",
       "  {'label': 'an iPad',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake'},\n",
       "  {'label': 'rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'food'},\n",
       "  {'label': 'hot soup and a container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'food'},\n",
       "  {'label': 'hot soup, rice, vegetables, chicken, meat, or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'food'},\n",
       "  {'label': 'soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'food'},\n",
       "  {'label': 'use the iPad',\n",
       "   'prediction': 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty I am not used to iPads, so she indicated I should go wit'},\n",
       "  {'label': 'I am now working on some more Chinese words',\n",
       "   'prediction': 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty I am not used to iPads, so she indicated I should go wit'},\n",
       "  {'label': 'go with her to her house',\n",
       "   'prediction': 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty I am not used to iPads, so she indicated I should go wit'},\n",
       "  {'label': 'Nicole',\n",
       "   'prediction': 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty I am not used to iPads, so she indicated I should go wit'},\n",
       "  {'label': 'Yes', 'prediction': 'Dennis Farina'},\n",
       "  {'label': 'yes', 'prediction': 'Dennis Farina'},\n",
       "  {'label': 'he was an actor', 'prediction': 'dapper'},\n",
       "  {'label': 'cop-turned-actor', 'prediction': 'dapper'},\n",
       "  {'label': 'Actor', 'prediction': 'dapper'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'Snatch', 'prediction': 'Yes'},\n",
       "  {'label': 'got into acting', 'prediction': 'He was on Law & Order'},\n",
       "  {'label': 'he was a consultant', 'prediction': 'He was on Law & Order'},\n",
       "  {'label': 'he got into acting', 'prediction': 'He was on Law & Order'},\n",
       "  {'label': 'Farina was cast in a film',\n",
       "   'prediction': 'He was on Law & Order'},\n",
       "  {'label': 'cops or gangsters', 'prediction': 'Police or gangsters'},\n",
       "  {'label': 'he joined the cast of  Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'label': 'joined the cast of Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'label': 'He joined a TV show cast',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'label': ', he joined the cast of Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'label': 'Detective Joe Fontana', 'prediction': 'Jerry Orbach'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'no', 'prediction': 'Yes'},\n",
       "  {'label': 'An expensive car', 'prediction': 'a expensive car'},\n",
       "  {'label': 'an expensive car', 'prediction': 'a expensive car'},\n",
       "  {'label': 'flashy clothes and an expensive car',\n",
       "   'prediction': 'a expensive car'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'no', 'prediction': 'Yes'},\n",
       "  {'label': 'A cop', 'prediction': 'Police officer'},\n",
       "  {'label': 'No', 'prediction': 'yes'},\n",
       "  {'label': 'no', 'prediction': 'yes'},\n",
       "  {'label': 'get cookies and milk',\n",
       "   'prediction': 'walk home from the bus stop'},\n",
       "  {'label': \"go to Quentin's house\",\n",
       "   'prediction': 'walk home from the bus stop'},\n",
       "  {'label': 'they go in for cookies and milk',\n",
       "   'prediction': 'walk home from the bus stop'},\n",
       "  {'label': 'go in for cookies and milk',\n",
       "   'prediction': 'walk home from the bus stop'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'no', 'prediction': 'Yes'},\n",
       "  {'label': 'no answer', 'prediction': 'she thought something might be wrong'},\n",
       "  {'label': 'no one answered',\n",
       "   'prediction': 'she thought something might be wrong'},\n",
       "  {'label': 'heverything would be okay',\n",
       "   'prediction': 'that he was sure that everything would be okay'},\n",
       "  {'label': \"Quinton's mother\", 'prediction': 'her mother'},\n",
       "  {'label': 'dentist', 'prediction': 'to the bus stop'},\n",
       "  {'label': 'the dentist', 'prediction': 'to the bus stop'},\n",
       "  {'label': 'to the dentist', 'prediction': 'to the bus stop'},\n",
       "  {'label': 'yes', 'prediction': 'tomorrow'},\n",
       "  {'label': 'after lunch', 'prediction': 'tomorrow'},\n",
       "  {'label': 'New York', 'prediction': 'New Jersey'},\n",
       "  {'label': 'New York', 'prediction': 'New Jersey'},\n",
       "  {'label': 'southernmost part of both the city and state of New York',\n",
       "   'prediction': 'in the southwest'},\n",
       "  {'label': 'the southernmost part of both the city and state of New York',\n",
       "   'prediction': 'in the southwest'},\n",
       "  {'label': 'the Arthur Kill and the Kill Van Kull,',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'label': 'the Arthur Kill and the Kill Van Kull',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'label': 'Arthur Kill and the Kill Van Kull',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'label': 'inhabitants who feel neglected by the city government',\n",
       "   'prediction': 'by residents who feel neglected by the city government'},\n",
       "  {'label': 'inhabitants feel neglected by the city government',\n",
       "   'prediction': 'by residents who feel neglected by the city government'},\n",
       "  {'label': 'because the inhabitants feel neglected by the city government',\n",
       "   'prediction': 'by residents who feel neglected by the city government'},\n",
       "  {'label': 'No', 'prediction': 'Yes'},\n",
       "  {'label': 'no', 'prediction': 'Yes'},\n",
       "  {'label': 'the suspect', 'prediction': 'FBI agents'},\n",
       "  {'label': 'Gary Giordano', 'prediction': 'FBI agents'},\n",
       "  {'label': 'Maryland', 'prediction': 'Gaithersburg'},\n",
       "  {'label': 'Gaithersburg', 'prediction': 'Montgomery County'},\n",
       "  {'label': 'suspect in the recent disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'label': 'he is a suspect in the recent disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'label': 'he is the suspect in the disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'label': 'Solicitor General', 'prediction': 'Taco Stein'},\n",
       "  {'label': 'Giordano', 'prediction': 'Gardner'},\n",
       "  {'label': '2, Giordano told authorities that he had been snorkeling with Gardner',\n",
       "   'prediction': 'three days after Robyn Gardner was last seen'},\n",
       "  {'label': 'Two',\n",
       "   'prediction': 'three days after Robyn Gardner was last seen'},\n",
       "  {'label': 'Great Britain', 'prediction': 'India'},\n",
       "  {'label': 'Steam it',\n",
       "   'prediction': 'it lost nearly all of its healthy qualities'},\n",
       "  {'label': 'prune it',\n",
       "   'prediction': 'it lost nearly all of its healthy qualities'},\n",
       "  {'label': 'steam it',\n",
       "   'prediction': 'it lost nearly all of its healthy qualities'},\n",
       "  {'label': 'They steam it',\n",
       "   'prediction': 'it lost nearly all of its healthy qualities'},\n",
       "  {'label': 'may prevent heart disease',\n",
       "   'prediction': 'prevents heart disease'},\n",
       "  {'label': 'may prevent heart disease',\n",
       "   'prediction': 'prevents heart disease'},\n",
       "  {'label': 'It may prevent heart disease',\n",
       "   'prediction': 'prevents heart disease'},\n",
       "  {'label': 'Prevent heart disease', 'prediction': 'prevents heart disease'},\n",
       "  {'label': 'unknown', 'prediction': 'bad digestion'},\n",
       "  {'label': 'bloody',\n",
       "   'prediction': 'it was laid over a patch of sand and grass'},\n",
       "  {'label': 'propped up, back to back',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'label': 'what appears to be two bodies propped up,',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'label': 'two bodies propped up',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'label': 'in front of a military vehicle', 'prediction': 'a post'},\n",
       "  {'label': 'military vehicle', 'prediction': 'a post'},\n",
       "  {'label': 'a military vehicle', 'prediction': 'a post'},\n",
       "  {'label': 'souvenirs or trophies', 'prediction': 'killing civilians'},\n",
       "  {'label': 'taking or retaining individual souvenirs or trophies,',\n",
       "   'prediction': 'killing civilians'},\n",
       "  {'label': 'taking or retaining individual souvenirs or trophies',\n",
       "   'prediction': 'killing civilians'}],\n",
       " [\"took it to Asta's papa\", 'Yes', 'bad digestion'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b915617",
   "metadata": {},
   "source": [
    "# Results\n",
    "Original accuracy: 0.7092198581560284\n",
    "\n",
    "using save_fever2: 0.7092198581560284\n",
    "\n",
    "using save_fever_with_qa_data_7: 0.7021276595744681"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fd385",
   "metadata": {},
   "source": [
    "## Repeat the test with save number 7 <= there was an error with the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87271e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispatcher",
   "language": "python",
   "name": "dispatcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
