{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/qa_train_list.json', encoding='utf8'))\n",
    "dev_list = json.load(open('../data/qa_dev_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed 52 out of 7199\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_trimmed = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        tokens = tokens[:, :_limit]\n",
    "        total_trimmed += 1\n",
    "    data.append(tokens)\n",
    "print(f'Trimmed {total_trimmed} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-69e8fa9af1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs, labels=inputs)[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4b5c7b343dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_small' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_answer_prompt = '\\nA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_answer_prompt)\n",
    "\n",
    "def get_answer_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_answer_prompt))\n",
    "    return text[pos + len(_answer_prompt):end]\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_number(model, text, number):\n",
    "    prompt = get_text_up_to_question_number(text, number)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 20\n",
    "    tokens_length = tokens.shape[1]\n",
    "    output = model.generate(\n",
    "             tokens,\n",
    "             max_length=tokens_length + _length,\n",
    "             temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_number(output, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model_with_all_answers(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "        total_questions = get_answers_number(text)\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        for number in range(total_questions):\n",
    "            prediction = generate_answer_number(model, text, number)\n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                else:\n",
    "                    wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "            if not it_was_answered:\n",
    "                print('No Answer for number', number)\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             #temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset + 1\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers(model, prompt, num_replicas=25):\n",
    "    model.train()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            offset = len(prompt)\n",
    "            start = offset + 1\n",
    "            end = text.find('\\n', start)\n",
    "            outputs.append(text[start:end].split(':')[-1].strip())\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "sentence_model = sentence_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_text(text):\n",
    "    outputs = sentence_model.encode(text)\n",
    "    return outputs\n",
    "\n",
    "def group_similar_answers_and_get_scores(answers):\n",
    "    answers_dict = {}\n",
    "    threshold = 0.7\n",
    "    embeddings = get_embeddings_from_text(answers)\n",
    "    embeddings = np.array([e/np.linalg.norm(e) for e in embeddings])\n",
    "    similarity_matrix = np.matmul(embeddings, embeddings.transpose())\n",
    "    superseded = set()\n",
    "    superseded_from = {}\n",
    "    for i in range(len(answers)):\n",
    "        for j in range(len(answers)):\n",
    "            if i > j:\n",
    "                continue\n",
    "            if i != j and answers[i] == answers[j]:\n",
    "                continue\n",
    "            if similarity_matrix[i][j] > threshold :\n",
    "                answers_dict.setdefault(i, 0)\n",
    "                answers_dict[i] += 1\n",
    "                if i != j:\n",
    "                    superseded.add(j)\n",
    "                    superseded_from.setdefault(i, [])\n",
    "                    superseded_from[i].append(j)\n",
    "\n",
    "    answers_and_scores = [(index, score/len(answers))\n",
    "                          for index, score in answers_dict.items() \n",
    "                          if index not in superseded]\n",
    "    \n",
    "    new_scores_dict = {}\n",
    "    total_score = sum(item[1] for item in answers_and_scores)\n",
    "    for answer_index, score in answers_and_scores:\n",
    "        answer_group = [answers[answer_index]]\n",
    "        if answer_index in superseded_from:\n",
    "            answer_group += [answers[i] for i in superseded_from[answer_index]]\n",
    "        answer_group = tuple(set(answer_group))\n",
    "        if answer_group in new_scores_dict:\n",
    "            new_scores_dict[answer_group] += score / total_score\n",
    "        else:\n",
    "            new_scores_dict[answer_group] = score / total_score\n",
    "    \n",
    "    \n",
    "    return sorted(list(new_scores_dict.items()), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('white', 'White'), 0.846153846153846), (('orange',), 0.15384615384615385)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_similar_answers_and_get_scores(['white', 'white', 'white', 'white', 'orange', 'orange', 'white', 'white', 'white', 'white', 'white', 'orange', 'white', 'white', 'White', 'white', 'orange', 'white', 'white', 'white']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('in a barn',\n",
       "   'in the barn',\n",
       "   'In the barn',\n",
       "   'in a barn near a farm house.',\n",
       "   'In the barn.'),\n",
       "  0.2727272727272727),\n",
       " (('',), 0.22727272727272724),\n",
       " (('What is Cotton?', 'Cotton'), 0.0909090909090909),\n",
       " (('She lived in a barn in a barn.',\n",
       "   'She lived in a barn in a farm near a farm. farm.'),\n",
       "  0.0909090909090909),\n",
       " (('The farm house in the middle of the woods.',), 0.04545454545454545),\n",
       " ((\"Cotton's mommy\",), 0.04545454545454545),\n",
       " (('Her mommy and 5 sisters were all white.',), 0.04545454545454545),\n",
       " (('Where did she live?',), 0.04545454545454545),\n",
       " (('Cotton lived in a nice, warm place above the barn.',),\n",
       "  0.04545454545454545),\n",
       " (('\"I was born in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn in a barn i',),\n",
       "  0.04545454545454545),\n",
       " ((\"Cotton's home\",), 0.04545454545454545)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_text = get_text_from_data_item(dev_dict['data'][0], \n",
    "                                     max_num_questions=8,\n",
    "                                     question_number=1,\n",
    "                                     last_question=False)\n",
    "\n",
    "answers = generate_multiple_answers(model, small_text)\n",
    "group_similar_answers_and_get_scores(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:100]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=8,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            predictions = generate_multiple_answers(model, small_text)\n",
    "            predictions = group_similar_answers_and_get_scores(predictions)\n",
    "            all_predictions = []\n",
    "            #for prediction in predictions[:2]:\n",
    "            #    all_predictions += prediction[0]\n",
    "            best_prediction_score = predictions[0][1]\n",
    "            for prediction in predictions:\n",
    "                if prediction[1]/best_prediction_score < 0.8:\n",
    "                    continue\n",
    "                all_predictions += prediction[0]\n",
    "            print(all_predictions)\n",
    "            for prediction in all_predictions:\n",
    "                it_was_answered = False\n",
    "                if not prediction:\n",
    "                    print('NO PREDICTION!!')\n",
    "                    prediction = 'unknown'\n",
    "                prediction = prediction.replace('.', '').replace('\"', '')\n",
    "                it_was_answered = False\n",
    "                for label in all_answers[number]:\n",
    "                    label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                    if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                        false_positives.append(prediction)\n",
    "\n",
    "                    if prediction.lower() == label.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    elif prediction.lower() in label.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    elif label.lower() in prediction.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    else:\n",
    "                        wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "                        \n",
    "                if it_was_answered:\n",
    "                    break\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white', 'White']\n",
      "['in a barn', 'a barn']\n",
      "['no']\n",
      "['her mom and 5 other sisters', 'her mommy and 5 other sisters']\n",
      "['orange']\n",
      "['no']\n",
      "['she used her fur to paint herself', 'used it to paint herself', 'she painted herself', 'she used it to paint herself', 'painted herself like them', 'painted herself', 'she used her paint', 'she used her paint to paint herself', 'she painted herself like them', 'used her paint to paint herself']\n",
      "[\"her mommy's\", 'her mommy']\n",
      "['laughed', 'they laughed']\n",
      "['in a bucket of water', 'a bucket of water', 'a bucket']\n",
      "['licked her face', 'they licked her face']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1/100 [00:12<20:21, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Asta']\n",
      "['The bottle', 'The bottle.', 'A bottle']\n",
      "[\"Asta's friend\"]\n",
      "['yes', 'Yes']\n",
      "['Yes']\n",
      "['a note', 'A note']\n",
      "['Yes']\n",
      "['Asta']\n",
      "[\"they took it to Asta's papa\", \"They took it to Asta's papa.\", 'They took it to the papa', 'they took it to his papa', \"took it to Asta's papa\", \"They took it to Asta's papa\"]\n",
      "['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:22<18:01, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'Yes']\n",
      "['a Chinese lady', 'The elderly Chinese lady', 'the elderly Chinese lady', 'a lady']\n",
      "['yes', 'Yes']\n",
      "['A paper carrier bag', 'a paper carrier bag', 'Paper carrier bag']\n",
      "['Yes']\n",
      "['Nicole']\n",
      "['Shanghai']\n",
      "['She is his grandmother', 'Her mother is his grandmother', \"He's her grandmother\", 'His grandmother', 'He is his grandmother']\n",
      "['a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and chicken, meat or shrimp, sometimes with a kind of pancake', 'a thermos with hot soup and a stainless-steel container with rice', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake.', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and chicken, meat or shrimp', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and shrimp']\n",
      "['Yes']\n",
      "['she brought an iPad']\n",
      "['communication', 'communication between us', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake.', 'a thermos with hot soup and a stainless-steel container with rice', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and shrimp, sometimes with a kind of pancake']\n",
      "['food']\n",
      "['pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty.', 'she points to the screen, which displayed a message from her daughter telling her that her mother wanted to know if the food was all right and whether it was too salty.', 'she pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty.', 'I can point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty.', 'she points to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go wit', 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty.', 'I point to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go wit']\n",
      "['yes', 'Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:41<23:49, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you', 'thank you', 'Thank you\"']\n",
      "['yes']\n",
      "['Dennis Farina']\n",
      "['dapper']\n",
      "['No.', 'No']\n",
      "['Yes', 'Yes.']\n",
      "['Yes', 'Yes.']\n",
      "['He joined the cast of Law & Order', 'He was on \"Law & Order\"', 'he joined the cast of the long-running \"Law & Order\"', 'He joined the cast of the long-running \"Law & Order\"']\n",
      "['Michael Mann']\n",
      "['\"Thief\"', 'Thief', '\"Thief.\"']\n",
      "['Police or gangsters', 'Police officers or gangsters', 'Police or Gangster', 'Police or gangster', 'cops or gangsters', 'Police and gangsters', 'Police officer']\n",
      "['he joined the cast of the long-running \"Law & Order\"', 'He joined the cast of the long-running \"Law & Order\"', 'joined the cast of the long-running \"Law & Order\"']\n",
      "['Law & Order', '\"Law & Order\"']\n",
      "['Jerry Orbach', 'Joe Fontana', 'Detective Joe Fontana']\n",
      "['No']\n",
      "['expensive car', 'a expensive car', 'a cheap car', 'a car']\n",
      "['No']\n",
      "['flashy', 'flashy clothes and an expensive car', 'They were expensive']\n",
      "['No']\n",
      "['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [01:04<28:40, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A cop', 'a cop', 'Police officer']\n",
      "['to and from school']\n",
      "['yes']\n",
      "['walk in and to the bus stop', 'walk home from the bus stop', 'walk to the bus stop']\n",
      "['No']\n",
      "['Yes']\n",
      "['story time']\n",
      "['right before bedtime', 'before bedtime']\n",
      "['she thought something might be wrong']\n",
      "['no', 'No']\n",
      "['No']\n",
      "['that she was upset', 'she was upset']\n",
      "['yes', 'Yes']\n",
      "['that everything would be okay', 'that he was sure everything would be okay', 'that he was sure that everything would be okay', 'he was sure that everything would be okay']\n",
      "['her teacher']\n",
      "['no', 'No']\n",
      "['the dentist', \"Quinton's mother\", 'Quinton', 'her mother']\n",
      "['the dentist', 'to the dentist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [01:23<29:15, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tomorrow']\n",
      "['Five']\n",
      "['New York', 'New York City']\n",
      "['New York']\n",
      "['yes']\n",
      "['Staten Island']\n",
      "['Conference House Park']\n",
      "['476,013', '476,010', '476,015']\n",
      "['No']\n",
      "['white', 'White']\n",
      "['the forgotten borough', '\"the forgotten borough\"', 'forgotten borough', 'The forgotten borough']\n",
      "['by inhabitants feel neglected by the city government', 'by inhabitants who feel neglected by the city government', 'by residents who feel neglected by the city government', 'because the city government feels neglected', 'by residents who feel neglected', 'residents feel neglected', 'residents feel neglected by the city government']\n",
      "['the East Shore', 'East Shore']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [01:37<26:15, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['St George, Tompkinsville, Clifton, and Stapleton', 'St. George, Tompkinsville, Clifton, and Stapleton', 'St. George, Tompkinsville, Clifton, and St. George Historic District, which feature large Victorian houses']\n",
      "['the morning', 'five in the morning', 'at five in the morning']\n",
      "['the weather forecast', 'weather', 'the weather']\n",
      "['yes', 'Yes']\n",
      "['firefighter', 'Fireman', 'Firefighter']\n",
      "['Yes']\n",
      "['The flashlight', 'flashlights', 'A flashlight', 'Flashlights', 'flashlight']\n",
      "['R.J.']\n",
      "['Joel', 'His son']\n",
      "['Glass, wood, and plaster']\n",
      "['Yes']\n",
      "['R.J.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [01:52<25:19, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The light of his father's flashlight\", 'The light of his flashlight']\n",
      "['Gary Giordano', 'Giordano']\n",
      "['Gaithersburg']\n",
      "['Montgomery County']\n",
      "['Alabama']\n",
      "['Aruban jail', 'in an Aruban jail']\n",
      "['he is being held in an jail', 'he is being held in an Aruba jail', 'He is being held in an Aruban jail', 'he is currently being held in an Aruban jail', 'he is being held in an Aruban jail']\n",
      "['FBI']\n",
      "['15']\n",
      "['Philip Celestini', 'Taco Stein']\n",
      "['Monday']\n",
      "['eight more days']\n",
      "['Robyn Gardner']\n",
      "['Baby Beach', 'near Baby Beach', 'baby beach']\n",
      "['swimming']\n",
      "['Robyn Gardner', 'with Gardner', 'Gardner']\n",
      "['no']\n",
      "['no', 'No']\n",
      "['50']\n",
      "['August 5,', 'August 5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [02:17<29:05, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unknown']\n",
      "['India']\n",
      "['India']\n",
      "['30 feet tall', '30 feet', 'Three or four feet tall', 'three or four feet', 'Three feet', 'Three or four feet tall.', 'Three or four feet']\n",
      "['they lost nearly all of its healthy qualities', 'it loses nearly all of its healthy qualities', 'They lose nearly all of its healthy qualities', 'they lose nearly all of its healthy qualities', 'it lost nearly all its healthy qualities', 'it lost all its healthy qualities', 'it loses nearly all its healthy qualities', 'it loses all of its healthy qualities']\n",
      "['prevents heart disease', 'prevent heart disease', 'it prevents heart disease']\n",
      "['by accident.', 'by accident']\n",
      "['Shen Nong']\n",
      "['about 2737 B.C', '2737 B.C', '2737', '2737 B.C.']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [02:29<25:19, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he drank several cups of hot water daily', 'he drank several cups of hot water']\n",
      "['Der Spiegel']\n",
      "['Germany']\n",
      "['posing over the bodies of dead Afghans', 'posing over dead Afghans']\n",
      "['sprawled over a patch of sand and grass', 'it was sprawled over a patch of sand', 'a bloody body sprawled over a patch of sand and grass', 'it was laid over a patch of sand and grass', 'it was laid on a patch of sand and grass', 'Sprawled over a patch of sand and grass']\n",
      "['soldiers kneeling by a bloody body', 'soldiers kneeling by a bloody body sprawled over a patch of sand', 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass.', 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass']\n",
      "['a post in front of a military vehicle']\n",
      "['weapons', 'a weapon']\n",
      "['Jeremy Morlock']\n",
      "['Pfc. Andrew Holmes', 'Andrew Holmes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [02:39<22:00, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['murder', 'the premeditated deaths of three civilians', 'premeditated deaths of three civilians', 'premeditated deaths']\n",
      "['Mayweather and Manny Pacquiao', 'Mayweather and Pacquiao', 'Floyd Mayweather and Manny Pacquiao']\n",
      "['The Money Man', '\"The Money Man\"']\n",
      "['TBE']\n",
      "['The Money Man']\n",
      "['The Money Team']\n",
      "['CEO of Team Sauerland']\n",
      "['5.65 million', 'nearly five million', 'Five million']\n",
      "['$300 million']\n",
      "['47', '47-0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [02:50<20:16, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"they're both playing their roles\", \"they're both playing roles\"]\n",
      "['OCLC Online Computer Library Center, Incorporated', 'OCLC Online Computer Library Library Center', 'OCLC Online Computer Library Center']\n",
      "['OCLC Online Library Center', 'OCLC Online Computer Library Center']\n",
      "['1967']\n",
      "['no']\n",
      "['Ohio College Library Center']\n",
      "['the Ohio College Library Center', 'Ohio College Library Center', 'College Library Center']\n",
      "['Frederick G. Kilgour']\n",
      "['Yes']\n",
      "['Librarian', 'librarian', 'He is a librarian']\n",
      "['WorldCat']\n",
      "['July 5, 1967']\n",
      "['Ohio State University', 'on the campus of the Ohio State University', 'the campus of the Ohio State University', 'campus of the Ohio State University', 'the Ohio State University']\n",
      "['the Alden Library', 'Alden Library']\n",
      "['Ohio University']\n",
      "['streamline operations, control costs, and increase efficiency', 'streamline operations']\n",
      "['August 26, 1971']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [03:08<22:01, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'Yes']\n",
      "['yes', 'Yes']\n",
      "['a sweater', 'A sweater']\n",
      "['$15', '$15.', '$10']\n",
      "['No']\n",
      "[\"It's only $10.\", \"It's only $15.\", 'It looks like diamonds', 'It looks like diamonds.']\n",
      "['Summer.', 'summer', 'Summer']\n",
      "['$15', '$15.']\n",
      "['No']\n",
      "['A pen.', 'A pen', 'Pen', 'Pen and pencils']\n",
      "[\"It's only $10.\", \"It's only $15.\", \"It's $15\", \"It's only $15\", \"It's only $10\"]\n",
      "['summer', 'Summer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [03:19<19:53, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$15']\n",
      "['by a big lake']\n",
      "['two little mice']\n",
      "['lots of toy boats', 'toy boats', 'a toy boat', 'a big toy boat']\n",
      "['yes']\n",
      "['little mouse and Mary', 'little mice', 'Mary, one of the little mice', 'little mice', 'mice']\n",
      "['the water', 'from the water']\n",
      "['looking at the water', 'look at the water']\n",
      "['swimming', 'swimming and splashing in the water', 'swimming and splashing', 'splashing in the water']\n",
      "['threw a ball', 'threw a ball into the water']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [03:29<18:17, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['landed by the toy boat', 'it landed by the toy boat']\n",
      "['He had been bombed', 'It had been bombed', 'it had been bombed', 'he had been bombed', 'It was bombed', 'bombed']\n",
      "['No.', 'No']\n",
      "['Germany.', 'Germany']\n",
      "['Eastern', 'Eastern.']\n",
      "['Western Germany', 'Western Germany.']\n",
      "['No.']\n",
      "['No.', 'No']\n",
      "['No.', 'No']\n",
      "['Yes', 'Yes.', 'No.', 'No']\n",
      "['Return to his home.', 'Go back into his home.', 'He returned to his home.', 'He returned to his home', 'Go back to his home.']\n",
      "['Yes']\n",
      "['A taxi-driver', 'taxi-driver']\n",
      "['Yes']\n",
      "['Hans Bussman', 'Franz Bussman']\n",
      "['No']\n",
      "['No', 'Yes']\n",
      "['He was thought to have been killed in action', 'He was thought to have been killed in action.', 'He was thought to have been killed', 'He was thought to have been killed twenty years ago.', 'He was thought to have been killed.']\n",
      "['Mrs. Bussman', 'Mrs. Busman', 'Mrs. Busman and her husband.']\n",
      "['He might be his brother.', 'He thought there was a chance that he might be his brother', 'He thought he might be his brother.', 'He thought his brother might be his brother.', 'He thought there was a chance in a million that he might be his brother.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [03:50<21:27, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No']\n",
      "['ard']\n",
      "['riel', 'riel_']\n",
      "['What was the name of the island?', 'What was the name of the vessel?', 'What was the name of the ship?']\n",
      "['s']\n",
      "['What color was it?']\n",
      "['What was the new name for himself?', 'What was the name of the new name?', 'What was the new name for him?', 'What was the new name?']\n",
      "['a', 'eward', 'eward coast', 'eward coast of Malaita']\n",
      "['']\n",
      "NO PREDICTION!!\n",
      "['sband']\n",
      "['', 'rangi_.', 'rangi_', 'rangi']\n",
      "NO PREDICTION!!\n",
      "['back to Tulagi', 'to Tulagi', 'we get back to Tulagi']\n",
      "['sband']\n",
      "['What was the name of the ship that was named after the boat?', 'What was the name of the boat that was in the lagoon?', 'What was the name of the boat?']\n",
      "[\"Riggs,'\", 'Riggs', \"Riggs,' or 'Mademoiselle de Maupin,' or 'Judas,' or 'Haman.'\"]\n",
      "[\".'\", \"oiselle de Maupin,'\", \"oiselle de Maupin'\", \"oiselle de Maupin,' or 'Judas,'\"]\n",
      "[\"'\", \",'\", \".'\"]\n",
      "['What was the name of the boat that was on the lagoon?', 'What was the name of the ship that was in the water?', 'What was the name of the boat that was near the lagoon?', 'What was the name of the ship that was in the sea?', 'What was the name of the ship that had been in the lagoon?', 'What was the name of the ship that was in the lagoon?', 'What was the name of the boat that was near the lagoon?', 'What was the name of the boat that was near the shore?', 'What was the name of the boat she was sailing on?', 'What was the name of the ship that was in the water?', 'What was the name of the ship that was in the sea?', 'What was the name of the boat she was sailing on?', 'What was the name of the ship she was sailing on?', 'What was the first name of the ship?']\n",
      "['What was the name of the island?']\n",
      "['walled inlet of the outer reef', 'walled inlet of the mainland', 'walled inlet', 'mangrove swamp of the mainland', 'walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life.', 'walled inlet of the outer reef and every mangrove', 'walled inlet of the outer reef', 'walled inlet of the outer reef and every mangrove swamp of the mainland', 'walled inlet of the mainland', 'walled inlet of the outer reef and every mangrove swamp of the mainland that looked promising of cannibal life']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [04:16<25:37, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What was the name of the ship that was in the lagoon?', 'What was the name of the ship that was in the sea?']\n",
      "['Brownie and Spotty']\n",
      "['every day']\n",
      "['loved each other', 'they loved each other']\n",
      "['between their houses', 'their respective houses', 'between their respective houses']\n",
      "['neighbor dog', 'neighbor dogs']\n",
      "['Brownie']\n",
      "['yes', 'Yes']\n",
      "['yes']\n",
      "['a deserted spot half a mile from the house', 'a deserted spot']\n",
      "['he followed Ted around', 'he followed Ted about', 'He followed Ted']\n",
      "['led him to a nearby empty lot and back', 'led him to a nearby empty lot', 'ran toward a nearby empty lot and back']\n",
      "['keep his spirits up', 'kept his spirits up']\n",
      "['yes', 'Yes']\n",
      "['a path through the field between their houses', 'a path through the grass of the field', 'a path through the grass of the field between their houses']\n",
      "['yes', 'Yes']\n",
      "['Yes']\n",
      "['no', 'No']\n",
      "['yes', 'Yes']\n",
      "['no', 'No']\n",
      "['he was still missing', 'he was missing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [04:41<28:02, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Follow me! It's urgent!\", '\"It\\'s urgent!\"', '\"Follow me! It\\'s urgent!\"']\n",
      "['A young girl and her dog', 'a young girl and her dog', 'a girl and her dog']\n",
      "['walking.', 'They were walking', 'They were walking.', 'walking']\n",
      "['Into the woods.', 'Into the woods']\n",
      "['a little scared', 'scared', 'She was scared']\n",
      "['very interested', 'interested']\n",
      "['very interested', 'interested']\n",
      "['In the bushes', 'What was in the bushes up ahead', 'What was in the bushes', 'in the bushes', 'bushes', 'what was in the bushes', 'the bushes', 'in what was in the bushes']\n",
      "['a brown bear', 'a small brown bear']\n",
      "['rested his head on his paws', 'He rested his head on his bear paws', 'rested his head on his bear paws']\n",
      "['yes', 'Yes']\n",
      "['very interested']\n",
      "['He then rested his head on his bear paws and went back to sleep', 'He rested his head on his bear paws']\n",
      "['rested his head on his paws', 'He rested his head on his bear paws', 'rested his head on his bear paws', 'He then rested his head on his bear paws', 'he rested his head on his bear paws']\n",
      "['no', 'No']\n",
      "['no', 'No']\n",
      "['the girl', 'The girl']\n",
      "['no', 'No']\n",
      "['Dark and cold', 'dark and cold']\n",
      "['yes', 'Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [04:57<25:56, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'Yes']\n",
      "['a 10-year-old boy fatally shot his father', 'A 10-year-old boy fatally shot his father', 'a boy fatally shot his father']\n",
      "[\"his mother's home\"]\n",
      "['no']\n",
      "[\"in the back seat of the man's Toyota 4-Runner\"]\n",
      "['when the shooting took place', 'when the shooting occurred', 'when the shooting occured']\n",
      "['3pm', 'about 3pm']\n",
      "['Rick James Lohstroh']\n",
      "['exited the back of the vehicle and continued to fire at the car', 'exited the back of the vehicle', 'he exited the back of the vehicle and continued to fire at the car', 'he exited the back of the vehicle', 'he exited the back seat of the vehicle']\n",
      "['no']\n",
      "['the University of Texas Medical Branch', 'University of Texas Medical Branch', 'at the University of Texas Medical Branch']\n",
      "['Memorial Herman Hospital']\n",
      "['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [05:11<23:53, 17.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a 7-year-old', 'the 7-year-old', \"the boy's mother\"]\n",
      "['Romola']\n",
      "['no', 'No']\n",
      "['no']\n",
      "['he was a clergyman', 'he was a priest', 'he was a heretic']\n",
      "['Romola']\n",
      "['Rome', 'in Rome']\n",
      "['in June', 'June']\n",
      "['a week', 'weeks']\n",
      "['the scene', 'Savonarola']\n",
      "['a conspiracy', 'the conspiracy', 'a revelation of the suspected conspiracy', 'a sign from Savonarola']\n",
      "['the lengthening sunny days', 'lengthening sunny days']\n",
      "['the Frate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20/100 [05:26<22:14, 16.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'Yes']\n",
      "['yes', 'Yes']\n",
      "['to Utah', 'Utah']\n",
      "['in Utah', 'Utah']\n",
      "['no']\n",
      "['a small apartment']\n",
      "['no']\n",
      "['her friends']\n",
      "['yes']\n",
      "['Three', 'three', 'Four']\n",
      "['the truck', 'a truck']\n",
      "['her dad']\n",
      "['yes']\n",
      "['yummy fast food', 'yummy']\n",
      "['yes']\n",
      "['a knock', 'a knock at the door']\n",
      "['a little girl']\n",
      "['to play']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [05:40<20:55, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she loved it', 'sad']\n",
      "['she fell on a ski slope', 'she fell']\n",
      "['Ski', 'Skiing', 'skiing']\n",
      "['Canada.', 'Canada']\n",
      "['Yes', 'Yes.']\n",
      "['Yes.']\n",
      "['About an hour.']\n",
      "['No.']\n",
      "['The Lenox Hill Hospital.', 'Lenox Hill Hospital.']\n",
      "['New York City.']\n",
      "['Lenox Hill Hospital.']\n",
      "['45.']\n",
      "['Actress.', 'Actor', 'Actress', 'A stage actress']\n",
      "['No.', 'No']\n",
      "['Liam Neeson']\n",
      "['Yes.']\n",
      "['Boys.', 'Boys']\n",
      "['Yes.']\n",
      "['Tony-winning stage actress']\n",
      "['Yes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [06:04<24:06, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She was a star', 'She was a film star', 'She was a star.', 'She was a stage actress.', 'As a stage actress', 'She was a stage actress']\n",
      "['His bark', 'his bark']\n",
      "['three months']\n",
      "['no']\n",
      "['Sammie']\n",
      "['a golden puppy', 'golden puppy']\n",
      "['no']\n",
      "['he was sad', 'he was very sad', 'he was very sad and tired']\n",
      "['yes']\n",
      "['Peter']\n",
      "['find a person']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [06:14<20:11, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['held him in her arms', 'he held her']\n",
      "['a cute alien dog', 'a cute dog']\n",
      "['false', 'False']\n",
      "['Three']\n",
      "['Ti, Dicky and their dog CJ7', 'Ti, his son, and CJ7', 'Dicky, Ti, and CJ7', 'Dicky, Ti and CJ7', 'Ti, Dicky and CJ7', 'Dicky, Dicky and CJ7']\n",
      "['no']\n",
      "['in the movie', 'in a movie']\n",
      "['no']\n",
      "['China']\n",
      "['Dicky']\n",
      "['no']\n",
      "['a doll', 'doll']\n",
      "['false']\n",
      "['in the trash', 'in a trash']\n",
      "['no']\n",
      "['Talk and do magic', 'talk and do magic']\n",
      "['o']\n",
      "['worker']\n",
      "['anuary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 24/100 [06:29<19:56, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['008']\n",
      "['art and crafts', 'art-and-crafts', 'art and craft']\n",
      "['recipe', 'recipes']\n",
      "['Heather Neroy', 'Neroy']\n",
      "['Southern California']\n",
      "['a stay-at-home mom', 'stay-at-home mom', 'stay at home mom', 'mom']\n",
      "['copied the link', 'copying the link', 'by copying the link', 'she copied the link']\n",
      "['e-mail', 'an e-mail']\n",
      "['no']\n",
      "['a new website called Pinterest', 'a website called Pinterest', 'Pinterest', 'a website calledPinterest']\n",
      "['no']\n",
      "['yes']\n",
      "['yes']\n",
      "['a Halloween board', 'Halloween board']\n",
      "['a shared color board', 'yes']\n",
      "[\"redecorating her daughter's bedroom\", \"her daughter's bedroom\"]\n",
      "[\"follow each other's boards\"]\n",
      "['re-pin']\n",
      "['yes']\n",
      "['neat to see what other moms are pinning', '\"It\\'s been really neat to see what other moms are pinning', \"it's been really neat to see what other moms are pinning\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [06:53<22:26, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Lord Earl', 'Sir Earl']\n",
      "['his traces', 'traces']\n",
      "['Get forward.', 'Get forward,', 'Get forward', 'get forward', 'to get forward']\n",
      "['No.', 'No']\n",
      "['orders to prepare to start with the hound', 'To prepare to start with the hound', 'Orders to prepare to start with the hound']\n",
      "['Three']\n",
      "['To prepare to start with the hound', 'prepare to start with the hound', 'Prepare to start with the hound', 'Start with the hound', 'Preparing to start with the hound', 'To start with the hound']\n",
      "['The earl and a large party of men-at-arms', 'A large party of men-at-arms']\n",
      "['A traitor', 'The traitor']\n",
      "['Where Bruce had slept', 'Where Bruce had slept the night before.', 'Where Bruce had slept the night before', 'where Bruce had slept', 'That Bruce had slept the night before']\n",
      "[\"He was determined that if under the dog's guidance the party came close up with Bruce, he would kill the dog and then try to escape by fleetness of foot\", \"He was determined that if under the dog's guidance the party came close up with Bruce\", \"He was resolved that if under the dog's guidance the party came close up with Bruce\", 'Yes']\n",
      "['the Hound', 'Hound', 'The Hound']\n",
      "['yes', 'Yes']\n",
      "['Yes']\n",
      "['by fleetness of foot', 'Fleetness of foot', 'By fleetness of foot']\n",
      "['Yes']\n",
      "['There are so many mounted men in the party', 'because there were so many mounted men in the party', 'He had so many mounted men in the party', 'There were so many mounted men in the party']\n",
      "['He sniffed', 'He sniffed here and there', 'He sniffed there and there', 'sniffed here and there']\n",
      "['A loud bay', 'A loud deep bay', 'a loud deep bay']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [07:15<23:39, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pembroke and his knights']\n",
      "['Bartolome Ramon Pineda Ortiz', 'Brig. Gen. Bartolome Ramon Pineda Ortiz', 'Cibar Benitez', \"the president's office\"]\n",
      "['Paraguay']\n",
      "['yes']\n",
      "['Fernando Lugo', 'President Fernando Lugo']\n",
      "['the top brass']\n",
      "['Bartolome Ramon Pineda Ortiz', 'Brig. Gen. Bartolome Ramon Pineda Ortiz']\n",
      "['yes']\n",
      "['Brig. Gen. Hugo Gilberto Aranda Chamorro and Rear Adm. Egberto Emerito Orie Benegas took over the top posts at the air force and navy', 'Brig. Gen. Hugo Gilberto Aranda Benegas', 'Brig. Gen. Hugo Gilberto Aranda Chamorro', 'Brig. Gen. Hugo Gilberto Aranda Chamorro and Rear Adm. Egberto Orie Benegas', 'Brig. Gen. Hugo Gilberto Aranda Chamorro and Rear Adm. Egberto Emerito Orie Benegas', 'Hugo Gilberto Aranda Chamorro']\n",
      "['Brig. Gen. Hugo Gilberto Aranda Chamorro', 'Hugo Gilberto Aranda Benegas', 'Hugo Gilberto Aranda Chamorro', 'Brig. Gen. Hugo Gilberto Aranda']\n",
      "['yes']\n",
      "['Benitez', 'Bartolome Benitez']\n",
      "['1989']\n",
      "['yes']\n",
      "['1999 and 2000', '1996 and 2000', '2000 and 1999', '2000']\n",
      "['Three', 'three']\n",
      "['bishop', 'Catholic bishop', 'a bishop', 'a Catholic bishop']\n",
      "['yes']\n",
      "['no']\n",
      "['no', 'shock', 'it was a shock']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [07:40<25:37, 21.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he has struggled to push reforms through a majority-opposition legislature', 'he has tried to push reforms through a majority-opposition legislature', 'he has struggled to push reforms through a majority-oppopposition legislature', 'he has struggled to push through reforms through a majority-opposition legislature', 'he has struggled to push reforms through a majority-opposition legislature.', 'has struggled to push reforms through a majority-opposition legislature', 'struggled to push reforms through a majority-opposition']\n",
      "['a dress']\n",
      "['to her party', 'to wear to her party']\n",
      "['es']\n",
      "['er dad', 'er mom', 'er parents']\n",
      "['']\n",
      "NO PREDICTION!!\n",
      "['ug']\n",
      "['in the morning']\n",
      "['se it was opposite day', 's opposite day']\n",
      "['what did they buy instead?', 'what did they buy?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [07:47<20:21, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['steaks', 'what did they get a milkshake for?', 'vanilla milkshake and went to more stores', 'vanilla milkshake', 'did they get a vanilla milkshake?']\n",
      "['Buckinghamshire']\n",
      "['south east', 'South East England', 'in South East England', 'South East']\n",
      "['Greater London']\n",
      "['Oxfordshire', 'Berkshire']\n",
      "['Northamptonshire']\n",
      "['Northamptonshire']\n",
      "['Northamptonshire']\n",
      "['Aylesbury']\n",
      "['home counties']\n",
      "['no', 'No']\n",
      "['the Metropolitan Green Belt', 'The Metropolitan Green Belt.', 'Metropolitan Green Belt', 'The Metropolitan Green Belt']\n",
      "['the Metropolitan Green Belt', 'Metropolitan Green Belt']\n",
      "['Yes', 'Yes.']\n",
      "['Milton Keynes']\n",
      "['the northeast', 'in the northeast']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [08:02<19:15, 16.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the Conservative Party', 'The Conservative Party', 'Conservative Party', 'Conservative']\n",
      "['Pierre']\n",
      "['yes']\n",
      "['to look after the horses']\n",
      "['twelve miles away', 'twelve miles', '12 miles away']\n",
      "['four hours']\n",
      "['no']\n",
      "['The landlady', 'the landlady']\n",
      "['yes']\n",
      "['no']\n",
      "['it was very poor', 'very poor']\n",
      "['yes']\n",
      "['yes']\n",
      "['yes']\n",
      "['a party']\n",
      "['no']\n",
      "['yes']\n",
      "['yes']\n",
      "['that he wished to speak to him', 'that he had spoken to him']\n",
      "['an honest man', 'he is an honest man', 'honest']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30/100 [08:23<20:46, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre']\n",
      "['high upland and wide valleys', 'high downland and wide valleys', 'its high downland and wide valleys']\n",
      "['in South West England', 'South West England']\n",
      "['no']\n",
      "['the counties of Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire and Berkshire', 'Dorset, Somerset, Hampshire, Gloucestershire, and Oxfordshire', 'Dorset, Somerset, Hampshire, Gloucestershire, Oxfordshire and Berkshire', 'Dorset, Somerset, Hampshire, Gloucestershire, Berkshire']\n",
      "['Wilton', 'Wiltonshire']\n",
      "['Trowbridge']\n",
      "['its mediaeval cathedral', 'it is famous for its mediaeval cathedral', 'it is notable for its mediaeval cathedral', 'its mediaeval cathedral.', 'for its mediaeval cathedral']\n",
      "['yes']\n",
      "['Longleat']\n",
      "['Warminster']\n",
      "['Stourhead', \"the National Trust's Stourhead\", 'Stourh']\n",
      "['near Mere', 'Mere', 'near Mere.']\n",
      "['as \"Wiltunscir\"', 'Wiltunscir', '\"Wiltunscir\"']\n",
      "['Pre-Roman', 'pre-Roman archaeology', 'pre-Roman']\n",
      "['Stonehenge and Avebury stone circles', 'Stonehenge and Avebury']\n",
      "['the Battle of Bedwyn', 'Battle of Bedwyn', 'The Battle of Bedwyn']\n",
      "['Escuin, a West Saxon nobleman', 'Escuin,', 'Escuin']\n",
      "['a West Saxon nobleman', 'West Saxon nobleman']\n",
      "['The Danes invaded the county.', 'the Danes invaded the county', 'The Danes invaded the county']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [08:47<22:32, 19.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large areas of the country came into the possession of the crown and the church', 'large areas of the country came into the possession of the crown', 'large areas of the country came into the possession of the crown and the church.']\n",
      "['Male', 'male', 'a male']\n",
      "['A male.']\n",
      "['Marsha.', 'Marsha']\n",
      "['Her mom', 'Her mom.']\n",
      "['To get him safe and sound at all times.', 'To make sure Joey was safe and sound at all times.', 'To be safe and sound at all times.', 'To make sure Joey is safe and sound at all times.']\n",
      "['Tuesday.', 'On one Tuesday.', 'On Tuesday.']\n",
      "[\"Marsha's favorite dinner was spaghetti\", \"Marsha's favorite dinner.\", \"Marsha's favorite dinner is spaghetti.\", \"Marsha's favorite dinner was spaghetti.\"]\n",
      "['No.']\n",
      "['He gets a little dried out.', 'He gets dried out.', 'Every few days.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [08:55<18:13, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes.']\n",
      "['Jane Humphreys', 'Jane Humphrey']\n",
      "['a book of Parthenissa', 'Parthenissa', 'a volume of Parthenissa']\n",
      "['disappointment']\n",
      "['Portchester', 'at Portchester']\n",
      "['no']\n",
      "['disappointment']\n",
      "['with the guests of royalty', 'no']\n",
      "['to her companions', 'her companions or anything around her', 'her companions or', 'her companions', 'to her companions or anything around her']\n",
      "['Hester', 'Hester Bridgeman']\n",
      "['no']\n",
      "['no']\n",
      "['Three', 'three']\n",
      "['no']\n",
      "['an attorney']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [09:15<19:06, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MACHINATIONS']\n",
      "['Tom Rover']\n",
      "['Stanley']\n",
      "['Stanley', 'Sam']\n",
      "['\"What did you do to him?\"', '\"What did he do to him?', '\"What did he do to him?\"', 'What did he do to him?']\n",
      "['put an advertisement of pills on his back', 'put an advertisement of pills on his back and some other ads', 'put an advertisement on his back', 'Put an advertisement of pills on his back', 'put an advertisement of pills on his back and some other ads.', 'Put an advertisement of pills on his back and some other ads.']\n",
      "['Put an ad on his back', 'An advertisement of pills on his back', 'put an advertisement of pills on his back', 'An advertisement of pills', 'Put an advertisement of pills on his back']\n",
      "['yes', 'Yes']\n",
      "['yes']\n",
      "['shaking his other fist', 'shaking his fist']\n",
      "['shaking his fist wrathfully', 'shaking his other fist', 'holding his fist', 'shaking his fist', 'holding the poster Tom had fastened on his back', 'holding the poster', 'shaking his fist wrathfully', 'shaking his other fist', 'shaking his other fist wrathfully']\n",
      "['yes']\n",
      "['he was so full of wrath', 'he was so full of wrath he could not speak']\n",
      "['\"Take a cough drop and clear your throat Billy,\"', 'Take a cough drop and clear your throat,\"', '\"Take a cough drop and clear your throat Billy\"', '\"Take a cough drop and clear your throat,\"', 'Take a cough drop and clear your throat', \"'Take a cough drop and clear your throat Billy\", 'Take a cough drop and clear your throat Billy']\n",
      "[\"you've humiliated me before the whole class!\", 'You\\'ve humiliated me before the whole class! I\\'ll--I\\'ll----\"', \"You've humiliated me before the whole class!\", 'you\\'ve humiliated me before the whole class! I\\'ll--I\\'ll----\"', \"he'd humiliated me before the whole class!\", \"you've humiliated me before the whole class\", 'he\\'ll--I\\'ll--I\\'ll----\"', 'he\\'ll--I\\'ll--I\\'ll--I\\'ll----\"', 'he said he\\'ll--I\\'ll----\"', \"he said he would--I'll----\", 'he\\'ll--I\\'ll----\"', \"he'll--I'll--I'll--I'll--\"]\n",
      "['take a cough drop and clear your throat Billy', 'take a cough drop and clear your throat', '\"Take a cough drop and clear your throat Billy,\"', 'Take a cough drop and clear your throat', 'Take a cough drop and clear your throat Billy,\"', 'Take a cough drop and clear your throat Billy']\n",
      "[\"Gumley's Red-Pills for Red-Blooded People\", \"Gumley's Red Pills for Red-Blooded People\", \"Gumley's Red Pills\"]\n",
      "[\"They are fine, Willie. Didn't you ever take 'em?\", \"they are fine, Willie. Didn't you ever take 'em?\", 'Say, they are fine, Willie.', 'They are fine, Willie.', 'They are fine, Willie. Didn\\'t you ever take \\'em?\"', \"They are fine, Willie. Didn't you ever take them?\", 'They are fine, Willie', 'they are fine, Willie.']\n",
      "['he was flat on his back with half a dozen fatal diseases.', 'he was flat on his back with half a dozen fatal diseases', 'he got a man over in Rottenberg who was flat on his back with half a dozen fatal diseases']\n",
      "['three days', 'three days,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 34/100 [09:43<22:36, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['cats', 'Cats']\n",
      "['8', '7']\n",
      "['Male']\n",
      "['1']\n",
      "['Brendan', 'cats']\n",
      "['special treats']\n",
      "['Three', 'three', '3']\n",
      "['because he likes them', 'because he loves them']\n",
      "['chips and cake', 'chips and cake and candy']\n",
      "[\"they aren't good for cats\", \"those foods aren't good for cats\", \"because they're not good for cats\", \"those aren't good for cats\", 'those are not good for cats', \"because those foods aren't good for cats\", \"Because they're not good for cats\", \"because they aren't good for cats\"]\n",
      "['ball of paper', 'chase balls of paper', 'balls of paper']\n",
      "['Brendan']\n",
      "['orange, black, and white', 'orange and black', 'white and black', 'orange, black, and spotted', 'white, black, and red']\n",
      "['Snowball', 'snowball']\n",
      "['no', 'No']\n",
      "['female']\n",
      "['Snowball']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 35/100 [09:58<20:23, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scruff', 'Scruffy']\n",
      "['a manuscript']\n",
      "['wiki-wiki', '\"Wik-wiki.\"', 'A wiki-wiki', \"'Wik-wiki.'\", 'Wik-wiki', \"'Wiki-wiki.'\"]\n",
      "['no', 'No']\n",
      "['In the Hawaiian Islands', 'Hawaii', 'In Hawaii']\n",
      "['Three', 'Four', 'Two']\n",
      "['No']\n",
      "[\"It's true\"]\n",
      "['Maria']\n",
      "['To see if he would be at their table', \"To see if he'd be at their table\", 'To see if he was at dinner', \"To make sure he wouldn't be at the table\", 'To see if he was at the table', 'To make certain whether or not he would be at their table']\n",
      "['No']\n",
      "['Yes']\n",
      "['Yes']\n",
      "['Martin']\n",
      "['at the gate', 'At the gate']\n",
      "['Maria']\n",
      "['No']\n",
      "['Martin']\n",
      "['No']\n",
      "['No']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 36/100 [10:20<20:57, 19.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's too strong\", \"Because it's too strong\"]\n",
      "['from his home in Dubai', 'his home in Dubai', 'McLaren']\n",
      "['because of a high impact', 'from his home in Dubai', 'he was concussed and airlifted to hospital']\n",
      "['high speed', 'high speed crash']\n",
      "['February 22']\n",
      "[\"Barcelona's Circuit de Catalunya\"]\n",
      "['the McLaren simulator', 'McLaren simulator', 'McLaren']\n",
      "['a comeback', 'his comeback', 'comeback']\n",
      "['Lewis Hamilton and his mighty Mercedes', 'Lewis Hamilton and his Mercedes']\n",
      "['world champion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 37/100 [10:30<17:45, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McLaren']\n",
      "['Ben and Johnny']\n",
      "['he was suspicious', 'he was a suspicious sort of a person', 'that he was a suspicious sort', 'that he was a suspicious sort of a person']\n",
      "['a suspicious sort of a person', 'a sort of a suspicious sort of a person']\n",
      "['Boarding-house', 'a regular boarding-house', \"Master Spry's misfortune\"]\n",
      "['yes']\n",
      "['five to ten dollars per week', 'five dollars per week']\n",
      "['ten dollars per week', 'three dollars per week', 'at various sums from five to ten dollars per week', '10 dollars per week', 'five to ten dollars per week']\n",
      "[\"Master Spry's misfortune\", \"Master Spry's misfortune and Tim's perfidy\", \"Master Spry's misfortune and Tim Dooley's perfidy\"]\n",
      "['yes']\n",
      "['they were to start a regular theatre', 'that they were to start a regular theatre, and had already engaged a hall', 'that Johnny had told them that they were to start a regular theatre', 'they were to start a regular theatre, and had already engaged a hall', 'that Johnny himself had told him that they were to start a regular theatre']\n",
      "['they were to start a regular theatre, and had already engaged a hall', 'they were to start a regular theatre', 'They were to start a regular theatre,', 'They were to start a regular theatre']\n",
      "['that they were to start a regular theatre, and had already engaged a hall', 'that Johnny had told him that they were to start a regular theatre', 'that Johnny himself had told him that they were to start a regular theatre, and had already engaged a hall', 'that Johnny himself had told him that they were to start a regular theatre, and had already engaged a hall,', 'that they were to start a regular theatre, and had already engaged a hall,', 'that Johnny himself had told him that they were to start a regular theatre']\n",
      "['yes']\n",
      "['no']\n",
      "['engaged a hall', 'he had already engaged a hall', 'had engaged a hall', 'he had engaged a hall', 'He had already engaged a hall']\n",
      "['yes']\n",
      "['it would be converted into a first-class place of amusement', 'would have to be converted into a first-class place of amusement', 'would be converted into a first-class place of amusement', 'they would be converted into a first-class place of amusement', 'would it be converted into a first-class place of amusement']\n",
      "['as soon as possible']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 38/100 [10:54<19:35, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a rumor', 'it was not a rumor', 'it would be regarded as a rumor', 'it was regarded as a rumor', 'it would have been regarded as a rumor', 'it would be regarded simply as a rumor', 'it was a rumor']\n",
      "['basketball', 'Basketball']\n",
      "['the NBA players', 'NBA players', 'the NBA players they watch on TV']\n",
      "[\"Jared's\"]\n",
      "['Jared', 'Adam']\n",
      "['they get to play on a real team', 'They get to play on a basketball team', 'They all get to play on a real team', 'they all get to play on a basketball team', 'they play on a basketball team', 'They play on a basketball team', 'they get to play on a basketball team']\n",
      "['at their school', 'their school']\n",
      "['practicing a lot', 'practicing']\n",
      "['every day', 'almost every day']\n",
      "['Adam', 'Ryan']\n",
      "['birthday', 'his birthday']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 39/100 [11:02<16:05, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alex and Brady']\n",
      "['no', 'No']\n",
      "['They were too yellow', 'They were too yellow.']\n",
      "['Green.', 'Green']\n",
      "['Yes', 'Yes.']\n",
      "['Yes.']\n",
      "['Clovers and leaves and vines.', 'Grass and clovers and vines.', 'Grass and clovers.', 'grass and clovers.', 'Grass and clovers and leaves.', 'Grass and clovers and leaves and vines.']\n",
      "['Sean', 'Sean.']\n",
      "['No.']\n",
      "['No.']\n",
      "['Zarah and Zarah.', 'Zarah.', 'Zarah']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 40/100 [11:13<14:12, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A piece of green in a necklace.', 'A piece of green in a golden necklace.']\n",
      "['Miss Baldwin', 'Miss Baldwin Wingate']\n",
      "['Miss Baldwin', 'Baldwin']\n",
      "['no', 'No']\n",
      "['On the following evening', 'on the following evening', \"after eleven o'clock on the following evening\"]\n",
      "['yes']\n",
      "['No']\n",
      "['Josephine']\n",
      "['easy-chair', 'Easy-chair', 'a easy-chair']\n",
      "['Domestic ructions', 'domestic ructions']\n",
      "['After dinner', 'After breakfast', 'after dinner']\n",
      "['Yes']\n",
      "['Wilshaw']\n",
      "['half a sovereign every time I leave the cab', 'at least half a sovereign every time I leave the cab', 'half a sovereign every time he leaves the cab', 'at least a sovereign every time I leave the cab', 'half a sovereign', 'a quarter a sovereign every time', 'a sovereign every time', 'half a sovereign every time']\n",
      "['when he leaves the cab', 'every time I leave the cab', 'when I leave the cab', 'every time']\n",
      "['yes', 'Yes']\n",
      "['Jimmy']\n",
      "['anywhere']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 41/100 [11:34<16:01, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandwiches']\n",
      "['2013']\n",
      "['Pope Emeritus']\n",
      "['April', 'April 27', 'April 16', '16 April']\n",
      "['Aloisius Ratzinger', 'Joseph Aloisius', 'Joseph Aloisius Ratzinger']\n",
      "['Pope Benedict', 'Pope Benedict XVI']\n",
      "['professor', 'a professor', 'priest']\n",
      "['no', 'No']\n",
      "['professor', 'a professor']\n",
      "['five']\n",
      "['Bavaria']\n",
      "['John Paul II', 'Pope John Paul II']\n",
      "['1926', '1927']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 42/100 [11:46<14:24, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aloisius Ratzinger', 'Ratzinger']\n",
      "[\"at Aaron Poole's home\", \"Aaron Poole's home\"]\n",
      "['he was in his right mind', 'He was in his right mind then']\n",
      "['Three']\n",
      "['Nat Poole, Dad, and Dave', 'Nat Poole, Dad', 'Nat Poole, Dave and Dave', 'Dave, Nat Poole and Laura', 'Nat Poole and Dave', 'Nat, Dave and Dave', 'Nat Poole, Dave, and Laura', 'Dave, Laura, and Roger', 'Dave and Roger', 'Dave, Roger, and Laura', 'Dave, Ben, Roger']\n",
      "['no', 'No']\n",
      "['no']\n",
      "['telegram', 'his telegram']\n",
      "['Aaron', 'to Aaron']\n",
      "['set off on a hunt', 'set off a hunt for the wild man', 'set off on a hunt for the wild man', 'they set off on a hunt for the wild man']\n",
      "['for the wild man', 'the wild man']\n",
      "['no']\n",
      "['yes']\n",
      "['a telegram']\n",
      "['tomorrow']\n",
      "['Dave']\n",
      "[\"he's tired out\"]\n",
      "['yes']\n",
      "[\"they'll have to give up\", \"they'll have to give it up\", 'give up']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 43/100 [12:08<16:16, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bear Camp', 'see Bear Camp', 'get to Bear Camp']\n",
      "['information from various perspectives', 'information']\n",
      "['transition', 'transition period', 'transition period', 'transitional period', 'a transitional period']\n",
      "['the preparation of children for adult roles', 'to prepare children for adult roles', 'preparation of children for adult roles']\n",
      "['transition']\n",
      "['hormones']\n",
      "[\"a deeper voice and larger adam's apple\", \"a deeper voice and larger adam's apple in boys\", 'a deeper voice and larger apple', 'a deep voice and larger apple in boys', 'a deeper voice and larger apple in boys']\n",
      "['hormones']\n",
      "['the hormones', 'hormones']\n",
      "['hormones', 'the hormones']\n",
      "['a chain reaction', 'the chain reaction']\n",
      "['tissue responsiveness', 'tissue responsiveness or obesity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 44/100 [12:19<14:23, 15.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hormones', 'the hormones', 'testes', 'the testes']\n",
      "['Paul', 'Chet Winthrop']\n",
      "['talking']\n",
      "['two', 'Two']\n",
      "['Barnaby Winthrop']\n",
      "['They were moving against Captain Grady', 'moving against Captain Grady', 'Stealing', 'stealing']\n",
      "[\"he was a member of the horse thieves' gang\", 'he was a horse thief', \"he was a colored member of the horse thieves' gang\", 'the words uttered by Jeff Jones', 'they were interested in the words uttered by Jeff Jones', 'They were interested in the words uttered by Jeff Jones', 'he was interested in the words uttered by Jeff Jones', 'in the words uttered by Jeff Jones']\n",
      "['yes']\n",
      "['he was a dirty horse thief', 'he was a thief', 'he was a downright horse thief', 'he was a horse thief', 'he was a fellow horse thief']\n",
      "['no']\n",
      "['yes']\n",
      "['he was lynched', 'he said he would be lynched', 'by Jeff Jones']\n",
      "['no']\n",
      "['that he is a downright horse thief', 'he was a downright horse thief', 'that he was a downright horse thief', 'you are a downright horse thief', 'that you are a downright horse thief', 'he is a downright horse thief']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 45/100 [12:38<14:58, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he was lynched', 'he will be lynched', 'he should be lynched']\n",
      "['Bonn', 'The Federal City of Bonn']\n",
      "['in the Rhine-Rhine in the German state of North Rhine-Westphalia', 'on the banks of the Rhine in the German state of North Rhine-Westphalia', 'in the Rhine-Westphalia', 'in the German state of North Rhine-Westphalia', 'North Rhine-Westphalia']\n",
      "['in the German state of North Rhine-Westphalia', \"Germany's state of North Rhine-Westphalia\", 'North Rhine-Westphalia', 'Germany']\n",
      "['no']\n",
      "['in the southernmost part of the Rhine-Westphalia', 'in the southernmost part of the Rhine-Ruhr region', 'the southernmost part of the Rhine-Westphalia region', 'the southernmost part of the Rhine-Ruhr region', 'south-southeast of Cologne']\n",
      "['no', 'No']\n",
      "['over 11 million', '11 million']\n",
      "['Bonn', 'the Federal City of Bonn']\n",
      "['yes']\n",
      "['in the 1st century BC', '1st century BC', 'the 1st century BC']\n",
      "['a Roman settlement', 'as a Roman settlement', 'Roman settlement']\n",
      "['no']\n",
      "['yes']\n",
      "['Ludwig van Beethoven']\n",
      "['from 1770 to 1990', 'in 1770', '1770']\n",
      "['the city was temporarily seat of the Federal institutions', 'it was the temporary seat of the Federal institutions', 'the city was the temporary seat of the Federal institutions', 'the city was temporarily a temporary seat of the Federal institutions']\n",
      "['Bonn was the provisional capital', 'Bonn served as the temporary seat of the Federal institutions']\n",
      "['the President, the Chancellor, the Bundesrat and the primary seat of six federal government ministries and twenty federal authorities.', 'the President, the Chancellor, the Bundesrat and the federal authorities', 'the President, the Bundesrat and the Federal government ministries', 'the President, the Chancellor, the Bundesrat and the Bundesrat', 'the President, the Chancellor, the Bundesrat and the primary seat of six federal government ministries', 'the President, the Chancellor, the Bundesrat and the federal government', 'the President, the Chancellor, the Bundesrat and the Federal Council']\n",
      "['yes', 'Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 46/100 [12:57<15:34, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', 'No']\n",
      "['Aquila']\n",
      "['Masuccio Torri', '\"Masuccio Torri.\"']\n",
      "['Seven', 'six', 'Five', 'Six']\n",
      "['Seven', 'Six.', 'Five', 'Six']\n",
      "['\"Armed men, my lords!\"', '\"We are betrayed!\"']\n",
      "['We are betrayed!\"', '\"We are betrayed!\"', 'We are betrayed!']\n",
      "['Ferrabraccio.', 'Ferrabraccio']\n",
      "['His sword', 'his sword', 'his sword.']\n",
      "['No.']\n",
      "['He was our only hope.', 'He is our only hope.']\n",
      "['The only hope of Babbiano.', 'Babbiano.']\n",
      "['Gian Maria.', 'Gian Maria,', 'The throne of Gian Maria.', 'the throne of Gian Maria.', 'Gian Maria']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 47/100 [13:14<14:58, 16.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ferrabraccio.']\n",
      "['the ice', 'ice']\n",
      "['a bitter breeze', 'a little bitter breeze']\n",
      "['Dampier']\n",
      "['get the mainsail', 'to get the mainsail', 'get the mainsail on', 'get the mainsail on to her', 'got the mainsail on to her', 'to get the mainsail on', 'the mainsail', 'to get the mainsail on to her']\n",
      "['yes', 'Yes']\n",
      "['that boat', 'the boat', 'a boat']\n",
      "['white caps', 'a little warmth']\n",
      "['they were beginning to feel uneasiness', 'a growing uneasiness', 'uneasiness']\n",
      "['white caps']\n",
      "['the ice']\n",
      "['the _Selache_', 'the _Selache', 'Selache', 'the Selache']\n",
      "['it had become detached', 'it was detached']\n",
      "['it had become perceptibly narrower', 'it became perceptibly narrower']\n",
      "['the ice', 'the ice.']\n",
      "['yes', 'Yes']\n",
      "['Wyllard']\n",
      "['he sent a man up into the foremast shrouds', 'heave the man up into the foremast shrouds', 'sent a man up into the foremast shrouds']\n",
      "['everything clear']\n",
      "['Wyllard']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 48/100 [13:36<16:09, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he should get the cover off the second boat', 'Get the cover off the second boat', 'to get the cover off the second boat', 'get the cover off the second boat']\n",
      "['Multimedia', 'Multimedia devices', 'multimedia']\n",
      "['Video']\n",
      "['yes', 'Yes']\n",
      "['Bob Goldstein']\n",
      "['July 1966', '1966']\n",
      "['singer and artist Bob Goldstein', 'Bob Goldstein', 'Bobb Goldsteinn', 'a singer', 'singer and artist', 'a singer and artist']\n",
      "['at Southampton', 'Southampton']\n",
      "['Lightworks', '\"Lightworks at L\\'Oursin\"', \"Lightworks at L'Oursin\", \"Light Works at L'Oursin\", \"LightWorks at L'Oursin\"]\n",
      "['a combination of different content forms', 'content that uses a combination of different content forms', 'content that uses a combination of different content forms such as text, audio, images, animations, video and interactive content.', 'a mix of different content forms']\n",
      "['text, audio, images, animations, video and interactive content.', 'text, audio, images, videos and interactive content', 'text, audio, images, animations, videos, video and interactive content', 'text, audio, images, animations, video and interactive content']\n",
      "['media that uses only rudimentary computer displays', 'media that uses only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.', 'media that use only basic computer displays', 'media that uses only basic computer displays', 'media that use only rudimentary computer displays']\n",
      "['Multimedia', 'Multimedia devices']\n",
      "['the term \"rich media\"', '\"rich media\"', 'Rich media', 'rich media']\n",
      "['Dick Higgins', 'a American artist named Dick Higgins', 'of an American artist named Dick Higgins']\n",
      "['\"intermedia\"']\n",
      "['yes', 'Yes']\n",
      "['Bob Goldstein', 'Richard Albarino']\n",
      "['Variety', '\"Variety\"']\n",
      "['July 1966', '1966']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████▉     | 49/100 [13:58<16:38, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris Sawyer', 'David Sawyer']\n",
      "['Crooked Creek Company', 'the Crooked Creek Company']\n",
      "['George']\n",
      "['yes']\n",
      "['Harry.', 'Harry']\n",
      "['Yes.']\n",
      "['Hetertown']\n",
      "['about a week', 'Almost a week', 'About a week']\n",
      "['a week', 'about a week', 'A week', 'About a week']\n",
      "['George', 'Harry']\n",
      "['Yes.']\n",
      "['Yes', 'Yes.']\n",
      "['George', 'George.']\n",
      "['Yes.']\n",
      "['relatives', 'some relatives']\n",
      "['Yes.']\n",
      "['No.']\n",
      "['George']\n",
      "['Harry.', 'Harry']\n",
      "['Yes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 50/100 [14:21<17:09, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slowly', 'horse', 'horses', 'a horse', 'slowly toward him']\n",
      "['a doctor', 'doctor']\n",
      "['Royal Liverpool', 'Royal Liverpool hospital', 'at the Royal Liverpool hospital', 'the Royal Liverpool hospital']\n",
      "['Palestine']\n",
      "['kidney transplants', 'received kidney transplants', 'kidney transplant', 'transplant', 'he donated kidney', 'donated kidney']\n",
      "['two', 'Two']\n",
      "['One kidney', 'kidney and liver', 'kidney and kidney', 'kidney', 'kidney and a kidney']\n",
      "['family']\n",
      "['Mohammed Duhair, Nadia, 36, and Mohammed Duhair', 'Mohammed Duhair, Mohammed Duhair', 'Mohammed Duhair and Mohammed Duhair', 'Mohammed Duhair, Mohammed Duhair, Mohammed Duhair', 'Mohammed Duhair, 42', 'Mohammed Duhair, 42, and Mohammed Duhair, 36', 'Mohammed Duhair, Mohammed Duhair,', 'Mohammed Duhair, Mohammed Duhair, and Nadia', 'Mohammed Duhair, 42, and Nadia', 'Mohammed Duhair, Nadia, 36', 'Mohammed Duhair, Mohammed Duhair, Nadia, and Mohammed Duhair', 'Mohammed Duhair and Nadia', 'Mohammed Duhair, 42, and his brother']\n",
      "['six months', 'six days']\n",
      "['yes']\n",
      "['that the Gaza medical teams will eventually carry out kidney transplants', 'that Gaza medical teams will eventually carry out kidney transplants.', 'that Gaza medical teams will eventually carry out kidney transplans independently', 'that Gaza medical teams will eventually carry out kidney transplants independently', 'that Gaza medical teams will eventually carry out kidney transplantations independently', 'that Gaza medical teams will eventually carry out kidney transplants']\n",
      "['funding is a problem']\n",
      "['go back as volunteers to Gaza', 'go back as volunteers to Gaza for the next couple of years to do more transplants', 'go back as volunteers to Gaza for the next couple of years to do more kidney transplants', 'Go back as volunteers to Gaza for the next couple of years to do more transplants', 'go back as volunteers to Gaza for the next couple of years to do more transplants.\"', 'Go back as volunteers to Gaza for the next couple of years', 'Go back to Gaza']\n",
      "['May']\n",
      "['a fortnight ago']\n",
      "['within six months', 'six months', 'several months']\n",
      "['42']\n",
      "['42', '36', '37', '38']\n",
      "['36']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████     | 51/100 [14:50<18:48, 23.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'Yes']\n",
      "['Robinson Crusoe']\n",
      "['A great storm', 'a terrible storm', 'a great storm']\n",
      "['yes', 'Yes']\n",
      "['on an island', 'On an island', 'Rome']\n",
      "['No']\n",
      "['He built a house']\n",
      "['No']\n",
      "['A wild man']\n",
      "['\"Friday\"', 'Friday']\n",
      "['A servant', 'Served as a servant']\n",
      "['Robinson Crusoe']\n",
      "['Yes']\n",
      "['a boat', 'A boat']\n",
      "['Use our own hands to work hard', 'Use your own hands', 'use your own hands', 'Use your own hands to work hard']\n",
      "['Cry']\n",
      "['He wanted to be a seaman', 'be a seaman', 'To be a seaman']\n",
      "['travel around the world']\n",
      "['Rome']\n",
      "[\"Robinson's hands were used to work hard\", 'Robinson used his hands to work hard', 'Robinson Crusoe used his hands to work hard', 'Robinson used his hands', 'A cave']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 52/100 [15:11<17:56, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No']\n",
      "['CHAPTER VI', 'VI', 'VI.']\n",
      "['Indian club', 'the Indian club']\n",
      "['Leo']\n",
      "['a severe injury', 'A severe injury', 'A severe injury.']\n",
      "['a severe injury', 'it would have inflicted a severe injury']\n",
      "['a crowd of performers', 'A crowd of performers', 'crowd of performers']\n",
      "['Leo']\n",
      "['Snipper']\n",
      "['he sprang at Leo', 'He sprang at Leo']\n",
      "['no', 'No']\n",
      "['yes', 'Yes']\n",
      "['Two']\n",
      "['a stinging slap', 'A stinging slap', 'What did Leo do to the boy?', 'What did Leo do?']\n",
      "['he sprang at Leo', 'He sprang at Leo', 'Killing the boy', 'He was going to kill the boy', 'he was going to kill the boy']\n",
      "['“I’m going to teach the boy a lesson', '“I’m going to teach the boy a lesson!”', 'I’ll teach you a lesson!”', '“I’ll teach you a lesson!”', '“I’ll teach the boy a lesson!”', 'I’ll teach you a lesson!']\n",
      "['No']\n",
      "['He’s going to teach the boy a lesson!”', 'He’re going to teach the boy a lesson', 'He’s going to teach the boy a lesson', 'He’m going to teach the boy a lesson!”', 'He’d be going to teach the boy a lesson']\n",
      "['Teach the boy a lesson', 'to teach the boy a lesson', 'teach the boy a lesson']\n",
      "['the dressing tent', 'the grass', 'on the grass']\n",
      "['a slap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 53/100 [15:37<18:36, 23.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the Indian club was caught']\n",
      "['Zink Media', 'Zink Media, Inc', 'Zink Media, Inc.']\n",
      "['Portland, Oregon, US', 'Portland, Oregon,', 'Portland, Oregon']\n",
      "['30 August 2000']\n",
      "['8 million', 'over 8 million', 'over 1 million']\n",
      "['over 4.9 million', 'nearly 4.9 million', 'over 1 million']\n",
      "['nearly 4.9 million', 'nearly 346,000', 'over 1 million']\n",
      "['yes', 'Yes']\n",
      "['Kevin Lewandowski']\n",
      "['community-built sites', 'the success of community-built sites', 'the success of community-built sites such as Slashdot, eBay, and Open Directory Project']\n",
      "['electronic music']\n",
      "['yes']\n",
      "['in 2003', '2003', 'in 2004']\n",
      "['to support other genres']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 54/100 [15:54<16:39, 21.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Chinese Buddhism', 'Buddhism']\n",
      "['Mahayana Buddhism', 'Mahayana']\n",
      "['China']\n",
      "['Tang dynasty', 'during the Tang dynasty', 'the Tang dynasty']\n",
      "['Taoism']\n",
      "['Vietnam']\n",
      "['strict self-control, meditation-practice, insight into Buddha-nature, and the personal expression of this insight in daily life', 'Self-control, meditation-practice, insight into Buddha-nature', 'self-control, meditation-practice, insight into Buddha-nature, and the personal expression of this insight in daily life', 'Self-control, meditation-practice, insight into Buddha-nature, and the personal expression of this insight in daily life', 'discipline, meditation-practice, insight into Buddha-nature, and the personal expression of this insight in daily life']\n",
      "['Self-control, insight into Buddha-nature', 'insight into Buddha-nature']\n",
      "['insight into Buddha-nature']\n",
      "['Daily life', 'daily life', 'In daily life', 'In everyday life', 'in daily life', 'Through zazen and interaction with an accomplished teacher.', 'Through zazen and interaction with an accomplished teacher']\n",
      "['No']\n",
      "['Direct understanding through zazen and interaction with an accomplished teacher', 'direct understanding through zazen and interaction with an accomplished teacher', 'Direct understanding through zazen and interaction with an accomplished teacher.', 'Direct understanding']\n",
      "['Zazen and interaction with an accomplished teacher', 'zazen and interaction with an accomplished teacher.', 'zazen and interaction with an accomplished teacher', 'Zazen and interaction with an accomplished teacher.']\n",
      "['Dhyana']\n",
      "['Dhyāna', '\"dhyāna\"', 'dhyāna', 'Dhyana']\n",
      "['\"absorption\"', 'Absorption', 'absorption']\n",
      "['The Japanese', 'Japanese']\n",
      "['Yes']\n",
      "['Yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 55/100 [16:17<16:25, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chan Buddhism']\n",
      "['Charles VIII']\n",
      "['Naples']\n",
      "[\"D'Aubigny\"]\n",
      "['July 7']\n",
      "['yes']\n",
      "['Montpensier']\n",
      "['the Viceroy', 'Viceroy']\n",
      "['the following year', 'in the following year']\n",
      "['Pozzuoli']\n",
      "['Gonzalo de Cordoba']\n",
      "['the House of Aragon']\n",
      "['the House of Aragon', 'the army of the House of Aragon']\n",
      "['Giuffredo Borgia']\n",
      "['habits', 'the habits']\n",
      "['yes']\n",
      "['Doña Sancia']\n",
      "['Prince of Squillace']\n",
      "['in Calabria', 'Calabria']\n",
      "['1496', 'in the spring of 1496']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 56/100 [16:39<16:14, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the allied forces of Venice', 'the allied forces of Venice and the Church']\n",
      "['Park House', 'at Park House']\n",
      "['Norfolk, England', 'Norfolk']\n",
      "['no']\n",
      "[\"the royal family's Sandringham estate\", 'the Sandringham estate', 'Sandringham estate']\n",
      "['Johnnie and Frances', 'Johnnie and Frances Spencer']\n",
      "['yes']\n",
      "['three', '3']\n",
      "['Andrew and Edward']\n",
      "['a mile away', 'about a mile away']\n",
      "['the butler']\n",
      "['to swim', 'swim in the pool', 'to swim in the pool']\n",
      "['play with animals', 'go outdoors, climb trees and play with animals', 'go outdoors, climbing trees and playing with animals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 57/100 [16:53<14:05, 19.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['barn', 'the barn']\n",
      "['to reach the chicken pen', 'the chicken pen', 'to get the chicken pen']\n",
      "['to make scrambled eggs for breakfast', 'her father could make scrambled eggs for breakfast', 'her father could make scrambled eggs', 'to make scrambled eggs']\n",
      "['four', 'five', 'Five']\n",
      "['four', 'Four']\n",
      "['quacking sound', 'quacking', 'a quacking sound']\n",
      "['a nest']\n",
      "['brown eggs', 'large brown eggs']\n",
      "['brown']\n",
      "['no']\n",
      "['yes', 'no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 58/100 [17:02<11:26, 16.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['Donovan', 'Fred']\n",
      "['yes', 'Yes']\n",
      "['very severe names']\n",
      "['stupid', 'for having been so stupid', 'for having been so stupid as to think it possible', 'for being so stupid', 'he was so stupid', 'because he was stupid', 'for having been stupid as a stupid fool', 'he was stupid', 'because he was so stupid']\n",
      "['his honesty']\n",
      "['congratulate him upon what', 'congratulate him']\n",
      "['yes', 'Yes']\n",
      "['congratulated him', 'congratulated him upon his innocence', 'congratulated him upon what', 'congratulate him']\n",
      "['Bill']\n",
      "['the miners']\n",
      "['yes', 'Yes']\n",
      "['making a stock company']\n",
      "['before returning home', 'after returning home']\n",
      "['No']\n",
      "['his clients', 'his clients relative']\n",
      "['Donovan']\n",
      "['Donovan']\n",
      "['a fish', \"an' a fish\"]\n",
      "['No']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 59/100 [17:24<12:22, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes']\n",
      "['to the city']\n",
      "['Yes']\n",
      "['yes']\n",
      "['8', '8th', 'eighth']\n",
      "['Ridge Road Middle School']\n",
      "['North Carolina', 'in North Carolina']\n",
      "['yes']\n",
      "['basketball']\n",
      "['yes']\n",
      "['$ 50, 000', '$ 250, 000', '$ 50 fee', '$ 50', '$ 50, 000', '$50']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 60/100 [17:34<10:26, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['\"The Proposal\"', 'The Proposal']\n",
      "['romantic comedy']\n",
      "['classic romantic comedy', 'traditional romantic comedy', 'romantic comedy', 'the charming, traditional romantic comedy', 'the classic romantic comedy', 'a charming, traditional romantic comedy']\n",
      "['Ryan Reynolds and Sandra Bullock', 'Sandra Bullock and Ryan Reynolds']\n",
      "['yes']\n",
      "['No']\n",
      "['unknown']\n",
      "['no', 'No']\n",
      "['executive assistant']\n",
      "['a younger man', 'young', 'a young man']\n",
      "['no', 'No']\n",
      "['yes']\n",
      "[\"because it's a romantic comedy\", \"it's a romantic comedy\"]\n",
      "['marry him', \"they'll marry her\", 'marry her']\n",
      "['sexual-harassment lawsuit', 'a sexual-harassment lawsuit', 'sexual-harassment suit', 'sexual harassment']\n",
      "['Alaska']\n",
      "['yes']\n",
      "[\"the groom-to-be's family\", 'a groom-to-be', \"a groom-to-be's family\", 'a groom-to-be family']\n",
      "['no', 'No', 'yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 61/100 [18:00<12:11, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the 1940s', \"the 1940s and the '50s\", \"the 1940s and '50s\"]\n",
      "['Effective Height Above Average Terrain', 'Height Above Average Terrain', 'Height Above Average terrain', 'Height Above average Terrain', 'Height above average terrain', 'Height Above average terrain']\n",
      "['EHAAT', 'Effective Height Above Average Terrain', 'Effective Height Above average terrain', 'Effective Height Above average', 'HAAT']\n",
      "['Yes']\n",
      "['in mountainous regions', 'In mountainous regions']\n",
      "['FM radio and television', 'Radio and television']\n",
      "['FM radio and UHF', 'VHF and UHF', 'UHF and UHF']\n",
      "['No']\n",
      "['Meter', 'meters']\n",
      "['It is more important than effective radiated power', 'Because it is more important than effective radiated power', 'Because it is more important than radiated power']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 62/100 [18:10<10:13, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The maximum distance their station is allowed to cover', 'The maximum distance their antenna class is allowed to cover', 'The maximum distance their station class is allowed to cover']\n",
      "['Michael Brewer']\n",
      "['He was burned', 'burned']\n",
      "['65%', '65 percent', 'Over 65%', '65 percent of his body', 'Over 65 percent of his body', 'Over 65 percent']\n",
      "['unknown']\n",
      "['A group of teenagers', 'a group of teenagers']\n",
      "['No']\n",
      "['unknown', 'Two years', 'About two years']\n",
      "['Valerie', 'Valerie Brewer']\n",
      "['Yes']\n",
      "['Their son.', 'His younger brother is his older brother.', 'Her brother is his brother.', \"Their son is Michael's older brother.\", 'Their son is his brother.', 'Their son is their son.']\n",
      "['CNN']\n",
      "['Second- and third-degree burns', 'Second- and third-degree burns.']\n",
      "['Dr. Carl Schulman.', 'Dr. Carl Schulman']\n",
      "['Dr. Carl Schulman.', 'Carl Schulman.', 'Carl Schulman']\n",
      "['Jackson Memorial Hospital', 'Jackson Memorial Hospital.', 'Jackson Memorial Hospital Burn Center.']\n",
      "['About two-thirds', 'Two-thirds', '2-thirds']\n",
      "['No']\n",
      "['October 12']\n",
      "['Jeremy Jarvis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 63/100 [18:32<11:04, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['focus on that', 'That I can focus on that.']\n",
      "['French', 'France']\n",
      "['Majdi Shakoura']\n",
      "['about 200 meters', '200 meters']\n",
      "['Palestinian militants']\n",
      "['no', 'No']\n",
      "['No']\n",
      "['Two']\n",
      "['Yes']\n",
      "['Hamas naval building', 'a Hamas naval building']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 64/100 [18:43<09:26, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes']\n",
      "['Windows 8']\n",
      "[\"major changes to the operating system's platform and user interface\", \"major changes to the operating system's platform\"]\n",
      "[\"perating system's platform\", 'perating system']\n",
      "['proved its user interface', 'proved user experience', 'prove its user experience', 'proved its user experience on tablets', 'proved its user experience', 'improve its user experience']\n",
      "['what was?', 'what was it?']\n",
      "['lay programs and dynamically updated content on a grid of tiles', 'lays programs and dynamically updated content on a grid', 'lays programs and dynamically updated content on a grid of tiles', 'lays programs and updates content on a grid of tiles']\n",
      "['oad and purchase new software', 'oaded and purchasing new software', 'oaded apps', 'oaded and purchased software', 'oad and purchasing new software', 'oads and purchasing new software', 'oaded and purchased new software', 'oaded software']\n",
      "['what was one of them?', 'what was one?', 'what is one of them?']\n",
      "['what is the name of the platform?', 'what was the name of the platform?', 'what was the name of the new platform?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 65/100 [18:50<07:45, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ng the boot process', 'the boot process']\n",
      "['Bradley Cooper', 'Clint Eastwood', 'Kyle', 'Taya Kyle']\n",
      "['yes', 'Yes', 'no']\n",
      "['Taya Kyle']\n",
      "['March 16']\n",
      "['she was shot and killed', 'He was shot', 'he was shot and killed', 'shot and killed', 'He was shot and killed']\n",
      "['Eddie Ray Routh']\n",
      "['Most successful sniper', 'most successful sniper', 'the most successful sniper', 'Most successful sniper in United States military history']\n",
      "['yes']\n",
      "['Clint Eastwood']\n",
      "['Clint Eastwood']\n",
      "['yes']\n",
      "[\"Kyle's book\", \"Kyle's bestselling autobiography\", \"Kyle's autobiography\"]\n",
      "['she wrote to him', 'she wrote a letter', 'she wrote']\n",
      "['to bury herself under the covers', 'bury herself', 'bury herself under the covers']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 66/100 [19:07<08:01, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life in prison without the possibility of parole', 'life in prison without the possibility of parole.']\n",
      "['Ronaldo and Ronaldo', 'Messi and Ronaldo', 'Messi and Cristiano Ronaldo', 'Ronaldo and Messi']\n",
      "['Ronaldo']\n",
      "[\"FIFA Ballon d'Or\", \"Ballon d'Or\"]\n",
      "['Ronaldo']\n",
      "['Ronaldo']\n",
      "['yes', 'Yes']\n",
      "['thousands of dollars', 'millions of dollars', 'millions']\n",
      "['Repucom']\n",
      "['87%', '91%', '92%', '86%']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 67/100 [19:19<07:24, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['87%']\n",
      "['Rudolph Virchow', 'Virchow']\n",
      "[\"Darwin's conclusions lacked empirical foundation\", \"that Darwin's conclusions lacked empirical foundation\", \"that Darwin's conclusions lacked foundation\", \"Darwin's conclusions lacked empirical foundation.\", \"Darwin's conclusions\", \"that Darwin's conclusions lacked empirical foundation.\"]\n",
      "['1869']\n",
      "['The Anthropological Society of Madrid', 'the Anthropological Society of Vienna', 'Anthropological Society of Madrid', 'the Anthropological Society of Madrid', 'The Anthropological Society of Vienna']\n",
      "['the Italian Society of Vienna (1871)', 'the Anthropological Society of Vienna (1871)', 'the Anthropological Society of Vienna', 'the Anthropological Society of Vienna (1870)', 'the Italian Society of Vienna (1870),']\n",
      "['1871', '1870']\n",
      "['after that', 'after']\n",
      "['1902']\n",
      "['no']\n",
      "['the study of humans and their societies in the past and the present', 'study of humans and their societies in the past and present', 'a study of humans and their societies in the past and present', 'the study of humans and their societies in the past and present', 'the study of humans and their societies', 'study of humans and their societies', 'the study of past human cultures']\n",
      "['Three', 'three', 'Four']\n",
      "['the study of past cultures', 'the study of human cultures', 'the study of past human cultures through investigation of physical evidence', 'the study of past human cultures through investigation of physical evidence,', 'the study of past human cultures', 'the study of past cultures through investigation of physical evidence', 'study of past human cultures through investigation of physical evidence']\n",
      "['no']\n",
      "['as a branch of anthropology in the United States', 'it is considered a branch of anthropology in the United States', 'it is thought of as a branch of anthropology in the United States', 'it is thought of as a branch of anthropology', 'it is seen as a branch of anthropology in the United States', 'it is a branch of anthropology in the United States', 'as a branch of anthropology']\n",
      "['as a discipline in its own right', 'it is thought of as a discipline in its own right', 'it is viewed as a discipline in its own right']\n",
      "['social anthropology']\n",
      "['cultural anthropology']\n",
      "[\"Darwin's\", 'Darwin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 68/100 [19:35<07:43, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['3 years old']\n",
      "['Halloween']\n",
      "['no']\n",
      "['a ghost']\n",
      "['lion', 'a lion']\n",
      "['no']\n",
      "['Todd']\n",
      "['his birthday']\n",
      "['Mom', 'my mom']\n",
      "['My mom', 'Mom', 'my Mom', 'my mom', 'my mommy', 'Mommy']\n",
      "[\"he'll come home from work\", 'he comes home from work', 'he comes home', 'He comes home', 'He comes home from work']\n",
      "[\"at his friend Kevin's house\", \"at Kevin's house\", \"Kevin's house\"]\n",
      "['my friend', \"my friend Kevin's\", \"Kevin's\"]\n",
      "['because Kevin is my friend', \"because he's my friend\", 'He is my friend.', \"he's my friend\", 'he is my friend', \"He's my friend\", 'He is my friend', 'Kevin is my friend']\n",
      "['no', 'No']\n",
      "['Kevin', 'my dad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▉   | 69/100 [19:51<07:36, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the best Halloween ever', 'the best Halloween ever.']\n",
      "['Billy']\n",
      "['to buy some beef', 'to buy beef']\n",
      "[\"for his brother's birthday\", \"his brother's birthday\"]\n",
      "['six', 'Five', 'Six']\n",
      "['yes']\n",
      "['brown spots', 'brown']\n",
      "['eating breakfast', 'eating their breakfast']\n",
      "['in a big grassy meadow', 'in a grassy meadow', 'a big grassy meadow']\n",
      "['strange', 'very strange']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 70/100 [19:58<06:17, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Olivia Pope']\n",
      "['Jake']\n",
      "['Fitz', 'a broken Fitz']\n",
      "['no', 'No']\n",
      "['Lost his son', 'He lost his son', 'he lost his son', 'lost his son']\n",
      "['He realized some horrible things about his father', 'realized some horrible things about his father', 'He realized some horrible things about his father.']\n",
      "['Portia de Rossi', 'Ellen DeGeneres']\n",
      "['Portia de Rossi']\n",
      "[\"Scandal's\", 'Scandal']\n",
      "['Sept. 25 at 9 p.m.', 'Sept. 25']\n",
      "['9 p.m.,', '9 p.m.', '9 p.m']\n",
      "['Columbus Short']\n",
      "['Texas']\n",
      "[\"Olivia's\", \"Olivia's own\", 'Olivia Pope']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████   | 71/100 [20:12<06:16, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABC']\n",
      "['George and Grierson', 'Grierson']\n",
      "['Grierson']\n",
      "['Grierson']\n",
      "[\"Grant's\"]\n",
      "['frosty', 'snow', 'snow-']\n",
      "['Grierson']\n",
      "['George']\n",
      "['cutting fuel']\n",
      "['no']\n",
      "['no']\n",
      "['trunks', 'the trunks']\n",
      "['night']\n",
      "['night']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 72/100 [20:29<06:34, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the mail-carrier']\n",
      "['it was more desirable for him', 'more desirable', 'it was more desirable']\n",
      "['half subdued and hostile to his rule', 'half subdued and hostile']\n",
      "['his rule']\n",
      "['England']\n",
      "['in attacking France']\n",
      "['by giving them great appointments', 'by giving them great appointments and trusting them fully']\n",
      "['by calling them to his Parliament', 'by calling them to his Parliament in London']\n",
      "['run up for her a new constitution', 'run up for her new constitution', 'he ran up for her a new constitution', 'ran up for her a new constitution', 'he hastily ran up for her a new constitution']\n",
      "['yes']\n",
      "['John of Brittany']\n",
      "['as governor', 'a governor', 'governor']\n",
      "['John of Brittany']\n",
      "['Scotland']\n",
      "['the Scottish clergy']\n",
      "['union with England']\n",
      "['the Scottish clergy']\n",
      "['Bruce', 'Robert Bruce']\n",
      "['Bishop of St Andrews', 'the Bishop of St Andrews']\n",
      "['made against him', 'they made against him']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 73/100 [20:52<07:32, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['December 10, 1306', 'February 10, 1306', 'March 10, 1306', 'June 1304', 'in June 1304']\n",
      "['Winter', 'winter']\n",
      "['socks', 'some socks']\n",
      "['Olive the owl', 'Olive']\n",
      "['in the maple tree', 'the old maple tree', 'in the old maple tree', 'on the maple tree', 'maple tree', 'old maple tree', 'on the old maple tree']\n",
      "['yes']\n",
      "['her feet were cold', 'her feet were very cold', 'her legs and toes were very cold', 'her legs and toes were cold']\n",
      "['the ice creek', 'the icy creek']\n",
      "['Rose']\n",
      "['six', 'Six']\n",
      "[\"she didn't have socks\", 'she had no use for socks', 'she had no use for them']\n",
      "['no']\n",
      "['her grandmother and mother', 'her grandmother']\n",
      "['the big hill', 'on the big hill', 'the farmhouse']\n",
      "['Henrietta', 'Henrietta the human']\n",
      "['her powerful beak', 'her beak']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 74/100 [21:12<07:41, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very happy', 'happy']\n",
      "['Outside', 'outside', 'A little cat', 'a little cat']\n",
      "['a can of fish', 'a fish', 'fish']\n",
      "['fish']\n",
      "['fell sound asleep', 'fell asleep']\n",
      "['the apartment', 'in his apartment', 'apartment', 'his apartment', 'in the apartment']\n",
      "['no']\n",
      "['thousand miles', 'a thousand miles']\n",
      "['neighbors']\n",
      "['the cat had been abandoned by his owner', 'that the cat had been abandoned by his owner', 'the cat had been abandoned']\n",
      "['his owner']\n",
      "['Willis']\n",
      "['five years']\n",
      "['Willis']\n",
      "['no']\n",
      "['it was far more heartbreaking', \"it was far more heartbreaking than I'd expected\", 'it was more heartbreaking']\n",
      "['yes']\n",
      "['he was nice', 'nice', 'how nice it was']\n",
      "['yes', 'no']\n",
      "['he was lonely', 'lonely']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 75/100 [21:35<08:01, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['Slovenia']\n",
      "['the Republic of Slovenia', 'Republic of Slovenia']\n",
      "['the Republic of Slovenia', 'the Republic of Ljubljana', 'Slovenia', 'Republic of Slovenia']\n",
      "['RS']\n",
      "['in southern Central Europe', 'in the southern Central Europe', 'Central Europe', 'southern Central Europe', 'in the Central Europe']\n",
      "['Eastern and Southern Europe']\n",
      "['to the west', 'the west']\n",
      "['Hungary']\n",
      "['Hungary']\n",
      "['Croatia', 'to the northeast']\n",
      "['yes']\n",
      "['the southeast', 'the south and southeast']\n",
      "['2.06 million']\n",
      "['Ljubljana']\n",
      "['yes']\n",
      "['yes']\n",
      "['the United Nations']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 76/100 [21:57<08:02, 20.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parliamentary', 'a parliamentary republic']\n",
      "['Aunt Ada']\n",
      "['Aunt Ada']\n",
      "['Gillian']\n",
      "['Brompton']\n",
      "[\"Mr. Clement Underwood's church\", 'Mrs. Clement Underwood', 'grandmother', 'her grandmother']\n",
      "['no']\n",
      "['yes']\n",
      "['during the latter part of the time']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|███████▋  | 77/100 [22:09<06:45, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Miss Mohun']\n",
      "['Arnscombe', 'the district of Arnscombe']\n",
      "['Mysie']\n",
      "['claim Miss Prescott', 'to claim Miss Prescott', 'to claim Miss Prescott for a game', 'claim Miss Prescott for a game']\n",
      "['no', 'No']\n",
      "['heedless']\n",
      "['Vera and Paula']\n",
      "['no']\n",
      "['delighted']\n",
      "['lawn tennis']\n",
      "['yes']\n",
      "['kittens']\n",
      "['Raki raki', 'Raki', 'raki']\n",
      "['Raki raki', 'Raki', 'raki']\n",
      "['Primrose']\n",
      "['the Goyle']\n",
      "['Thekla']\n",
      "['Thekla']\n",
      "['her aunt', 'the maiden aunt', 'her maiden aunt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 78/100 [22:34<07:15, 19.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a lyre', 'a Greek lyre']\n",
      "['Michael Scott Moore']\n",
      "['he was freed', 'freed', 'freed him']\n",
      "['Somali pirates']\n",
      "['more than two years', 'two years']\n",
      "['yes', 'Yes']\n",
      "['Saunders', 'Marlis Saunders']\n",
      "['his mother', 'her mother']\n",
      "['She was elated', 'elated', 'Elated']\n",
      "['He was free', 'to hear he is free', 'he was free', 'she was told he was free', 'to hear he was free']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▉  | 79/100 [22:44<05:58, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just joyful', 'Joyful', \"joyful, I can't describe it\", 'joyful', \"Joyful, I can't describe it\", 'Just joyful']\n",
      "['Jerry']\n",
      "['train cars', 'train car', 'a train car', 'in a train car']\n",
      "['all his life']\n",
      "['Marge']\n",
      "['36']\n",
      "['because she wanted to keep him safe', 'she wanted to keep him safe', 'to keep him safe']\n",
      "['the people who lived in the town nearby', 'the people in the town nearby', 'people who lived in the town', 'people in the town nearby', 'people who lived in the town nearby']\n",
      "['Qarth']\n",
      "['100', '100 people']\n",
      "['the people of Qarth would attack them', 'the people of Qarth would attack them if they knew they were there.', 'the people of Qarth would attack them if they knew they were there']\n",
      "['corn']\n",
      "['no', 'No']\n",
      "['George']\n",
      "['2 years older than Marge']\n",
      "[\"the people in the town didn't like him\", \"because the people in the town didn't like him\", \"Because the people in the town didn't like him\"]\n",
      "['yes', 'Yes']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 80/100 [23:02<05:47, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['Three', 'Physical, climatic, and biological', 'Physical, climatic, and biological factors', 'physical, climatic, and biological']\n",
      "['yes', 'Yes', 'Yes.']\n",
      "['Yes.']\n",
      "['Environmental and climatic.', 'Environmental and Biological.', 'Physical, climatic and biological.', 'Physical and climatic.', 'Physical, climatic, and biological.', 'Environmental and biological.', 'Physical, climatic, and biological']\n",
      "['South Africa.', 'southwestern South Africa.']\n",
      "['No.']\n",
      "['Southwest', 'southwestern']\n",
      "['southwestern']\n",
      "['No.', 'No']\n",
      "['\"precinctive\"', 'Precinctive']\n",
      "['In \"in\"', '\"in\"']\n",
      "['\"the people\"', '\"the people\".']\n",
      "['\"precinctive\"', 'Precinctive']\n",
      "['MacCaughey', 'David Sharp']\n",
      "['Yes']\n",
      "['Yes', 'Yes.']\n",
      "['unknown']\n",
      "['Cosmopolitan distribution.', 'Cosmopolitan distribution']\n",
      "['The extreme opposite of Endemism', 'The extreme opposite of endemism', 'Endemism']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████  | 81/100 [23:25<06:03, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A island', 'an island', 'An island']\n",
      "['yes', 'Yes']\n",
      "['a dog', 'dog']\n",
      "['a dog.', 'a dog', \"Bob's wife\", \"Bob's wife.\", \"Her mother and Bob's wife\", 'Bob']\n",
      "['yes']\n",
      "[\"Frank's wife\", 'Frank', 'a dog']\n",
      "['a fight', 'the long fight', 'a long fight']\n",
      "['yes']\n",
      "[\"Bob's wife\"]\n",
      "['he passed away', 'passed away']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 82/100 [23:33<04:40, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a bad heart attack', 'bad heart attack']\n",
      "['in the park', 'the park']\n",
      "['yes', 'Yes']\n",
      "['baseball', 'playing baseball']\n",
      "['the park']\n",
      "['two baseballs']\n",
      "['two']\n",
      "['cola', 'colas', 'yes']\n",
      "['ten dollars']\n",
      "['the park']\n",
      "['Mike']\n",
      "['yes', 'Yes']\n",
      "['yes']\n",
      "['Sally']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 83/100 [23:43<03:58, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['linguist', 'a linguist']\n",
      "['a new language use', 'a new language']\n",
      "['Stanford University']\n",
      "['his girlfriend and their three screaming kids', 'his brother, his girlfriend, and their three screaming kids', 'their three screaming kids', 'his girlfriend and three screaming kids', 'his girlfriend, and their three screaming kids']\n",
      "['unknown', 'We are in New York', 'New York']\n",
      "['unknown']\n",
      "['Netspeak']\n",
      "['Internet or cell phones']\n",
      "['linguists']\n",
      "['Increasing spelling and grammatical mistakes', 'increased spelling and grammatical mistakes', 'spelling and grammatical mistakes']\n",
      "['linguists']\n",
      "['David Crystal']\n",
      "['centuries', 'For centuries', 'for centuries']\n",
      "['Linguist James', 'James']\n",
      "['by writing']\n",
      "['teenagers']\n",
      "['languages', 'text messaging', 'Text messaging and writing']\n",
      "['teenagers']\n",
      "['diary writing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 84/100 [24:11<04:51, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['linguist']\n",
      "['Grandpa Peter', 'Peter']\n",
      "['yes']\n",
      "['His grandparents', 'his grandparents']\n",
      "['no']\n",
      "['a cat']\n",
      "['no']\n",
      "['no']\n",
      "['he had understood that if he wanted to sleep in his bed, he would have to get into it before Tubby did', 'Andy had understood that if he wanted to sleep in his bed, he would have to get into it before Tubby did', 'Andy had understood that if he wanted to sleep in his bed, he would have to get into it before Tubby did.']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 85/100 [24:22<03:58, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no']\n",
      "['fatigued', 'fatigue']\n",
      "['Philip']\n",
      "['discussing a plan']\n",
      "['no']\n",
      "['to write letters', 'to write']\n",
      "['writing', 'write', 'write her letters', 'write letters', 'writing her letters']\n",
      "['began discussing a plan', 'he began discussing a plan']\n",
      "['offering himself as chief of the constabulary force', 'a chief of the constabulary force', 'of offering himself as chief of the constabulary force', 'chief of the constabulary force', 'offering himself as chief of the constabulary force', 'a plan which had occurred to him of offering himself as chief of the constabulary force', 'of offering himself as chief of the constabulary force']\n",
      "['constabulary force', 'the constabulary force']\n",
      "['in the county where Redclyffe', 'in the county where Redclyffe was', 'in the county where Redclyffe was situated', 'the county where Redclyffe was situated']\n",
      "['no']\n",
      "['Guy']\n",
      "['no']\n",
      "['fatigue']\n",
      "['yes']\n",
      "['yes']\n",
      "['a couple of hours', 'for a couple of hours']\n",
      "['yes']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 86/100 [24:43<04:05, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes']\n",
      "['all']\n",
      "['ow old is she now?', 'how old is she?', 'ow old is she?', 'ow old was she?']\n",
      "['how old is Hannah?', 'how old was Hannah?']\n",
      "[\"what was her cousin's name?\", 'what was her cousin called?']\n",
      "['how old is she?', 'how old was she?', \"what was her cousin's name?\"]\n",
      "[\"the shark's nose\", \"e shark's nose\", \"shark's nose\", 'her', 'Hannah', 'him']\n",
      "['from the water to lie on her back', 'her from the water to lie on his back,\"she says', 'from the water to lie on his back', 'her from the water to lie on his back', 'gged from the water to lie on his back']\n",
      "['from the water to lie on his back', 'her from the water to lie on his back', 'dragged from the water to lie on her back', 'gged from the water to lie on his back', 'her from the water to lie on her back']\n",
      "[\"r life to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans,\", \"r survival to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans\", \"to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans,\", \"r survival to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans,\", \"to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans\", \"r life to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans\", \"r life to Syb's bravery and the fact that great whites,despite their reputation as man-eaters,typically don't target humans\", \"r bravery to Syb's bravery and the fact that great whites,despite their reputation as man-eaters, typically don't target humans,\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 87/100 [24:56<03:29, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['award did Hannah win?', 'award did Hannah owe her?', 'award did Hannah owe her survival to?', 'award did Hannah receive?', 'award did Hannah earn?', 'award did Hannah earn for her bravery?']\n",
      "['five', 'Five']\n",
      "['Westchester County', 'Westchester']\n",
      "['Jonas Bronck']\n",
      "['the first settlement', 'The New Netherland colony', 'the New Netherland colony', 'New Netherland', 'New Netherland colony']\n",
      "['in the New Netherland colony', 'the New Netherland colony', 'New Netherland', 'New Netherland colony']\n",
      "['1639', 'in 1639']\n",
      "['yes']\n",
      "['the natives were displaced', 'the natives displaced', 'they were displaced', 'the native Lenape']\n",
      "['1643']\n",
      "['Five']\n",
      "['42 square miles']\n",
      "['2014', 'in 2014']\n",
      "['1,442,159', '1,434,159', '1,448,159', '1,438,159']\n",
      "['No']\n",
      "['fourth', 'fourth highest']\n",
      "['hip hop', 'Latin music']\n",
      "['Hip hop', 'hip hop']\n",
      "['American South', 'from the American South', 'the American South']\n",
      "['1643']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 88/100 [25:15<03:22, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the East River', 'the Harlem River', 'Harlem River', 'The Harlem River']\n",
      "['Caden Rodgers']\n",
      "['head injury', 'a head injury']\n",
      "['yes']\n",
      "['his mother']\n",
      "['threw him across the room']\n",
      "['no']\n",
      "['meth']\n",
      "['marijuana']\n",
      "['yes']\n",
      "['yes']\n",
      "['first-degree murder']\n",
      "['Aurora Sentinel', 'the Aurora Sentinel', 'the Aurora Sentinel.']\n",
      "['two', '2']\n",
      "['Los Angeles International Airport']\n",
      "['to see his dying grandson', 'to see his grandson', 'to see his grandson one last time']\n",
      "['CNN affiliate KABC', 'CNN', 'CNN affiliate']\n",
      "['no']\n",
      "['a pilot', 'pilot']\n",
      "['Southwest Airlines']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 89/100 [25:40<03:32, 19.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Southwest spokeswoman', 'a Southwest spokeswoman']\n",
      "['Joey']\n",
      "['tooth brush', 'to brush his teeth', 'brush his teeth']\n",
      "['His mother', 'His mom', 'his mother', 'his mom']\n",
      "['yes', 'Yes']\n",
      "['He brushed his teeth', 'Joey brushed his teeth', 'brushed his teeth']\n",
      "['No']\n",
      "['Yes']\n",
      "['He was told to brush his teeth', 'he was told to brush his teeth', \"he told his mom that he didn't brush his teeth\", \"He was told that he didn't brush his teeth\", 'he was told he did not brush his teeth']\n",
      "['He pushed him over and started to cry', 'He pushed the boy over', 'He pushed the boy over and started to cry', 'He pushed the boy over and started crying', 'He pushed the boy', 'He pushed him over']\n",
      "['He pushed the boy over and started to make fun of him', 'pushed the boy over', 'He pushed the boy over', 'He pushed the boy over and started to cry', 'pushed the boy over and started to cry', 'He pushed him over', 'pushed the boy over and started to cry', 'He pushed the boy over and started to cry', 'He pushed him over', 'pushed him over and started to cry']\n",
      "['A teacher', 'The teacher']\n",
      "[\"Joey's mom\"]\n",
      "[\"Joey's mom came over and took him home\", \"Joey's mom took him home\", \"Joey's mom came to school\", \"Joey's mom came over\", \"Joey's mom\", \"Joey's mom marched him up to the bathroom\"]\n",
      "['He was being loud', 'Because he was being loud']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 90/100 [25:54<02:57, 17.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes']\n",
      "['Virginia governor']\n",
      "['Virginia']\n",
      "['Terry McAuliffe']\n",
      "['Republican Ken Cuccinelli', 'Ken Cuccinelli', 'Bob McDonnell']\n",
      "['Republican']\n",
      "['Democrat']\n",
      "['yes', 'Yes']\n",
      "['No']\n",
      "['unknown']\n",
      "['Tuesday evening.', 'Tuesday evening', 'Tuesday']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 91/100 [26:06<02:24, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Richmond.']\n",
      "['F**k you', '\"F**k you.\"', \"'F**k you.'\"]\n",
      "['Chris Carroll']\n",
      "[\"at the scene of Tupac Shakur's 1996 drive-by murder\", \"Tupac Shakur's 1996 drive-by murder\", 'the 1996 drive-by murder', '1996']\n",
      "['Chris Carroll']\n",
      "['a sergeant', 'retired sergeant', 'Sergeant']\n",
      "['Las Vegas Metropolitan Police Department', 'the Las Vegas Metropolitan Police Department']\n",
      "['a drive-by murder', 'drive-by murder']\n",
      "['September 7th']\n",
      "['Orlando Anderson', 'Orlando Anderson Anderson']\n",
      "['yes']\n",
      "['with 21-year-old Crips gang member Orlando Anderson', 'Crips gang member Orlando Anderson']\n",
      "['Death Row Records']\n",
      "['the lobby of the MGM Grand', 'in the lobby of the MGM Grand casino', 'in the lobby of the MGM Grand', 'a MGM Grand casino', 'the lobby of the MGM Grand casino']\n",
      "['Mike Tyson fight', 'Mike Tyson']\n",
      "['no']\n",
      "['a traffic light', 'the traffic light']\n",
      "['no']\n",
      "['Cadillac', 'a Cadillac']\n",
      "['white']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 92/100 [26:33<02:35, 19.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a white Cadillac pulled up beside Knight and Shakur while they were stopped', 'a white Cadillac pulled up beside Knight and Shakur', 'a white Cadillac pulled up beside Knight and Shakur while they were stopped at a traffic light', 'the white Cadillac pulled up beside Knight and Shakur while they were stopped at a traffic light', 'a white Cadillac pulled up beside Knight and Shakur while they were stopped at a traffic light and one man began shooting out of the back window.', 'a white Cadillac pulled up', 'a white Cadillac']\n",
      "['Mayfair', 'in Mayfair']\n",
      "['Paul']\n",
      "['Arthur']\n",
      "['once a day', 'every day']\n",
      "['his regiment', 'from his regiment', 'regiment']\n",
      "['Paul']\n",
      "['London', 'in London']\n",
      "['Mayfair', 'in Mayfair']\n",
      "['shook hands', 'they shook hands']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 93/100 [26:46<02:01, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pale cheeks', 'a lot of pale cheeks', 'sunken, pale cheeks,', 'sunken, pale cheeks', 'He was still in his dressing-gown', 'he was in his dressing-gown', 'he was still in his dressing-gown']\n",
      "['Whiskers has a white spot on her chest', 'Whiskers is black with a white spot on her chest', 'Whiskers is black', 'Whiskers is black with a white spot on her chest.', 'on Saturday, Whiskers turns two years old', 'on Saturday, Whiskers turns two years old.', 'because on Saturday, Whiskers turns two years old.', 'because on Saturday, Whiskers turns two years old']\n",
      "['a cat', 'a pet cat', 'A pet cat', 'cat', 'A cat']\n",
      "['like little white mittens', 'a little white mitten', 'little white mittens']\n",
      "['pet store', 'the pet store']\n",
      "[\"to buy Whiskers' birthday presents\", \"Whiskers' birthday presents\"]\n",
      "['a play mouse and a feather', 'a play mouse', 'a play mouse and a blue feather']\n",
      "['a blue feather']\n",
      "['a ball of yarn', 'a red ball of yarn']\n",
      "['a bowl with a picture of a cat', 'a bowl with a picture of a cat on the side']\n",
      "['a picture of a cat']\n",
      "['a cat']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 94/100 [26:55<01:30, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the sun']\n",
      "['not using \"good judgment,\"', 'not using good judgment', 'not using \"good judgment\"']\n",
      "['not got out of that car', 'not gotten out of that car', 'not have gotten out of that car', 'not gotten out of the car', 'got out of the car', 'got out of that car', 'gotten out of that car', 'not have been in the car']\n",
      "['no']\n",
      "['the car', 'in the car']\n",
      "[\"he didn't use good judgment\", 'he didn\\'t use \"good judgment\"']\n",
      "['these people', 'the people']\n",
      "['George Zimmerman', 'Zimmerman', 'the Zimmerman family', 'the Zimmerman']\n",
      "['he went above and beyond what he should have done', 'he went above and beyond what he really should have done,\" she said.', 'he went above and beyond what he really should have done']\n",
      "['yes']\n",
      "['Juror B37']\n",
      "[\"CNN's Anderson Cooper 360\", 'Anderson Cooper 360', 'Anderson Cooper', 'CNN\\'s \"Anderson Cooper 360\"']\n",
      "['no']\n",
      "['Monday night']\n",
      "['yes']\n",
      "['he was shot', 'he got shot', 'he shot him']\n",
      "[\"he didn't use good judgment\", 'he didn\\'t use \"good judgment\"']\n",
      "['no']\n",
      "[\"he didn't use good judgment\", 'he was guilty of not using \"good judgment\"', 'he didn\\'t use \"good judgment\"']\n",
      "['no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 95/100 [27:17<01:25, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in the neighborhoods', 'in the neighborhood']\n",
      "['reviewing films', 'reviewing']\n",
      "['Tom Seaton']\n",
      "['no', 'No']\n",
      "['Tom Seaton']\n",
      "['editor of The Front Page', 'first arts editor of The Front Page', 'arts editor', 'first arts editor', 'The first arts editor of The Front Page', 'the first arts editor of The Front Page', 'the first editor of The Front Page']\n",
      "['written for television', 'he had written for television.', 'he had also written for television', 'he had written for television', 'writing for television']\n",
      "['no']\n",
      "['yes']\n",
      "['yes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 96/100 [27:31<01:03, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to make the atmosphere sociable', 'to make sociable atmosphere', 'there would also be guests to make the atmosphere sociable', 'there would also be guests to make sociable', 'there would be a guest to make the atmosphere sociable']\n",
      "['Safedom']\n",
      "['European partners or acquisitions']\n",
      "['Brian Fu']\n",
      "['200m']\n",
      "['1bn']\n",
      "[\"the world's biggest player\", \"the world's largest player\"]\n",
      "['Three', 'three']\n",
      "['chief executive']\n",
      "['UK', 'the UK', 'in the UK']\n",
      "['yes']\n",
      "['meeting potential partners', 'met potential partners and acquisitions', 'meeting potential partners and acquisitions']\n",
      "['condom', 'condoms']\n",
      "['Beautiful Girl, Take Me and Green Lemon']\n",
      "['it is in manufacturing and technology', 'manufacturing', \"it's in manufacturing and technology\", 'manufacturing and technology', 'in manufacturing and technology']\n",
      "['condoms', 'half of all condoms are bought by women', 'half of all condoms']\n",
      "['200m', 'half of all condoms are bought by women', 'half of all condoms']\n",
      "['branding part', 'branding', 'the branding part']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|█████████▋| 97/100 [27:50<00:51, 17.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No']\n",
      "['Foy']\n",
      "['the naked back of Martin', 'the back of Martin']\n",
      "[\"the shelter of Mother Martha's lair\", \"Mother Martha's lair\"]\n",
      "['the forced marriage', 'forced marriage']\n",
      "['in the Red Mill', 'the Red Mill']\n",
      "['weeks', 'some weeks']\n",
      "['the sword']\n",
      "['the Haarlemer Meer', 'Haarlemer Meer', 'in the Haarlemer Meer']\n",
      "['Lysbeth']\n",
      "['many days']\n",
      "['the sword cut', 'the sword cut in his thigh', 'the sword was cut in his thigh', 'sword cut in his thigh']\n",
      "['yes']\n",
      "['gangrene']\n",
      "['yes']\n",
      "['his own strength and healthy constitution']\n",
      "['Leyden']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 98/100 [28:08<00:34, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for him to visit', 'to visit']\n",
      "['late twentieth-century development', 'Late twentieth-century', 'late twentieth-century', 'late twentieth century development', 'late twentieth century', 'Late twentieth-century development']\n",
      "['Limited Overs Internationals', 'Limited overs International', 'Limited overs internationals', 'Limited overs cricket']\n",
      "['Limited Overs Internationals', 'Limited Overs', 'Limited overs internationals', 'Limited Overs International', 'Limited Overs Internationals Internationals']\n",
      "['Twenty20 International matches', 'Twenty20 Internationals', 'Twenty20 International']\n",
      "['One Day International', 'A One Day International', 'One Day International matches']\n",
      "['5 January 1971', '1971']\n",
      "['5 January 1971', '1971']\n",
      "['Melbourne', 'Melbourne Cricket Ground']\n",
      "['no', 'No']\n",
      "['Australia and England']\n",
      "['Australia']\n",
      "['England']\n",
      "['40 wickets', '5 wickets']\n",
      "['a red ball.', 'a red ball']\n",
      "['the rival World Series Cricket competition', 'World Series Cricket', 'World Series Cricket competition', 'The rival World Series Cricket competition']\n",
      "['many of the features of One Day International cricket that are now commonplace', 'One Day International cricket competition', 'many features of One Day International cricket', 'many of the features of One Day International cricket']\n",
      "['the use of white flannels and a red ball', 'use of white flannels and a red ball.', 'the use of white flannels and a red ball.', 'use of white flannels and a red ball', 'the use of white flannels and a red ball in ODIs']\n",
      "['television rights to cricket in Australia', 'TV rights to cricket in Australia', 'Channel 9 getting the TV rights to cricket', 'Channel 9 getting the TV rights to cricket in Australia']\n",
      "['players worldwide being paid to play cricket', 'players worldwide being paid to play, and becoming international professionals', 'players worldwide being paid to play', 'players worldwide being paid to play cricket.', 'players worldwide being paid to play,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████▉| 99/100 [28:32<00:19, 19.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2001.', '2001']\n",
      "['the long night', 'a long night', 'long night']\n",
      "['Billy']\n",
      "['Byrne']\n",
      "['he was a mucker', 'a mucker', 'mucker']\n",
      "['no', 'No']\n",
      "['Anthony Harding']\n",
      "['Mallory']\n",
      "['Mallory']\n",
      "['Mallory']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:42<00:00, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he was sure that the old love would be aroused', \"he was sure that the old love would be aroused in the girl's breast\", 'he was a prospective son-in-law']\n",
      "Epoch: 6  with accuracy: 0.7346809854706254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "false_positives = None\n",
    "wrong_answers = None\n",
    "\n",
    "\n",
    "checkpoint = torch.load('save_adversarial' + str(6))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "accuracy, wrong_answers, false_positives = compute_accuracy_of_model(model.cuda())\n",
    "print('Epoch:', 6, ' with accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6910928616550853"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save_small 6\n",
    "### num_replicas=5, len(dlist)=10 \n",
    "Epoch: 6  with accuracy: 0.8439716312056738\n",
    "\n",
    "## save_adversarial 6\n",
    "### num_replicas=5, len(dlist)=100\n",
    "Epoch: 6  with accuracy: 0.8313329121920404\n",
    "\n",
    "### num_replicas=7, len(dlist)=100\n",
    "Epoch: 6  with accuracy: 0.8458622867972204\n",
    "\n",
    "### num_replicas=10, len(dlist)=100\n",
    "Epoch: 6  with accuracy: 0.868603916614024\n",
    "\n",
    "### num_replicas=20, len(dlist)=100\n",
    "0.8989260897030954\n",
    "\n",
    "## Using only most frequent answer\n",
    "### num_replicas=25, len(dlist)=10\n",
    "0.7092198581560284\n",
    "\n",
    "## Using the whole group of most frequent answers\n",
    "### num_replicas=25, len(dlist)=10\n",
    "0.7659574468085106\n",
    "### num_replicas=25, len(dlist)=100\n",
    "0.7195198989260897\n",
    "\n",
    "## Using the first two groups of most frequent answers\n",
    "### num_replicas=25, len(dlist)=100\n",
    "0.8439671509791535\n",
    "\n",
    "## Using all groups over a threshold\n",
    "### num_replicas=25, len(dlist)=100, threshold>0.4\n",
    "0.6910928616550853\n",
    "\n",
    "### num_replicas=25, len(dlist)=100, score/best_prediction > 0.80.\n",
    "0.7346809854706254\n",
    "\n",
    "## TEST WITH 2 groups and fixed YES/NO answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "#### USE DROPOUT ON A BATCH OF IDENTICAL INPUTS!\n",
    "This way there is no need to run the model 5-6-7 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('save_adversarial' + str(1))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subfinder(lst, pattern, offset=0):\n",
    "    index1 = offset\n",
    "    index2 = offset\n",
    "    \n",
    "    while index1 < len(lst):\n",
    "        if lst[index1] != pattern[index2]:\n",
    "            index2 = 0\n",
    "            index1 += 1\n",
    "            continue\n",
    "        index1 += 1\n",
    "        index2 += 1\n",
    "        if index2 == len(pattern):\n",
    "            return index1 - index2\n",
    "        \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_attention(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             #temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset + 1\n",
    "    end = output.find('\\n', start)\n",
    "    att_output = model(tokens.cuda(), output_attentions=True)\n",
    "    weights = torch.mean(att_output.attentions[11][0][2][-4:-1], dim=-2)\n",
    "    return output[start:end].split(':')[-1].strip(), weights, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answer_with_attention(model, prompt, num_replicas=10):\n",
    "    model.train()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            offset = len(prompt)\n",
    "            start = offset + 1\n",
    "            end = text.find('\\n', start)\n",
    "            att_output = model(tokens.cuda(), output_attentions=True)\n",
    "            weights = torch.mean(att_output.attentions[11][0][2][-4:-1], dim=-2)\n",
    "            outputs.append((text[start:end].split(':')[-1].strip(), weights, tokens))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO IMPROVEMENTS\n",
    "* Do not just use layer 11\n",
    "* Average among all \"similar\" answers (same group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_text = get_text_from_data_item(dev_dict['data'][0], \n",
    "                                                 max_num_questions=8,\n",
    "                                                 question_number=0,\n",
    "                                                 last_question=False)\n",
    "tokens = tokenizer.encode(small_text, return_tensors='pt')\n",
    "answer_weights_tokens_list = generate_multiple_answer_with_attention(model, small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [tokenizer.convert_tokens_to_string(\n",
    "         tokenizer.convert_ids_to_tokens([item], skip_special_tokens=True)[0])\n",
    "         for item in tokens[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('orange',\n",
       " tensor([3.6089e-01, 8.1199e-05, 1.0805e-04, 6.5980e-05, 8.9267e-04, 3.7233e-05,\n",
       "         0.0000e+00, 1.0273e-04, 8.4571e-05, 6.3877e-05, 1.2993e-04, 7.8752e-04,\n",
       "         1.6993e-04, 7.4322e-06, 8.6734e-04, 4.2481e-03, 3.9622e-03, 5.9246e-04,\n",
       "         9.9492e-05, 1.0623e-04, 7.9502e-04, 2.6960e-04, 1.0184e-04, 5.8681e-03,\n",
       "         9.3598e-04, 4.1873e-05, 5.5301e-04, 4.0890e-04, 1.5313e-03, 1.4488e-03,\n",
       "         1.1528e-02, 1.7011e-03, 8.0181e-04, 5.3033e-03, 4.9567e-02, 1.8595e-03,\n",
       "         1.9864e-02, 5.4416e-03, 4.1015e-03, 8.2716e-04, 1.9289e-03, 7.8403e-05,\n",
       "         1.2960e-04, 1.7088e-04, 1.0379e-03, 3.4712e-03, 2.4533e-04, 4.0942e-04,\n",
       "         8.7080e-05, 8.9755e-04, 1.3617e-03, 2.6039e-04, 8.5770e-05, 1.7864e-04,\n",
       "         2.7513e-03, 2.0561e-04, 2.9695e-03, 2.6667e-03, 2.8464e-03, 1.2307e-03,\n",
       "         2.8963e-03, 1.6620e-03, 9.3932e-04, 1.6716e-03, 1.9829e-04, 9.3560e-04,\n",
       "         2.3852e-04, 2.8511e-04, 1.1969e-03, 1.4051e-05, 1.1094e-04, 6.2961e-04,\n",
       "         1.8531e-03, 2.2193e-04, 1.5989e-03, 1.6125e-03, 3.2344e-03, 1.5154e-03,\n",
       "         1.9374e-02, 7.9991e-03, 1.1944e-03, 1.5086e-03, 7.2038e-03, 1.4501e-03,\n",
       "         1.6834e-04, 4.1081e-03, 5.3220e-05, 4.6590e-03, 3.2261e-03, 4.1729e-04,\n",
       "         7.9744e-05, 6.1210e-04, 6.7814e-03, 3.9214e-04, 9.1676e-03, 3.8189e-04,\n",
       "         1.3807e-02, 1.4782e-03, 2.7661e-03, 3.6671e-03, 3.3441e-03, 2.1237e-03,\n",
       "         2.5146e-03, 2.2790e-04, 1.1358e-03, 1.5903e-03, 1.0716e-02, 8.6578e-04,\n",
       "         2.1518e-04, 2.3120e-04, 5.0318e-03, 5.0739e-03, 1.1386e-03, 4.4429e-04,\n",
       "         1.4276e-04, 6.6844e-04, 4.5771e-03, 8.4568e-05, 4.4300e-04, 3.9109e-02,\n",
       "         8.0756e-04, 2.0532e-02, 4.3029e-03, 9.6088e-03, 8.2088e-03, 9.4657e-04,\n",
       "         3.2295e-03, 5.8628e-04, 4.4887e-03, 4.8717e-04, 7.7319e-04, 9.5580e-05,\n",
       "         3.8119e-04, 1.5833e-03, 2.3375e-03, 8.9963e-05, 6.3887e-03, 2.7596e-03,\n",
       "         6.9010e-05, 6.8727e-04, 6.1013e-03, 2.5866e-03, 1.6873e-03, 1.1393e-03,\n",
       "         1.1141e-03, 4.6198e-04, 8.7882e-05, 1.0928e-04, 4.4336e-04, 1.2614e-03,\n",
       "         4.8478e-04, 9.4295e-04, 6.7500e-04, 1.0644e-03, 4.7347e-04, 2.0009e-03,\n",
       "         2.3662e-03, 9.6010e-04, 2.4876e-04, 4.1994e-04, 1.9241e-03, 1.4313e-03,\n",
       "         1.7176e-03, 8.4403e-05, 1.4072e-02, 1.1524e-02, 3.1664e-04, 6.7182e-04,\n",
       "         8.5280e-04, 2.2633e-04, 2.4461e-04, 6.7838e-03, 1.7294e-03, 5.9535e-04,\n",
       "         2.9589e-04, 1.5269e-03, 1.0864e-03, 8.3478e-04, 1.0214e-03, 4.2600e-04,\n",
       "         1.9056e-04, 1.0038e-03, 1.4570e-03, 3.3540e-04, 3.3611e-04, 4.8605e-04,\n",
       "         8.5757e-03, 1.8999e-03, 2.4679e-03, 3.3053e-03, 1.1575e-03, 5.8815e-03,\n",
       "         5.9596e-04, 3.0869e-04, 2.7054e-04, 2.9525e-04, 3.3238e-04, 2.9336e-03,\n",
       "         2.8851e-03, 5.1548e-03, 1.9949e-03, 1.9305e-03, 7.4727e-04, 2.7403e-04,\n",
       "         3.0083e-05, 7.3132e-05, 5.1723e-04, 8.6024e-05, 3.8149e-04, 6.8527e-04,\n",
       "         2.3783e-04, 3.0368e-03, 1.6084e-03, 1.5701e-03, 1.8657e-03, 9.8158e-04,\n",
       "         1.3365e-03, 1.0981e-03, 1.7175e-03, 6.3750e-04, 3.6503e-03, 1.5584e-04,\n",
       "         3.2564e-04, 8.1778e-05, 2.6410e-03, 3.9419e-04, 1.0376e-04, 1.5264e-04,\n",
       "         3.9090e-04, 1.3035e-03, 2.7491e-03, 6.7034e-04, 5.6835e-04, 5.6749e-04,\n",
       "         2.5512e-02, 2.9788e-04, 2.7578e-04, 3.5962e-04, 1.7349e-04, 2.2838e-03,\n",
       "         4.2889e-04, 4.5234e-04, 5.3537e-04, 1.1106e-03, 4.9497e-04, 1.9550e-04,\n",
       "         9.2681e-06, 2.2853e-04, 3.1096e-05, 1.7364e-04, 2.9833e-05, 3.7233e-05,\n",
       "         6.0555e-05, 3.5835e-05, 2.6677e-03, 2.0597e-03, 5.3849e-04, 2.3249e-04,\n",
       "         7.2292e-04, 8.4704e-04, 2.5460e-04, 8.3403e-05, 2.3263e-04, 3.8835e-03,\n",
       "         1.0416e-03, 5.2424e-04, 8.1197e-05, 1.6897e-03, 3.1250e-04, 1.9211e-04,\n",
       "         1.4051e-04, 1.4057e-04, 2.9231e-03, 2.5386e-04, 2.6990e-03, 8.0636e-04,\n",
       "         5.9662e-04, 5.7265e-04, 2.7959e-04, 3.7745e-04, 3.3683e-04, 1.1379e-04,\n",
       "         1.1308e-03, 1.2812e-04, 1.4186e-03, 7.2413e-04, 2.0264e-03, 2.6883e-04,\n",
       "         1.2209e-03, 1.6957e-04, 1.4260e-04, 2.0253e-04, 5.9596e-04, 4.7509e-05,\n",
       "         1.1483e-02, 1.7566e-04, 7.0107e-05, 8.9344e-06, 1.6971e-03, 2.1882e-03,\n",
       "         4.5136e-04, 1.9778e-03, 1.3686e-03, 1.4396e-03, 1.3852e-04, 1.4655e-05,\n",
       "         6.2270e-05, 2.9823e-05, 4.8990e-05, 6.4599e-05, 4.5424e-04, 7.1858e-04,\n",
       "         3.3852e-03, 2.9240e-04, 1.5357e-04, 1.8120e-03, 4.8538e-04, 4.7666e-04,\n",
       "         3.0542e-04, 1.9426e-04, 2.5980e-04, 1.5621e-04, 8.1491e-04, 1.8816e-04,\n",
       "         1.3836e-04, 5.1820e-04, 3.8854e-03, 3.7687e-03, 2.7895e-04, 3.8021e-04,\n",
       "         3.3007e-05, 1.6382e-04, 2.3140e-04, 1.1737e-05, 2.1254e-05, 1.2734e-05,\n",
       "         1.5747e-03, 1.1753e-03, 1.3004e-03, 1.8270e-03, 9.0531e-04, 1.5480e-03,\n",
       "         2.2922e-04, 1.5186e-04, 5.3273e-04, 3.3388e-04, 1.1998e-04, 1.8682e-04,\n",
       "         7.8306e-05, 9.1104e-04, 1.3292e-04, 4.0878e-04, 1.5775e-04, 6.6400e-04,\n",
       "         2.5580e-03, 1.6409e-03, 6.8133e-04, 2.8832e-05, 5.5665e-04, 4.5404e-03,\n",
       "         3.4263e-03, 8.9795e-04, 4.7305e-05, 2.3997e-03, 6.8932e-04, 6.3386e-03,\n",
       "         1.2145e-03, 0.0000e+00], device='cuda:0'),\n",
       " tensor([[  818,   262,  2420,  ..., 23915,    30,   198],\n",
       "         [  818,   262,  2420,  ..., 23915,    30,   198],\n",
       "         [  818,   262,  2420,  ..., 23915,    30,   198],\n",
       "         ...,\n",
       "         [  818,   262,  2420,  ..., 23915,    30,   198],\n",
       "         [  818,   262,  2420,  ..., 23915,    30,   198],\n",
       "         [  818,   262,  2420,  ..., 23915,    30,   198]]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_weights_tokens_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;15m\u001b[48;5;0mIn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m text\u001b[0m\u001b[38;5;0m\u001b[48;5;255m below\u001b[0m\u001b[38;5;0m\u001b[48;5;255m two\u001b[0m\u001b[38;5;0m\u001b[48;5;255m people\u001b[0m\u001b[38;5;0m\u001b[48;5;255m are\u001b[0m\u001b[38;5;0m\u001b[48;5;255m discussing\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m story\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mStory\u001b[0m\u001b[38;5;0m\u001b[48;5;255m:\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mOnce\u001b[0m\u001b[38;5;0m\u001b[48;5;255m upon\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m time\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m in\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m barn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m near\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m farm\u001b[0m\u001b[38;5;0m\u001b[48;5;255m house\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m there\u001b[0m\u001b[38;5;0m\u001b[48;5;255m lived\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m little\u001b[0m\u001b[38;5;0m\u001b[48;5;255m white\u001b[0m\u001b[38;5;15m\u001b[48;5;0m kitten\u001b[0m\u001b[38;5;0m\u001b[48;5;255m named\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m lived\u001b[0m\u001b[38;5;0m\u001b[48;5;255m high\u001b[0m\u001b[38;5;0m\u001b[48;5;255m up\u001b[0m\u001b[38;5;0m\u001b[48;5;255m in\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m nice\u001b[0m\u001b[38;5;0m\u001b[48;5;255m warm\u001b[0m\u001b[38;5;0m\u001b[48;5;255m place\u001b[0m\u001b[38;5;0m\u001b[48;5;255m above\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m barn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m where\u001b[0m\u001b[38;5;0m\u001b[48;5;255m all\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m farmer\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m horses\u001b[0m\u001b[38;5;0m\u001b[48;5;255m slept\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m But\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m wasn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m't\u001b[0m\u001b[38;5;0m\u001b[48;5;255m alone\u001b[0m\u001b[38;5;0m\u001b[48;5;255m in\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m little\u001b[0m\u001b[38;5;0m\u001b[48;5;255m home\u001b[0m\u001b[38;5;0m\u001b[48;5;255m above\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m barn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m oh\u001b[0m\u001b[38;5;0m\u001b[48;5;255m no\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m She\u001b[0m\u001b[38;5;0m\u001b[48;5;255m shared\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m hay\u001b[0m\u001b[38;5;0m\u001b[48;5;255m bed\u001b[0m\u001b[38;5;0m\u001b[48;5;255m with\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mom\u001b[0m\u001b[38;5;0m\u001b[48;5;255mmy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m 5\u001b[0m\u001b[38;5;0m\u001b[48;5;255m other\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sisters\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m All\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sisters\u001b[0m\u001b[38;5;0m\u001b[48;5;255m were\u001b[0m\u001b[38;5;0m\u001b[48;5;255m cute\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m fluffy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m But\u001b[0m\u001b[38;5;0m\u001b[48;5;255m she\u001b[0m\u001b[38;5;0m\u001b[48;5;255m was\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m only\u001b[0m\u001b[38;5;0m\u001b[48;5;255m white\u001b[0m\u001b[38;5;0m\u001b[48;5;255m one\u001b[0m\u001b[38;5;0m\u001b[48;5;255m in\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m bunch\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m The\u001b[0m\u001b[38;5;0m\u001b[48;5;255m rest\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sisters\u001b[0m\u001b[38;5;0m\u001b[48;5;255m were\u001b[0m\u001b[38;5;0m\u001b[48;5;255m all\u001b[0m\u001b[38;5;15m\u001b[48;5;0m orange\u001b[0m\u001b[38;5;0m\u001b[48;5;255m with\u001b[0m\u001b[38;5;0m\u001b[48;5;255m beautiful\u001b[0m\u001b[38;5;0m\u001b[48;5;255m white\u001b[0m\u001b[38;5;0m\u001b[48;5;255m tiger\u001b[0m\u001b[38;5;0m\u001b[48;5;255m stripes\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mom\u001b[0m\u001b[38;5;0m\u001b[48;5;255mmy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Being\u001b[0m\u001b[38;5;0m\u001b[48;5;255m different\u001b[0m\u001b[38;5;0m\u001b[48;5;255m made\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m quite\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sad\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m She\u001b[0m\u001b[38;5;0m\u001b[48;5;255m often\u001b[0m\u001b[38;5;0m\u001b[48;5;255m wished\u001b[0m\u001b[38;5;0m\u001b[48;5;255m she\u001b[0m\u001b[38;5;0m\u001b[48;5;255m looked\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m rest\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m family\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m So\u001b[0m\u001b[38;5;0m\u001b[48;5;255m one\u001b[0m\u001b[38;5;0m\u001b[48;5;255m day\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m when\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m found\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m can\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m the\u001b[0m\u001b[38;5;0m\u001b[48;5;255m old\u001b[0m\u001b[38;5;0m\u001b[48;5;255m farmer\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m orange\u001b[0m\u001b[38;5;0m\u001b[48;5;255m paint\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m she\u001b[0m\u001b[38;5;0m\u001b[48;5;255m used\u001b[0m\u001b[38;5;0m\u001b[48;5;255m it\u001b[0m\u001b[38;5;0m\u001b[48;5;255m to\u001b[0m\u001b[38;5;0m\u001b[48;5;255m paint\u001b[0m\u001b[38;5;0m\u001b[48;5;255m herself\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m them\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m When\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mom\u001b[0m\u001b[38;5;0m\u001b[48;5;255mmy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sisters\u001b[0m\u001b[38;5;0m\u001b[48;5;255m found\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m they\u001b[0m\u001b[38;5;0m\u001b[48;5;255m started\u001b[0m\u001b[38;5;0m\u001b[48;5;255m laughing\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mWhat\u001b[0m\u001b[38;5;0m\u001b[48;5;255m are\u001b[0m\u001b[38;5;0m\u001b[48;5;255m you\u001b[0m\u001b[38;5;0m\u001b[48;5;255m doing\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m?!\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mI\u001b[0m\u001b[38;5;0m\u001b[48;5;255m only\u001b[0m\u001b[38;5;0m\u001b[48;5;255m wanted\u001b[0m\u001b[38;5;0m\u001b[48;5;255m to\u001b[0m\u001b[38;5;0m\u001b[48;5;255m be\u001b[0m\u001b[38;5;0m\u001b[48;5;255m more\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m you\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\".\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mC\u001b[0m\u001b[38;5;0m\u001b[48;5;255motton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mom\u001b[0m\u001b[38;5;0m\u001b[48;5;255mmy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m rubbed\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m face\u001b[0m\u001b[38;5;0m\u001b[48;5;255m on\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m said\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mOh\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m but\u001b[0m\u001b[38;5;0m\u001b[48;5;255m your\u001b[0m\u001b[38;5;0m\u001b[48;5;255m fur\u001b[0m\u001b[38;5;0m\u001b[48;5;255m is\u001b[0m\u001b[38;5;0m\u001b[48;5;255m so\u001b[0m\u001b[38;5;0m\u001b[48;5;255m pretty\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m special\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m you\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m We\u001b[0m\u001b[38;5;0m\u001b[48;5;255m would\u001b[0m\u001b[38;5;0m\u001b[48;5;255m never\u001b[0m\u001b[38;5;0m\u001b[48;5;255m want\u001b[0m\u001b[38;5;0m\u001b[48;5;255m you\u001b[0m\u001b[38;5;0m\u001b[48;5;255m to\u001b[0m\u001b[38;5;0m\u001b[48;5;255m be\u001b[0m\u001b[38;5;0m\u001b[48;5;255m any\u001b[0m\u001b[38;5;0m\u001b[48;5;255m other\u001b[0m\u001b[38;5;0m\u001b[48;5;255m way\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\".\u001b[0m\u001b[38;5;0m\u001b[48;5;255m And\u001b[0m\u001b[38;5;0m\u001b[48;5;255m with\u001b[0m\u001b[38;5;0m\u001b[48;5;255m that\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mom\u001b[0m\u001b[38;5;0m\u001b[48;5;255mmy\u001b[0m\u001b[38;5;0m\u001b[48;5;255m picked\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m up\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m dropped\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m into\u001b[0m\u001b[38;5;0m\u001b[48;5;255m a\u001b[0m\u001b[38;5;0m\u001b[48;5;255m big\u001b[0m\u001b[38;5;0m\u001b[48;5;255m bucket\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m water\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m When\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m came\u001b[0m\u001b[38;5;0m\u001b[48;5;255m out\u001b[0m\u001b[38;5;0m\u001b[48;5;255m she\u001b[0m\u001b[38;5;0m\u001b[48;5;255m was\u001b[0m\u001b[38;5;0m\u001b[48;5;255m herself\u001b[0m\u001b[38;5;0m\u001b[48;5;255m again\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m sisters\u001b[0m\u001b[38;5;0m\u001b[48;5;255m l\u001b[0m\u001b[38;5;0m\u001b[48;5;255micked\u001b[0m\u001b[38;5;0m\u001b[48;5;255m her\u001b[0m\u001b[38;5;0m\u001b[48;5;255m face\u001b[0m\u001b[38;5;0m\u001b[48;5;255m until\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m's\u001b[0m\u001b[38;5;0m\u001b[48;5;255m fur\u001b[0m\u001b[38;5;0m\u001b[48;5;255m was\u001b[0m\u001b[38;5;0m\u001b[48;5;255m all\u001b[0m\u001b[38;5;0m\u001b[48;5;255m all\u001b[0m\u001b[38;5;0m\u001b[48;5;255m dry\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mDon\u001b[0m\u001b[38;5;0m\u001b[48;5;255m't\u001b[0m\u001b[38;5;0m\u001b[48;5;255m ever\u001b[0m\u001b[38;5;0m\u001b[48;5;255m do\u001b[0m\u001b[38;5;0m\u001b[48;5;255m that\u001b[0m\u001b[38;5;0m\u001b[48;5;255m again\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m!\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255m they\u001b[0m\u001b[38;5;0m\u001b[48;5;255m all\u001b[0m\u001b[38;5;0m\u001b[48;5;255m cried\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mNext\u001b[0m\u001b[38;5;0m\u001b[48;5;255m time\u001b[0m\u001b[38;5;0m\u001b[48;5;255m you\u001b[0m\u001b[38;5;0m\u001b[48;5;255m might\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mess\u001b[0m\u001b[38;5;0m\u001b[48;5;255m up\u001b[0m\u001b[38;5;0m\u001b[48;5;255m that\u001b[0m\u001b[38;5;0m\u001b[48;5;255m pretty\u001b[0m\u001b[38;5;0m\u001b[48;5;255m white\u001b[0m\u001b[38;5;0m\u001b[48;5;255m fur\u001b[0m\u001b[38;5;0m\u001b[48;5;255m of\u001b[0m\u001b[38;5;0m\u001b[48;5;255m yours\u001b[0m\u001b[38;5;0m\u001b[48;5;255m and\u001b[0m\u001b[38;5;0m\u001b[48;5;255m we\u001b[0m\u001b[38;5;0m\u001b[48;5;255m wouldn\u001b[0m\u001b[38;5;0m\u001b[48;5;255m't\u001b[0m\u001b[38;5;0m\u001b[48;5;255m want\u001b[0m\u001b[38;5;0m\u001b[48;5;255m that\u001b[0m\u001b[38;5;0m\u001b[48;5;255m!\"\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mThen\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m thought\u001b[0m\u001b[38;5;0m\u001b[48;5;255m,\u001b[0m\u001b[38;5;0m\u001b[48;5;255m \"\u001b[0m\u001b[38;5;0m\u001b[48;5;255mI\u001b[0m\u001b[38;5;0m\u001b[48;5;255m change\u001b[0m\u001b[38;5;0m\u001b[48;5;255m my\u001b[0m\u001b[38;5;0m\u001b[48;5;255m mind\u001b[0m\u001b[38;5;0m\u001b[48;5;255m.\u001b[0m\u001b[38;5;0m\u001b[48;5;255m I\u001b[0m\u001b[38;5;0m\u001b[48;5;255m like\u001b[0m\u001b[38;5;0m\u001b[48;5;255m being\u001b[0m\u001b[38;5;0m\u001b[48;5;255m special\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\".\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mDiscussion\u001b[0m\u001b[38;5;0m\u001b[48;5;255m:\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m\u001b[38;5;0m\u001b[48;5;255mQ\u001b[0m\u001b[38;5;0m\u001b[48;5;255m:\u001b[0m\u001b[38;5;0m\u001b[48;5;255m What\u001b[0m\u001b[38;5;0m\u001b[48;5;255m color\u001b[0m\u001b[38;5;0m\u001b[48;5;255m was\u001b[0m\u001b[38;5;0m\u001b[48;5;255m Cotton\u001b[0m\u001b[38;5;0m\u001b[48;5;255m?\u001b[0m\u001b[38;5;0m\u001b[48;5;255m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from colored import colored, fg, bg, attr\n",
    "\n",
    "\n",
    "answer, weights, tokens = answer_weights_tokens_list[7]\n",
    "\n",
    "regular_words = [' ', '\"', '\\'', '.', ',', '.\"', ':', '?', '!', '\".']\n",
    "\n",
    "start = subfinder(words, ['\\n', '\\n', 'Story', ':']) + 4\n",
    "end = subfinder(words, ['\\n', '\\n', 'Discussion', ':'])\n",
    "maximum = torch.max(weights[start:end])\n",
    "minimum = torch.min(weights[start:end])\n",
    "\n",
    "\n",
    "for word, weight, in zip(words, weights):\n",
    "    int_color = int(max(0, (1 - (weight - minimum)/(maximum - minimum))) * 20 + 235)\n",
    "    if int_color < 240:\n",
    "        int_color = 0\n",
    "        \n",
    "    else:\n",
    "        int_color = 255\n",
    "        \n",
    "    if word in regular_words:\n",
    "        int_color = 255\n",
    "        \n",
    "    if int_color > 245:\n",
    "        color = fg('black') + bg(int_color)\n",
    "        \n",
    "    else:\n",
    "        color = fg('white') + bg(int_color)\n",
    "        \n",
    "    res = attr('reset')\n",
    "    print(color + word + res, end='')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
