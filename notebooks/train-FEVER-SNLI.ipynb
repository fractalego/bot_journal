{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = json.load(open('../data/supporting.json'))\n",
    "refuting_texts = json.load(open('../data/refuting.json'))\n",
    "\n",
    "dev_supporting_texts = json.load(open('../data/dev_supporting.json'))\n",
    "dev_refuting_texts = json.load(open('../data/dev_refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160305"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supporting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59868"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refuting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Nikolaj Coster-Waldau ([neɡ̊olaɪ̯ kʰʌsd̥ɐ ˈʋald̥ɑʊ̯]; born 27 July 1970) is a Danish actor, producer and screenwriter . He graduated from Danish National School of Theatre in Copenhagen in 1993. Coster-Waldau 's breakthrough performance in Denmark was his role in the film Nightwatch (1994). Since then he has appeared in numerous films in his native Scandinavia and Europe in general, including Headhunters (2011) and A Thousand Times Good Night (2013). In the United States, his debut film role was in the war film Black Hawk Down (2001), playing Medal of Honor recipient Gary Gordon. He then played Detective John Amsterdam in the short-lived Fox television series New Amsterdam (2008), as well as appearing as Frank Pike in the 2009 Fox television film Virtuality, originally intended as a pilot. He became widely known to a broad audience for his current role as Ser Jaime Lannister, in the HBO series Game of Thrones. In 2017, he became one of the highest paid actors on television and earned # 2 million per episode of Game of ThronesThe Fox Broadcasting Company (often shortened to Fox and stylized as FOX) is an American English language commercial broadcast television network that is owned by the Fox Entertainment Group subsidiary of 21st Century Fox. The network is headquartered at the 20th Century Fox studio lot on Pico Boulevard in the Century City section of Los Angeles, with additional major offices and production facilities at the Fox Television Center in nearby West Los Angeles and the Fox Broadcasting Center in the Yorkville neighborhood of Manhattan, New York City. It is the third largest major television network in the world based on total revenues, assets, and international coverage. Launched on October 9, 1986 as a competitor to the Big Three television networks (ABC, NBC and CBS), Fox went on to become the most successful attempt at a fourth television network. It was the highest-rated broadcast network in the 18 -- 49 demographic from 2004 to 2012, and earned the position as the most-watched American television network in total viewership during the 2007 -- 08 season. Fox and its affiliated companies operate many entertainment channels in international markets, although these do not necessarily air the same programming as the U.S. network.\n",
      "\n",
      "\n",
      "Claim:\n",
      "Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n"
     ]
    }
   ],
   "source": [
    "print(supporting_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts += json.load(open('../data/snli_supporting.json'))\n",
    "refuting_texts += json.load(open('../data/snli_refuting.json'))\n",
    "\n",
    "dev_supporting_texts += json.load(open('../data/dev_snli_supporting.json'))\n",
    "dev_refuting_texts += json.load(open('../data/dev_snli_refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243055"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refuting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "split = 0.8\n",
    "\n",
    "_prompt = '\\n\\n\\nThe evidence supports the claim:\\n'\n",
    "train_list = [item + _prompt + 'Yes.' for item in supporting_texts]\n",
    "train_list += [item + _prompt + 'Nope.' for item in refuting_texts]\n",
    "random.shuffle(train_list)\n",
    "\n",
    "dev_list = [item + _prompt + 'Yes.' for item in dev_supporting_texts]\n",
    "dev_list += [item + _prompt + 'Nope.' for item in dev_refuting_texts]\n",
    "random.shuffle(dev_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_list, open('../data/train_list.json', 'w'))\n",
    "json.dump(dev_list, open('../data/dev_list.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/train_list.json'))\n",
    "dev_list = json.load(open('../data/dev_list.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586776"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Imagine Dragons is an American rock band from Las Vegas, Nevada, consisting of lead vocalist Dan Reynolds, lead guitarist Wayne Sermon, bassist Ben McKee, and drummer Daniel Platzman. The band first gained exposure in 2012 with the releases of their debut studio album Night Visions and its first single `` It 's Time ''. Billboard placed them at the top of their `` Year In Rock '' rankings for 2013, and named them their `` Breakthrough Band of 2013 ''. Rolling Stone named their single `` Radioactive '' from Night Visions the `` biggest rock hit of the year '', and MTV called them `` the year 's biggest breakout band ''. Night Visions peaked at number two on the weekly US Billboard 200 chart and the UK Albums Chart. The band 's second studio album Smoke + Mirrors was released in 2015, and reached number one in the US, Canada and the UK. Imagine Dragons has won two American Music Awards for Favorite Alternative Artist, a Grammy Award for Best Rock Performance, five Billboard Music Awards, and a World Music Award. In May 2014, the band was nominated for a total of fourteen different Billboard Music Awards, including Top Artist of the Year and a Milestone Award, which recognizes innovation and creativity of artists across different genres.\n",
      "\n",
      "\n",
      "Claim:\n",
      "Imagine Dragons have a drummer.\n",
      "\n",
      "\n",
      "The evidence supports the claim:\n",
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(train_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1809 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 341 out of 586776\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append(tokens)\n",
    "print(f'Skipped {total_skipped} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_batches = train_batches[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22453/586435 [20:21<8:31:12, 18.39it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-620dc759a2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#test(model, dev_list[:2000])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-eef11aac721d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_model, batches, optimizer, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    #test(model, dev_list[:2000])\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_fever' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    for item in tqdm(data):\n",
    "        expected = get_answer_from_text(item)\n",
    "        predicted = ''\n",
    "        try:\n",
    "            predicted = generate_answer(model, item)\n",
    "        except (IndexError, RuntimeError) as e:\n",
    "            #print(str(e))\n",
    "            #exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            #print(repr(traceback.extract_tb(exc_traceback)))\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if expected == predicted:\n",
    "            tp += 1\n",
    "        if expected == 'N' and predicted == 'Y':\n",
    "            fp += 1\n",
    "        if expected == 'Y' and predicted == 'N':\n",
    "            fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1:', f1)\n",
    "    print('Skipped:', skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    return text[:text.find(_claim_yn) + len(_claim_yn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 1\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_from_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_answer(model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 3\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    score = model(output, labels=output)[0]\n",
    "    out_text = tokenizer.decode(output[0][tokens_length:], skip_special_tokens=True)\n",
    "\n",
    "    return out_text, float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "checkpoint = torch.load(f'save_fever0')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 403/18924 [00:05<03:56, 78.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 18924/18924 [04:02<00:00, 78.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9407106048277732\n",
      "Recall: 0.9738866737799742\n",
      "F1: 0.9570112024722697\n",
      "Skipped: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = json.load(open('../data/adv_supporting.json'))\n",
    "refuting_texts = json.load(open('../data/adv_refuting.json'))\n",
    "\n",
    "dev_supporting_texts = json.load(open('../data/adv_dev_supporting.json'))\n",
    "dev_refuting_texts = json.load(open('../data/adv_dev_refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_list = supporting_texts + refuting_texts\n",
    "random.shuffle(train_list)\n",
    "\n",
    "dev_list = dev_supporting_texts + dev_refuting_texts\n",
    "random.shuffle(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 13724\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append(tokens)\n",
    "print(f'Skipped {total_skipped} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13724"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch = 5\n",
    "#checkpoint = torch.load(f'save_fever{epoch}')\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:48<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.8265762857699737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.398946189952684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:41<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 2.0794248728950744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.848062120221666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:39<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 1.6539967937258713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 1.513019484298695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:46<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 1.3904902372768198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:39<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 1.300064485630059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:34<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 1.2185287391775048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 1.1582863060133197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 1.101385164749951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 1.056454688653185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:45<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 1.0138452767031563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 0.987048549780362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 0.9519627090963975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 0.9310495258080663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:33<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 0.9055487251582932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 0.8882209499927384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 0.8701309680753325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 0.85805117948585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 0.8433181485769219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 0.833092354444466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 0.8230131825909107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.8151353167549658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 0.8054880833495393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 0.7983001110122787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.7931169445982843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 0.7878529269327509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:32<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 0.7815568830894757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:21<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 0.7777050177764047\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    #test(model, dev_list[:2000])\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_fever_adv' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5761947700631199\n",
      "Recall: 0.8565683646112601\n",
      "F1: 0.6889487870619946\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5304054054054054\n",
      "Recall: 0.9515151515151515\n",
      "F1: 0.6811279826464208\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8162031438935913\n",
      "Recall: 0.6343984962406015\n",
      "F1: 0.7139079851930196\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9070422535211268\n",
      "Recall: 0.56\n",
      "F1: 0.6924731182795699\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8591549295774648\n",
      "Recall: 0.6066907775768535\n",
      "F1: 0.7111817700052995\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5585970915312233\n",
      "Recall: 0.9328571428571428\n",
      "F1: 0.6987693953986089\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6425166825548141\n",
      "Recall: 0.8014268727705113\n",
      "F1: 0.7132275132275133\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7395048439181916\n",
      "Recall: 0.7053388090349076\n",
      "F1: 0.722017866526537\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8177514792899409\n",
      "Recall: 0.6506591337099812\n",
      "F1: 0.7246984792868381\n",
      "Skipped: 0\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.875\n",
      "Recall: 0.5932203389830508\n",
      "F1: 0.707070707070707\n",
      "Skipped: 0\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9634146341463414\n",
      "Recall: 0.5302013422818792\n",
      "F1: 0.683982683982684\n",
      "Skipped: 0\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7148803329864725\n",
      "Recall: 0.7292993630573248\n",
      "F1: 0.7220178665265371\n",
      "Skipped: 0\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6907114624505929\n",
      "Recall: 0.7740863787375415\n",
      "F1: 0.7300261096605745\n",
      "Skipped: 0\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7384780278670954\n",
      "Recall: 0.7088477366255144\n",
      "F1: 0.7233595800524933\n",
      "Skipped: 0\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7903780068728522\n",
      "Recall: 0.6679574056147144\n",
      "F1: 0.7240293809024133\n",
      "Skipped: 0\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7052313883299799\n",
      "Recall: 0.7594799566630552\n",
      "F1: 0.7313510693792383\n",
      "Skipped: 0\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6012323943661971\n",
      "Recall: 0.8951507208387942\n",
      "F1: 0.7193259610321222\n",
      "Skipped: 0\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7513397642015005\n",
      "Recall: 0.7123983739837398\n",
      "F1: 0.7313510693792383\n",
      "Skipped: 0\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8284313725490197\n",
      "Recall: 0.6282527881040892\n",
      "F1: 0.7145877378435518\n",
      "Skipped: 0\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8582375478927203\n",
      "Recall: 0.6081447963800904\n",
      "F1: 0.711864406779661\n",
      "Skipped: 0\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7415254237288136\n",
      "Recall: 0.720164609053498\n",
      "F1: 0.7306889352818372\n",
      "Skipped: 0\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.848180677540778\n",
      "Recall: 0.617351598173516\n",
      "F1: 0.7145877378435518\n",
      "Skipped: 0\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7298136645962733\n",
      "Recall: 0.7382198952879581\n",
      "F1: 0.7339927121290993\n",
      "Skipped: 0\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7173252279635258\n",
      "Recall: 0.7556029882604055\n",
      "F1: 0.735966735966736\n",
      "Skipped: 0\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7112676056338029\n",
      "Recall: 0.7610333692142088\n",
      "F1: 0.735309412376495\n",
      "Skipped: 0\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7526997840172787\n",
      "Recall: 0.7061803444782169\n",
      "F1: 0.7286983795086253\n",
      "Skipped: 0\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7643865363735071\n",
      "Recall: 0.7047047047047047\n",
      "F1: 0.7333333333333334\n",
      "Skipped: 0\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8231780167264038\n",
      "Recall: 0.6451310861423221\n",
      "F1: 0.7233595800524933\n",
      "Skipped: 0\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7822671156004489\n",
      "Recall: 0.6819960861056752\n",
      "F1: 0.7286983795086253\n",
      "Skipped: 0\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7274590163934426\n",
      "Recall: 0.7473684210526316\n",
      "F1: 0.7372793354101765\n",
      "Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever_adv{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8253768844221105\n",
      "Recall: 0.6100278551532033\n",
      "F1: 0.7015483182060865\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9275568181818182\n",
      "Recall: 0.5605150214592275\n",
      "F1: 0.6987693953986089\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7562642369020501\n",
      "Recall: 0.6626746506986028\n",
      "F1: 0.7063829787234043\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8538754764930114\n",
      "Recall: 0.6103542234332425\n",
      "F1: 0.7118644067796611\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8784313725490196\n",
      "Recall: 0.5983971504897596\n",
      "F1: 0.711864406779661\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7762237762237763\n",
      "Recall: 0.650390625\n",
      "F1: 0.7077577045696068\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8467243510506799\n",
      "Recall: 0.6272893772893773\n",
      "F1: 0.7206733298264071\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8681177976952625\n",
      "Recall: 0.6091644204851752\n",
      "F1: 0.7159450897571277\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7633333333333333\n",
      "Recall: 0.6849451645064806\n",
      "F1: 0.7220178665265371\n",
      "Skipped: 0\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8235294117647058\n",
      "Recall: 0.6417212347988774\n",
      "F1: 0.7213459516298634\n",
      "Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever_adv{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5278491859468724\n",
      "Recall: 0.9263157894736842\n",
      "F1: 0.6724890829694323\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.523645743766122\n",
      "Recall: 0.9199395770392749\n",
      "F1: 0.6673972602739725\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5274442538593482\n",
      "Recall: 0.924812030075188\n",
      "F1: 0.6717640633533589\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.521404109589041\n",
      "Recall: 0.9269406392694064\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5222222222222223\n",
      "Recall: 0.9299847792998478\n",
      "F1: 0.668856048166393\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5134907251264755\n",
      "Recall: 0.9530516431924883\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5165394402035624\n",
      "Recall: 0.9427244582043344\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5186125211505922\n",
      "Recall: 0.9474497681607419\n",
      "F1: 0.6703116457080373\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 43.91it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5c0ab998521f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a73c56bfcae5>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests with dev from COQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 5\n",
    "checkpoint = torch.load(f'save_fever{epoch}')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n"
     ]
    }
   ],
   "source": [
    "print(dev_dict['data'][0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = dev_dict['data'][0]['story'].replace('\\n\\n', '\\n').replace('\\n', '')\n",
    "\n",
    "text = f\"\"\"\n",
    "Evidence:\n",
    "{story}\n",
    "\n",
    "Claim:\n",
    "What color was Cotton? Blue.\n",
    "\n",
    "The evidence supports the claim:\n",
    "\"\"\"[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \"What are you doing, Cotton?!\" \"I only wanted to be more like you\". Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Claim:\n",
      "What color was Cotton? Blue.\n",
      "\n",
      "The evidence supports the claim:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yes.\\n\\n', 7.669206142425537)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_full_answer(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "number = 0\n",
    "small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                     max_num_questions=8,\n",
    "                                     question_number=number,\n",
    "                                     last_question=True)\n",
    "last_question = small_text.split('\\n')[-1].replace('A: ', '')\n",
    "discussion_anchor = 'Discussion:\\n'\n",
    "discussion = small_text[small_text.find(discussion_anchor) + len(discussion_anchor):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What color was Cotton?\n",
      "A: white\n"
     ]
    }
   ],
   "source": [
    "print(discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(2))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt(item, max_num_questions=0, question_number=-1, use_answer=None):\n",
    "    text = 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if use_answer:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n' + 'A: ' + use_answer + '\\n'\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statement_from_dialogue(statement_model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = statement_model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_from_data_item(item):\n",
    "    return item['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = get_description_from_data_item(dev_dict['data'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_prompt = get_statement_prompt(dev_dict['data'][index], \n",
    "                         max_num_questions=5,\n",
    "                         question_number=number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: What color was Cotton?\n",
      "A: white\n",
      "Statement:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(statement_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claim_from_description_and_dialogue(description, dialogue):\n",
    "    if not dialogue:\n",
    "        return ''\n",
    "    if dialogue[-1] == '.':\n",
    "        dialogue = dialogue[:-1]    \n",
    "    text = 'Evidence:\\n'\n",
    "    text += description.replace('\\n\\n', '\\n') + '\\n\\n'\n",
    "    text += 'Claim:\\n'\n",
    "    text += dialogue + '\\n\\n'\n",
    "    text += 'The evidence supports the claim:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "text = create_claim_from_description_and_dialogue(description, statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evidence:\\nOnce upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\\'s horses slept. But Cotton wasn\\'t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton\\'s mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer\\'s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\"What are you doing, Cotton?!\" \\n\"I only wanted to be more like you\". \\nCotton\\'s mommy rubbed her face on Cotton\\'s and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton\\'s mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton\\'s fur was all all dry. \\n\"Don\\'t ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn\\'t want that!\" \\nThen Cotton thought, \"I change my mind. I like being special\".\\n\\nClaim:\\nCotton was white\\n\\nThe evidence supports the claim:\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "sentence_model = sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_text(text):\n",
    "    outputs = sentence_model.encode(text)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model, dlist):\n",
    "    correct_predictions = []\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=8,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=True)\n",
    "            last_question = small_text.split('\\n')[-1].replace('A: ', '')\n",
    "            description = get_description_from_data_item(dev_dict['data'][index])\n",
    "            statement_prompt = get_statement_prompt(dev_dict['data'][index], \n",
    "                         max_num_questions=5,\n",
    "                         question_number=number)\n",
    "            statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "            text = create_claim_from_description_and_dialogue(description, statement)\n",
    "            try:\n",
    "                y_n_from_fever = generate_answer(model, text)\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "            \n",
    "            label = last_question\n",
    "            \n",
    "\n",
    "            if y_n_from_fever == 'Y' or y_n_from_fever == 'N' and label[:2].lower() == 'no':\n",
    "                correct_answers += 1\n",
    "\n",
    "                correct_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'label': label,\n",
    "                })                    \n",
    "            else:\n",
    "                wrong_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'label': label,\n",
    "                })                 \n",
    "\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:54<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285714285714286\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:49<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9235588972431078\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:39<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9223057644110275\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9273182957393483\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931077694235589\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423558897243107\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:38<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398496240601504\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9461152882205514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "wrong_predictions = []\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    accuracy, wrong_prediction, _, _ = compute_accuracy_of_model(model, dev_list[:50])\n",
    "    accuracies.append(accuracy)\n",
    "    wrong_predictions.append(wrong_prediction)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'number': 10,\n",
       "  'statement': 'The other cats licked her face when Cotton emerged from the bucket of water.',\n",
       "  'label': 'licked her face'},\n",
       " {'index': 1,\n",
       "  'number': 8,\n",
       "  'statement': 'They did not know what the note was.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 2, 'number': 1, 'statement': 'Yes.', 'label': 'Yes'},\n",
       " {'index': 2, 'number': 3, 'statement': 'I know her.', 'label': 'Yes'},\n",
       " {'index': 4,\n",
       "  'number': 5,\n",
       "  'statement': 'Kendra does not want to miss story time.',\n",
       "  'label': 'story time'},\n",
       " {'index': 5,\n",
       "  'number': 5,\n",
       "  'statement': 'Arthur Kill and the Kill Van Kull separate New York from new jersey.',\n",
       "  'label': 'Arthur Kill and the Kill Van Kull'},\n",
       " {'index': 7, 'number': 17, 'statement': 'He is 50.', 'label': '50'},\n",
       " {'index': 10, 'number': 8, 'statement': 'Mayweather is 38.', 'label': '38'},\n",
       " {'index': 11,\n",
       "  'number': 7,\n",
       "  'statement': 'Frederick G. Kilgour is not currently enrolled at the University.',\n",
       "  'label': 'He is not'},\n",
       " {'index': 14,\n",
       "  'number': 16,\n",
       "  'statement': \"Franz didn't stay in touch because he assumed Hans was dead.\",\n",
       "  'label': 'He assumed Hans was dead.'},\n",
       " {'index': 16,\n",
       "  'number': 12,\n",
       "  'statement': 'Ted regretted not paying more attention to Spotty.',\n",
       "  'label': 'yes'},\n",
       " {'index': 17,\n",
       "  'number': 4,\n",
       "  'statement': \"The dog wasn't.\",\n",
       "  'label': \"He wasn't\"},\n",
       " {'index': 20,\n",
       "  'number': 7,\n",
       "  'statement': 'Jenny helped unpack.',\n",
       "  'label': 'yes'},\n",
       " {'index': 21,\n",
       "  'number': 4,\n",
       "  'statement': 'Natasha did not follow their instructions.',\n",
       "  'label': 'She did not.'},\n",
       " {'index': 21, 'number': 12, 'statement': 'No.', 'label': 'Yes.'},\n",
       " {'index': 21,\n",
       "  'number': 15,\n",
       "  'statement': 'Sonors are boys or girls.',\n",
       "  'label': 'Sons'},\n",
       " {'index': 22,\n",
       "  'number': 6,\n",
       "  'statement': 'Peter did not want to be tired.',\n",
       "  'label': 'tired'},\n",
       " {'index': 23, 'number': 1, 'statement': 'the dog is ugly.', 'label': 'false'},\n",
       " {'index': 24,\n",
       "  'number': 10,\n",
       "  'statement': 'No, she changed her opinion.',\n",
       "  'label': 'yes'},\n",
       " {'index': 25,\n",
       "  'number': 1,\n",
       "  'statement': \"Sir Earl should not get too close to Archie's traces.\",\n",
       "  'label': \"Archie's traces.\"},\n",
       " {'index': 26,\n",
       "  'number': 6,\n",
       "  'statement': 'No commanders were retained.',\n",
       "  'label': 'yes'},\n",
       " {'index': 26,\n",
       "  'number': 16,\n",
       "  'statement': 'Lugo has no children.',\n",
       "  'label': 'yes'},\n",
       " {'index': 28,\n",
       "  'number': 8,\n",
       "  'statement': 'London commuter belt is not a place.',\n",
       "  'label': 'London commuter belt'},\n",
       " {'index': 29,\n",
       "  'number': 16,\n",
       "  'statement': 'hilip did not ask him not to tell her husband something.',\n",
       "  'label': 'yes'},\n",
       " {'index': 31,\n",
       "  'number': 6,\n",
       "  'statement': 'Marsha lives in a plastic bag.',\n",
       "  'label': 'A plastic bag.'},\n",
       " {'index': 31,\n",
       "  'number': 7,\n",
       "  'statement': 'Marsha stays fresh.',\n",
       "  'label': 'Yes.'},\n",
       " {'index': 31,\n",
       "  'number': 8,\n",
       "  'statement': \"Marsha's mom told her to soak him in water every few days.\",\n",
       "  'label': \"Marsha's mom told her to soak him in water every few days.\"},\n",
       " {'index': 32,\n",
       "  'number': 1,\n",
       "  'statement': 'The only secular book Anne saw while at whitehall was an odd volume of Parthenissa.',\n",
       "  'label': 'an odd volume of Parthenissa'},\n",
       " {'index': 32,\n",
       "  'number': 14,\n",
       "  'statement': 'eter Bridgeman did not have any pursuit.',\n",
       "  'label': 'only ones that were to her advantage'},\n",
       " {'index': 33,\n",
       "  'number': 11,\n",
       "  'statement': \"William couldn't speak well because he was so full of wrath.\",\n",
       "  'label': 'he was so full of wrath he could not speak'},\n",
       " {'index': 34,\n",
       "  'number': 8,\n",
       "  'statement': 'Chip and cake and candy are avoided.',\n",
       "  'label': 'chips and cake and candy'},\n",
       " {'index': 35,\n",
       "  'number': 16,\n",
       "  'statement': 'Martin reads the manuscript to himself.',\n",
       "  'label': 'Martin'},\n",
       " {'index': 35,\n",
       "  'number': 19,\n",
       "  'statement': \"Martin believes Maria may not love it because it's too strong for the magazines.\",\n",
       "  'label': \"It's too strong for the magazines\"},\n",
       " {'index': 36,\n",
       "  'number': 4,\n",
       "  'statement': 'And Barcelona.',\n",
       "  'label': 'Barcelona'},\n",
       " {'index': 37,\n",
       "  'number': 5,\n",
       "  'statement': 'The low estimate of the money they were spending was five dollars per week.',\n",
       "  'label': 'five dollars per week'},\n",
       " {'index': 37,\n",
       "  'number': 13,\n",
       "  'statement': 'Doing that would be a bad thing.',\n",
       "  'label': 'yes'},\n",
       " {'index': 38,\n",
       "  'number': 1,\n",
       "  'statement': 'Ryan and Adam are trying to be NBA players.',\n",
       "  'label': 'NBA players'},\n",
       " {'index': 40,\n",
       "  'number': 6,\n",
       "  'statement': \"Josephine didn't turn up.\",\n",
       "  'label': 'Josephine'},\n",
       " {'index': 42,\n",
       "  'number': 11,\n",
       "  'statement': 'They find no sign that he has been there.',\n",
       "  'label': 'yes'},\n",
       " {'index': 43,\n",
       "  'number': 1,\n",
       "  'statement': 'Unknowable is the one word best described as unknown.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 44,\n",
       "  'number': 7,\n",
       "  'statement': 'Wnated liberty was not cooperating.',\n",
       "  'label': 'wnated liberty'},\n",
       " {'index': 44,\n",
       "  'number': 8,\n",
       "  'statement': 'Paul did not get it.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 45,\n",
       "  'number': 4,\n",
       "  'statement': 'Bonn is in the northernmost zone.',\n",
       "  'label': 'southernmost'},\n",
       " {'index': 45,\n",
       "  'number': 16,\n",
       "  'statement': 'Bonn served as the seat of government, but not the capitol from 1990 to 1999.',\n",
       "  'label': 'Bonn served as the seat of government, but not the capitol'},\n",
       " {'index': 46,\n",
       "  'number': 9,\n",
       "  'statement': 'Lodi was not their only hope.',\n",
       "  'label': 'He was their only hope.'},\n",
       " {'index': 49,\n",
       "  'number': 11,\n",
       "  'statement': 'Harry did not sell the telegraph.',\n",
       "  'label': 'Harry'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY WITHOUT ADDITIONAL NEGATIONS IN TRAINING DATA!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_list = json.load(open('../data/adversarial_claims_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the text below two people are discussing a story.\n",
      "\n",
      "Story:\n",
      "My doorbell rings. On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
      "\n",
      "I know this lady. It is not her first visit. She is the boy's grandmother, and her daughter bought the house next door last October. \n",
      "\n",
      "Her daughter, Nicole, speaks fluent English. But she is now in Shanghai, and her parents are here with the little boy. Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients. \n",
      "\n",
      "I know what is inside the bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake. This has become an almost-daily practice. \n",
      "\n",
      "Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello. Once, she brought an iPad as well as the food. She pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go with her to her house. Then, she handed the iPad to her husband and almost immediately I found myself looking at Nicole in Shanghai and discussing her mother's cooking and salt intake. Instantly, tears welled in my eyes. \n",
      "\n",
      "\"Your mother just can't be bringing me meals like this all the time,\" I insisted. \"I can hardly do dishes in return.\" \n",
      "\n",
      "\"Oh, no, Lucy.\" Nicole said. \"Mum doesn't like western food. Don't worry about it; she has to cook for the three of them anyway, and she wants to do it.\" \n",
      "\n",
      "The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
      "\n",
      "I am now working on some more Chinese words--it's the least I can do after such display of kindness. \n",
      "\n",
      "\"Thank you\" is, of course, the first one. Somehow, it seems inadequate.\n",
      "\n",
      "Discussion:\n",
      "Q: How is she related to the boy?\n",
      "WA: He is his grandmother\n",
      "CA: mother.\n",
      "Q: What is in the bag?\n",
      "WA: rice, vegetables and either chicken, meat or shrimp\n",
      "CA: food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(adversarial_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_correct_answer_prompt = '\\nCA: '\n",
    "_wrong_answer_prompt = '\\nWA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_correct_answer_prompt)\n",
    "\n",
    "def get_correct_answer_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_correct_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_correct_answer_prompt))\n",
    "    return text[pos + len(_correct_answer_prompt):end]\n",
    "\n",
    "def get_wrong_answer_number(text, number):\n",
    "    pos = text.find(_wrong_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_wrong_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_wrong_answer_prompt))\n",
    "    return text[pos + len(_wrong_answer_prompt):end]\n",
    "\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_description_from_text(text):\n",
    "    start_prompt = 'Story:'\n",
    "    end_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):text.find(end_prompt)]\n",
    "\n",
    "def get_discussion_from_text(text):\n",
    "    start_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):].strip()\n",
    "\n",
    "def get_statement_prompt_from_text(full_text, number, max_questions=5):\n",
    "    text = 'Discussion:\\n'\n",
    "    questions_and_answers_list = get_discussion_from_text(full_text).split('\\n')\n",
    "    start = max(0, (number + 1 - max_questions) * 3)\n",
    "    end = (number + 1) * 3\n",
    "    questions_and_answers_list = questions_and_answers_list[start:end]\n",
    "    text += '\\n'.join(questions_and_answers_list)\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q: Whose paint was it?\\nWA: Cotton's mommy\\nCA: the farmer.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_discussion_from_text(adversarial_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: How is she related to the boy?\n",
      "WA: He is his grandmother\n",
      "CA: mother.\n",
      "Q: What is in the bag?\n",
      "WA: rice, vegetables and either chicken, meat or shrimp\n",
      "CA: food.\n",
      "Statement:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_statement_prompt_from_text(adversarial_list[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the farmer.\n"
     ]
    }
   ],
   "source": [
    "print(get_correct_answer_number(adversarial_list[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotton's mommy\n"
     ]
    }
   ],
   "source": [
    "print(get_wrong_answer_number(adversarial_list[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adversarial_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([get_answers_number(item) for item in adversarial_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answers_number(adversarial_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_y_n_answers(model, prompt, num_replicas=25):\n",
    "    model.train()\n",
    "    outputs_count = {}\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            answer = get_answer_from_text(text)\n",
    "            outputs_count.setdefault(answer, 0)\n",
    "            outputs_count[answer] += 1\n",
    "\n",
    "    total = sum(v for v in outputs_count.values())\n",
    "    return [(k, v / total) for k, v in outputs_count.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_anchor = 'Discussion:\\n'\n",
    "\n",
    "def compute_precision_of_model(model, dlist):\n",
    "    correct_predictions = []\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        total_questions = get_answers_number(text)\n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = text\n",
    "            description = get_description_from_text(text)\n",
    "            statement_prompt = get_statement_prompt_from_text(text, number)\n",
    "            statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "            claim = create_claim_from_description_and_dialogue(description, statement)\n",
    "            #try:\n",
    "            y_n_with_score = generate_multiple_y_n_answers(model, claim)\n",
    "            #except RuntimeError:\n",
    "            #    continue\n",
    "            \n",
    "\n",
    "            #if y_n_with_score[0][0] == 'N':\n",
    "            if len(y_n_with_score) == 2:\n",
    "                correct_answers += 1\n",
    "\n",
    "                correct_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'yn': y_n_with_score,\n",
    "                })\n",
    "            else:\n",
    "                wrong_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'yn': y_n_with_score,\n",
    "                })\n",
    "\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "    print(total_number_of_questions)\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.03508771929824561\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:07<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.08771929824561403\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.12280701754385964\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.14035087719298245\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.10526315789473684\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.03508771929824561\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.10526315789473684\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.07017543859649122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "wrong_predictions = []\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    config = GPT2Config(attn_pdrop=0.01, resid_pdrop=0.01, embd_pdrop=0.01)\n",
    "    model = GPT2LMHeadModel(config).from_pretrained('gpt2').cuda()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    accuracy, wrong_prediction, _, _ = compute_precision_of_model(model, adversarial_list)\n",
    "    accuracies.append(accuracy)\n",
    "    wrong_predictions.append(wrong_prediction)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'number': 0,\n",
       "  'statement': 'The library for history, law, philosophy, science and theology. is the library for.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 0,\n",
       "  'number': 1,\n",
       "  'statement': '150,000 books survived the Pre Lateran period.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 0,\n",
       "  'statement': 'Venters called Lassiter a dark bay.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 1,\n",
       "  'statement': 'The man who had led Milly Erne to Cottonwoods was oppressing her.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 2,\n",
       "  'statement': 'Venters was hoping she could keep the need of a helper from happening to her.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 3,\n",
       "  'number': 0,\n",
       "  'statement': 'He saves the Abominable Snow Monster during a snow storm.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 4,\n",
       "  'number': 0,\n",
       "  'statement': 'Francesco noticed it.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 4,\n",
       "  'number': 1,\n",
       "  'statement': \"The garrison was in Francesco's group.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 6,\n",
       "  'number': 0,\n",
       "  'statement': 'BY WHET THEY IDENTIFY that BUS, they knowingly participated in the planned hazing activity over several years.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 6,\n",
       "  'number': 1,\n",
       "  'statement': 'Their MAIN FOCUS is to fix it.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 7,\n",
       "  'number': 0,\n",
       "  'statement': 'The driver should have called the bus driver.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 0,\n",
       "  'statement': 'It began in the early 1990s.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 1,\n",
       "  'statement': 'It began easier to compare prices in online.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 2,\n",
       "  'statement': \"A reason online is cheaper is it's easier to compare prices.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 3,\n",
       "  'statement': \"Because they don't offer a service like we do. Why is it easier to compare prices.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 0,\n",
       "  'statement': 'The energy source for an incandescent bulb is electric current.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 1,\n",
       "  'statement': 'In a halogen bulb, by feeding-through terminals or wires embedded in the glass.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 2,\n",
       "  'statement': 'They convert 16 lumens per watt to light.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 11,\n",
       "  'number': 0,\n",
       "  'statement': 'We would have diagnosed him with today by diagnosing him with learning to read and write.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 12,\n",
       "  'number': 0,\n",
       "  'statement': 'The this period began when the beginning of farming.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 13,\n",
       "  'number': 0,\n",
       "  'statement': 'People might have kept quail in captivity to feed them. Because they are to feed them.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 13,\n",
       "  'number': 2,\n",
       "  'statement': 'The term poultry is defined as small animal.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 15,\n",
       "  'number': 0,\n",
       "  'statement': 'This divorce fits a certain trope.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 16,\n",
       "  'number': 0,\n",
       "  'statement': 'L into the pool.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 16,\n",
       "  'number': 1,\n",
       "  'statement': 'He got closer to the nk of the pool.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 18,\n",
       "  'number': 0,\n",
       "  'statement': 'Its values are mirrored by a culture.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 18,\n",
       "  'number': 1,\n",
       "  'statement': 'He thought it was sudden glory arising from sudden thought of feeling far better than others.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 19,\n",
       "  'number': 0,\n",
       "  'statement': 'A child was beat.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 19,\n",
       "  'number': 1,\n",
       "  'statement': \"His condition was `` severely beaten '' with `` fists and feet. ''. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 21,\n",
       "  'number': 0,\n",
       "  'statement': 'About that and that and that.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 21,\n",
       "  'number': 1,\n",
       "  'statement': 'It made the giant feel like he was asking me about the beans and their roots.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 22,\n",
       "  'number': 0,\n",
       "  'statement': 'A specific identy emerged in the Celtic Britons.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 23,\n",
       "  'number': 0,\n",
       "  'statement': 'The first form of brackets in written English was a bracket.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 24,\n",
       "  'number': 0,\n",
       "  'statement': 'The recovery helped.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 24,\n",
       "  'number': 1,\n",
       "  'statement': 'The recovery helped to get it introduced.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 25,\n",
       "  'number': 0,\n",
       "  'statement': 'Lilly drew a picture of her best friend Lilly.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 25,\n",
       "  'number': 1,\n",
       "  'statement': 'Lilly drew this time by her mother.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 26,\n",
       "  'number': 1,\n",
       "  'statement': \"Rochester got the `` Places Rated Almanac '' in 2007. ''s. Rated Almanac. '' in 2007. ''s. was. '' Places Rated Almanac. '' in 2007. ''s. was. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 27,\n",
       "  'number': 0,\n",
       "  'statement': 'There have been 17,400 people in the last few years.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 29,\n",
       "  'number': 0,\n",
       "  'statement': \"The use of the word `` water '' on is used to help people who fall into the water. '' is used on to help people who fall into the water. '' is used on to help people who fall into the water. '' is used o\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 30,\n",
       "  'number': 1,\n",
       "  'statement': 'If he makes a mess in his diaper, he is worried.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 30,\n",
       "  'number': 2,\n",
       "  'statement': 'They got a rattle.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 31,\n",
       "  'number': 0,\n",
       "  'statement': \"In the beaver's expression when he answered, `` Well, now that you have found out that you cann't do that, you are going to do that. '' was in the beaver's expression when he answered. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 32,\n",
       "  'number': 0,\n",
       "  'statement': 'He made it to the Porta Pinti today.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 33,\n",
       "  'number': 0,\n",
       "  'statement': 'For three hours, the game is played for three hours.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 34,\n",
       "  'number': 0,\n",
       "  'statement': 'He was searching vainly.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 34,\n",
       "  'number': 1,\n",
       "  'statement': \"D'Arbino went to seek help from Count Fabio d'Ascoli.\",\n",
       "  'yn': [('Y', 1.0)]}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model with custom sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"add batteries to the shopping list\"\n",
    "text= create_claim_from_description_and_dialogue(f\"When asked what item do you want to add, the user replies: '{item}.'.\", \n",
    "                                                 f\"'{item}' belongs to the shopping list\")\n",
    "                                                 #f\"'{item}' looks like a grocery item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "When asked what item do you want to add, the user replies: 'add batteries to the shopping list.'.\n",
      "\n",
      "Claim:\n",
      "'add batteries to the shopping list' belongs to the shopping list\n",
      "\n",
      "The evidence supports the claim:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "generate_answer(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Y', 1.0)]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_multiple_y_n_answers(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "The user says: 'Add earthworms'.\n",
      "\n",
      "Claim:\n",
      "'earthworms' belongs to a shopping list \n",
      "\n",
      "The evidence supports the claim:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Y', 0.92), ('N', 0.08)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = \"earthworms\"\n",
    "text= create_claim_from_description_and_dialogue(f\"The user says: 'Add {item}'.\", \n",
    "                                                 f\"'{item}' belongs to a shopping list \")\n",
    "\n",
    "print(text)\n",
    "\n",
    "model.eval()\n",
    "generate_answer(model, text)\n",
    "\n",
    "generate_multiple_y_n_answers(model, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/fractalego/fact-checker/commit/a3185c8c177d8866908ea46c6b40abe9c7afddcb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "epoch = 5\n",
    "checkpoint = torch.load(f'save_fever{epoch}')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.push_to_hub(\"fractalego/fact-checking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/fractalego/fact-checker/commit/ef06b4530a000f7671efed80a4440e752f2da351'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.push_to_hub(\"fractalego/fact-checking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
