{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb85ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5e0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicts = [json.loads(item) for item in open('../data/FEVER/train.jsonl', encoding='utf8').readlines()]\n",
    "dev_dicts = [json.loads(item) for item in open('../data/FEVER/paper_dev.jsonl', encoding='utf8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d585929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line[2:line[2:].find('\\t')]\n",
    "    line = line.replace('-LRB-', '(')\n",
    "    line = line.replace('-RRB-', ')')\n",
    "    line = line.replace('-LSB-', '[')\n",
    "    line = line.replace('-RSB-', ']')\n",
    "    line = line.replace('( ', '(')\n",
    "    line = line.replace(' )', ')')\n",
    "    line = line.replace('[ ', '[')\n",
    "    line = line.replace(' ]', ']')\n",
    "    line = line.replace(' .', '.')\n",
    "    line = line.replace(' , ', ', ')\n",
    "    line = line.replace(' ; ', '; ')\n",
    "    line = line.replace(' : ', ': ')\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e68c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 75397,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'SUPPORTS',\n",
       " 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.',\n",
       " 'evidence': [[[92206, 104971, 'Nikolaj_Coster-Waldau', 7],\n",
       "   [92206, 104971, 'Fox_Broadcasting_Company', 0]]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5613ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043366e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_lines_dict = {}\n",
    "_wiki_folder = '../data/FEVER/wiki-pages'\n",
    "for file in os.listdir(_wiki_folder):\n",
    "    for item in open(f'{_wiki_folder}/{file}', encoding='utf8').readlines():\n",
    "        wiki_dict = json.loads(item)\n",
    "        wiki_id = wiki_dict['id']\n",
    "        lines = [clean_line(line) for line in wiki_dict['lines'].split('\\n')]\n",
    "        wiki_lines_dict[wiki_id] = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876124e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_texts_for_label(data_dicts, label):\n",
    "    all_texts = []\n",
    "\n",
    "    for item in data_dicts:\n",
    "        if item['label'] == label:\n",
    "            for paragraph in item['evidence']:\n",
    "                text = ''\n",
    "                for line in paragraph:\n",
    "                    wiki_id = line[-2]\n",
    "                    sentence_num = line[-1]\n",
    "                    if wiki_id not in wiki_lines_dict:\n",
    "                        continue\n",
    "                    to_add = '. '.join([item for item in wiki_lines_dict[wiki_id] if item.strip()]) \n",
    "                    if to_add not in text:\n",
    "                        text += to_add\n",
    "                text = text.strip()\n",
    "                if text:\n",
    "                    text += '.\\n'\n",
    "                if text and text != '.':\n",
    "                    claim = item['claim']\n",
    "                    all_texts.append('Evidence:\\n'\n",
    "                                     + text + '\\n\\n'\n",
    "                                     + 'Claim:\\n'\n",
    "                                     + claim\n",
    "                                    )\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6908e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = populate_texts_for_label(train_dicts, 'SUPPORTS')\n",
    "refuting_texts = populate_texts_for_label(train_dicts, 'REFUTES')\n",
    "\n",
    "dev_supporting_texts = populate_texts_for_label(dev_dicts, 'SUPPORTS')\n",
    "dev_refuting_texts = populate_texts_for_label(dev_dicts, 'REFUTES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07afd4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160305"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(supporting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638d6c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59868"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refuting_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623dd6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "The Ten Commandments is a 1956 American biblical epic film produced, directed, and narrated by Cecil B. DeMille, shot in VistaVision (color by Technicolor), and released by Paramount Pictures. The film is based on Prince of Egypt by Dorothy Clarke Wilson, Pillar of Fire by J.H. Ingraham, On Eagle 's Wings by A.E. Southon, and the Book of Exodus. The Ten Commandments dramatizes the biblical story of the life of Moses, an adopted Egyptian prince who becomes the deliverer of his real brethren, the enslaved Hebrews, and therefore leads the Exodus to Mount Sinai, where he receives, from God, the Ten Commandments. The film stars Charlton Heston in the lead role, Yul Brynner as Rameses, Anne Baxter as Nefretiri, Edward G. Robinson as Dathan, Yvonne De Carlo as Sephora, Debra Paget as Lilia, and John Derek as Joshua; and features Sir Cedric Hardwicke as Sethi, Nina Foch as Bithiah, Martha Scott as Yoshebel, Judith Anderson as Memnet, and Vincent Price as Baka, among others. Filmed on location in Egypt, Mount Sinai and the Sinai Peninsula, the film was DeMille 's last and most successful work. It is a partial remake of his 1923 silent film of the same title, and features one of the largest sets ever created for a film. The film was released to cinemas in the USA on 5 October 1956 and, at the time of its release, was the most expensive film ever made.\n",
      "\n",
      "\n",
      "Claim:\n",
      "The Ten Commandments is an epic film.\n"
     ]
    }
   ],
   "source": [
    "print(supporting_texts[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ebfb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d120cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_claim(claim):\n",
    "    negated_sentence_words = []\n",
    "    tuples = nlp(claim)\n",
    "    to_negate = True\n",
    "    \n",
    "    for token in tuples:\n",
    "        tag = token.tag_\n",
    "        if tag not in ['VB', 'VBZ', 'VBD', 'VBP']:\n",
    "            negated_sentence_words.append(token.text)\n",
    "        elif to_negate:\n",
    "            if tag == 'VB':\n",
    "                negated_sentence_words.append('not')\n",
    "                negated_sentence_words.append(token.text)\n",
    "                to_negate = False\n",
    "                continue\n",
    "                \n",
    "            if token.text == 'is':\n",
    "                negated_sentence_words.append(token.text)\n",
    "                negated_sentence_words.append('not')\n",
    "                to_negate = False\n",
    "                continue\n",
    "                \n",
    "            if tag == 'VBP':\n",
    "                negated_sentence_words.append('do not')\n",
    "                negated_sentence_words.append(token.lemma_)\n",
    "                to_negate = False\n",
    "                continue\n",
    "                \n",
    "            if tag == 'VBZ':\n",
    "                negated_sentence_words.append('does not')\n",
    "                negated_sentence_words.append(token.lemma_)\n",
    "                to_negate = False\n",
    "                continue\n",
    "                \n",
    "            if tag == 'VBD':\n",
    "                negated_sentence_words.append('did not')\n",
    "                negated_sentence_words.append(token.lemma_)\n",
    "                to_negate = False\n",
    "                continue\n",
    "        else:\n",
    "            negated_sentence_words.append(token.text)\n",
    "            \n",
    "    return ' '.join(negated_sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d250e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice did not have a car that she used to drive to the church where she sang'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invert_claim('Alice had a car that she used to drive to the church where she sang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bfa45a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_negative_texts_for_label(data_dicts, label):\n",
    "    all_texts = []\n",
    "\n",
    "    for item in data_dicts:\n",
    "        if item['label'] == label:\n",
    "            for paragraph in item['evidence']:\n",
    "                text = ''\n",
    "                for line in paragraph:\n",
    "                    wiki_id = line[-2]\n",
    "                    sentence_num = line[-1]\n",
    "                    if wiki_id not in wiki_lines_dict:\n",
    "                        continue\n",
    "                    to_add = '. '.join([item for item in wiki_lines_dict[wiki_id] if item.strip()]) \n",
    "                    if to_add not in text:\n",
    "                        text += to_add\n",
    "                text = text.strip()\n",
    "                if text:\n",
    "                    text += '.\\n'\n",
    "                if text and text != '.':\n",
    "                    claim = item['claim']\n",
    "                    claim = invert_claim(claim)\n",
    "                    all_texts.append('Evidence:\\n'\n",
    "                                     + text + '\\n\\n'\n",
    "                                     + 'Claim:\\n'\n",
    "                                     + claim\n",
    "                                    )\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f169bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sampled_train = random.sample(train_dicts, len(train_dicts)//10)\n",
    "sampled_dev = random.sample(dev_dicts, len(dev_dicts)//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d31dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "refuting_texts += populate_negative_texts_for_label(sampled_train, 'SUPPORTS')\n",
    "supporting_texts += populate_negative_texts_for_label(sampled_train, 'REFUTES')\n",
    "\n",
    "dev_refuting_texts += populate_negative_texts_for_label(sampled_dev, 'SUPPORTS')\n",
    "dev_supporting_texts += populate_negative_texts_for_label(sampled_dev, 'REFUTES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5607f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(supporting_texts, open('../data/supporting.json', 'w'))\n",
    "json.dump(refuting_texts, open('../data/refuting.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f59d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dev_supporting_texts, open('../data/dev_supporting.json', 'w'))\n",
    "json.dump(dev_refuting_texts, open('../data/dev_refuting.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30701ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
