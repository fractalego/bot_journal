{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/qa_train_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list = json.load(open('../data/qa_dev_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_small' + str(6))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = \"cuda\"\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokens, length):\n",
    "    return model.generate(\n",
    "        tokens.to(device),\n",
    "        max_length=tokens.shape[1] + length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt, topk=10):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        text = prompt\n",
    "        start = len(prompt)\n",
    "        while \"\\n\" not in text[start:]:\n",
    "            output = inference(model, tokens, length)\n",
    "            decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            text += decoded[len(text):]\n",
    "            tokens = output\n",
    "\n",
    "    end = text.find('\\n', start)\n",
    "    return text[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_answers(model, prompt, num_replicas=10):\n",
    "    model.eval()\n",
    "    \n",
    "    answers_and_scores = [(\"\", 1) for _ in range(num_replicas*num_replicas)]\n",
    "    \n",
    "    texts = [prompt] * num_replicas\n",
    "    start = len(prompt)\n",
    "    with torch.no_grad():\n",
    "        while any(\"\\n\" not in text[start:] for text in texts):\n",
    "            tokens = tokenizer.batch_encode_plus(texts, return_tensors='pt').input_ids\n",
    "            tokens = tokens.to(device)\n",
    "            output = model(tokens)\n",
    "            for answer_index in range(num_replicas):\n",
    "                probs = torch.softmax(output.logits[answer_index][-1], dim=-1)\n",
    "                indices = torch.topk(probs, k=num_replicas).indices\n",
    "                for output_index, token_index in enumerate(indices):\n",
    "                    p = probs[token_index]\n",
    "                    total_index = answer_index * num_replicas + output_index\n",
    "                    new_tokens = tokens[answer_index].tolist() + [int(token_index)]\n",
    "                    decoded_text = tokenizer.decode(new_tokens)\n",
    "                    new_score =  answers_and_scores[total_index][1] * float(p)\n",
    "                    answers_and_scores[total_index] = decoded_text, new_score\n",
    "            \n",
    "            new_answers_and_scores = []\n",
    "            already_answered = set()\n",
    "            for answer, score in answers_and_scores:\n",
    "                if answer in already_answered:\n",
    "                    continue\n",
    "                \n",
    "                new_answers_and_scores.append((answer, score))\n",
    "                already_answered.add(answer)\n",
    "            \n",
    "            new_answers_and_scores = sorted(new_answers_and_scores, key=lambda x: -x[1])\n",
    "            texts = [new_answers_and_scores[index][0] for index in range(num_replicas)]\n",
    "            for i in range(num_replicas):\n",
    "                for j in range(num_replicas):\n",
    "                    answers_and_scores[i * j] = new_answers_and_scores[j]\n",
    "\n",
    "    scores = []\n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', start)\n",
    "        texts[index] = texts[index][start:end].split(':')[-1].strip()\n",
    "        scores.append(answers_and_scores[index][1])\n",
    "\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.3 ms, sys: 821 Âµs, total: 64.1 ms\n",
      "Wall time: 63.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "answers, scores = generate_all_answers(model, prompt, num_replicas=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['st. George', 'St. George', 'Sir George', 'George']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29918041831489056,\n",
       " 0.29318870276476167,\n",
       " 0.29318870276476167,\n",
       " 0.29318870276476167]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers_with_dropout(model, prompt, num_replicas=25, length=5):\n",
    "    model.train()\n",
    "    outputs = []\n",
    "    start = len(prompt)\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        texts = [prompt] * num_replicas\n",
    "        while any(\"\\n\" not in text[start:] for text in texts):\n",
    "            if tokens.shape[1] + length > 1024:\n",
    "                break\n",
    "            \n",
    "            output = model.generate(\n",
    "                tokens.cuda(),\n",
    "                max_length=tokens.shape[1] + length,\n",
    "                pad_token_id=50256\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            tokens = output\n",
    "            \n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', start)\n",
    "        texts[index] = texts[index][start:end].split(':')[-1].strip()\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.9 ms, sys: 0 ns, total: 75.9 ms\n",
      "Wall time: 75.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['St. George',\n",
       " 'st. George',\n",
       " 'George',\n",
       " 'st. George',\n",
       " 'st. George',\n",
       " \"George's\",\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'st. George',\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'St. George',\n",
       " 'st. George']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers(model, prompt, num_replicas, length=100):\n",
    "    tokens = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "    tokens_length = tokens.input_ids.shape[1]\n",
    "    if tokens_length + length > 1024:\n",
    "        return ''\n",
    "    generated_ids = model.generate(**tokens,\n",
    "                                   num_beams=num_replicas,\n",
    "                                   num_return_sequences=num_replicas,\n",
    "                                   max_length=tokens_length + length,\n",
    "                                  )\n",
    "    generated_sentences = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    sentences = []\n",
    "    for index, item in enumerate(generated_sentences):\n",
    "        output = generated_sentences[index]\n",
    "        offset = len(prompt)\n",
    "        start = offset + 1\n",
    "        end = min(output.find('\\n', start), output.find('Q:', start))\n",
    "        sentences.append(item[start: end])\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(0))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 5.97 ms, total: 1.57 s\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_answer_prompt(story, question):\n",
    "    return f\"\"\"\n",
    "In the text below two people are discussing a story.\n",
    "\n",
    "Story:\n",
    "{story}\n",
    "\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: \n",
    "\"\"\".strip()\n",
    "\n",
    "prompt = get_answer_prompt(story, question)\n",
    "#answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)\n",
    "answers = generate_multiple_answers(model, prompt, num_replicas=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Dict, List\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Entailer:\n",
    "    def __init__(self):\n",
    "        model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\n",
    "            _device\n",
    "        )\n",
    "\n",
    "    def get_relation(self, premise: str, hypothesis: str) -> Dict[str, float]:\n",
    "        with torch.no_grad():\n",
    "            encodings = self._tokenizer(\n",
    "                premise, hypothesis, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            output = self._model(encodings[\"input_ids\"].to(_device))\n",
    "            prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "            label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            prediction = {name: float(pred) for pred, name in zip(prediction, label_names)}\n",
    "            return prediction\n",
    "\n",
    "    def entails(self, premise: str, hypothesis: str, threshold=0.5) -> bool:\n",
    "        prediction = self.get_relation(premise, hypothesis)\n",
    "        if prediction[\"entailment\"] > threshold:\n",
    "            return True\n",
    "        \n",
    "        return prediction[\"entailment\"] > threshold\n",
    "\n",
    "    def batch_entails(self, premise: List[str], hypothesis: List[str], threshold=0.5) -> bool:\n",
    "        with torch.no_grad():\n",
    "            encodings = self._tokenizer.batch_encode_plus([lhs + \" [SEP] \" + rhs \n",
    "                                                           for lhs, rhs in zip(premise, hypothesis)], \n",
    "                                                          return_tensors=\"pt\", \n",
    "                                                          padding=True\n",
    "            )\n",
    "            output = self._model(**encodings.to(_device))\n",
    "            prediction = torch.softmax(output[\"logits\"], dim=-1)\n",
    "            return (prediction[:, 0] > threshold).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alce/src/the_chatbot_experiment/.env/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entailer = Entailer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entailer.batch_entails([\"my name is alberto\", \"my name is John\"], [\"Alberto is the name\", \"Alberto is speaking\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discussion_from_text(text, last_n=None):\n",
    "    trigger = \"Discussion:\\n\"\n",
    "    start = text.find(trigger) + len(trigger)\n",
    "    end = text.rfind(\"\\n\")\n",
    "    text = text[start:end]\n",
    "    if last_n:        \n",
    "        chunks = text.split(\"\\nQ:\")\n",
    "        text = \"Q:\" + \"\\nQ:\".join(chunks[-last_n:])\n",
    "        return text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = get_discussion_from_text(dev_list[0], last_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt_from_dialogue_and_answer(dialogue, answer):\n",
    "        return f\"\"\"\n",
    "Discussion:\n",
    "{dialogue.strip()}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: Where did Cotton's mother put her to clean the paint off?\n",
      "A: a bucket of water\n",
      "Q: What did the other cats do when Cotton emerged from the bucket of water?\n",
      "A: licked her face\n",
      "Q: Did they want Cotton to change the color of her fur?\n",
      "A: No\n",
      "\n",
      "Statement:\n"
     ]
    }
   ],
   "source": [
    "print(get_statement_prompt_from_dialogue_and_answer(dialogue, answer=\"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt_from_question_and_answer(question, answer):\n",
    "    return f\"\"\"\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers_from_multiple_prompts(model, prompts, length=5):\n",
    "    model.eval()\n",
    "    starts = [len(prompt) for prompt in prompts]\n",
    "    num_replicas = len(prompts)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        tokens = tokenizer.batch_encode_plus(prompts, padding=True, return_tensors='pt').input_ids\n",
    "        texts = prompts\n",
    "        while any(\"\\n\" not in text[start + 1:] for text, start in zip(texts, starts)):\n",
    "            if tokens.shape[1] + length > 1024:\n",
    "                break\n",
    "            \n",
    "            output = model.generate(\n",
    "                tokens.cuda(),\n",
    "                max_length=tokens.shape[1] + length,\n",
    "                pad_token_id=50256\n",
    "            )\n",
    "            texts = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "            tokens = output\n",
    "\n",
    "    for index in range(num_replicas):\n",
    "        end = texts[index].find('\\n', starts[index] + 1)\n",
    "        texts[index] = texts[index][starts[index]:end].split(':')[-1].strip()\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_from_question_and_answer(question, answer):\n",
    "    text = f\"\"\"\n",
    "Discussion:\n",
    "Q: {question}\n",
    "A: {answer}\n",
    "\n",
    "Statement:\n",
    "    \"\"\".strip()\n",
    "    return generate_answer(statement_model, text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 Âµs, sys: 2 Âµs, total: 15 Âµs\n",
      "Wall time: 17.2 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_dict = {}\n",
    "for answer in answers:\n",
    "    count_dict.setdefault(answer, 0)\n",
    "    count_dict[answer] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Score on Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(0))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = \"Albert says: 'My father's ship is called st. George, mine is Sir George'.\"\n",
    "question = \"How is albert's ship called?\"\n",
    "prompt = get_answer_prompt(story, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_answer_prompt = '\\nA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_answer_prompt)\n",
    "\n",
    "def get_answer_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_answer_prompt))\n",
    "    return text[pos + len(_answer_prompt):end]\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_with_entailment(model, statement_model, prompt, story, dialogue):\n",
    "    #answers = generate_multiple_answers_with_dropout(model, prompt, num_replicas=15)\n",
    "    answers, _ = generate_all_answers(model, prompt, num_replicas=10)\n",
    "    statements = []\n",
    "    \n",
    "    valid_answers = set()\n",
    "    repetitions = 0\n",
    "    while not valid_answers and repetitions < 1:\n",
    "        count_dict = {}\n",
    "        for answer in answers:\n",
    "            count_dict.setdefault(answer, 0)\n",
    "            count_dict[answer] += 1\n",
    "        \n",
    "        answers = count_dict.keys()\n",
    "        \n",
    "        statement_prompts = []\n",
    "        for answer in answers:\n",
    "            statement_prompts.append(get_statement_prompt_from_dialogue_and_answer(dialogue, answer))\n",
    "        \n",
    "        statements = generate_answers_from_multiple_prompts(statement_model, statement_prompts)\n",
    "        entailment_predictions = entailer.batch_entails([story] * len(statements), statements)\n",
    "        for answer, answer_is_true in zip(answers, entailment_predictions):\n",
    "            if answer_is_true:\n",
    "                valid_answers.add(answer)\n",
    "                \n",
    "        repetitions += 1\n",
    "            \n",
    "    if not valid_answers:\n",
    "        return \"unknown\"\n",
    "        \n",
    "    ranked_answers = [(answer, count_dict[answer]) for answer in valid_answers]\n",
    "    return sorted(ranked_answers, key=lambda x: -x[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "number = 1\n",
    "\n",
    "small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                     max_num_questions=3,\n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "prediction = get_answer_with_entailment(model, \n",
    "               statement_model,\n",
    "               small_text,\n",
    "               dev_dict['data'][index][\"story\"],\n",
    "               get_discussion_from_text(small_text, last_n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I know what is inside the bag--a thermos with hot soup and a stainless-steel container'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Discussion:\\nQ:Q: Who is at the door?\\nA: An elderly Chinese lady and a little boy\\nQ: Is she carrying something?\\nA: Her mommy's\\n\\nStatement:\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_statement_prompt_from_dialogue_and_answer(get_discussion_from_text(small_text, last_n=3), \"Her mommy's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the text below two people are discussing a story.\n",
      "\n",
      "Story:\n",
      "My doorbell rings. On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
      "\n",
      "I know this lady. It is not her first visit. She is the boy's grandmother, and her daughter bought the house next door last October. \n",
      "\n",
      "Her daughter, Nicole, speaks fluent English. But she is now in Shanghai, and her parents are here with the little boy. Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients. \n",
      "\n",
      "I know what is inside the bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake. This has become an almost-daily practice. \n",
      "\n",
      "Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello. Once, she brought an iPad as well as the food. She pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go with her to her house. Then, she handed the iPad to her husband and almost immediately I found myself looking at Nicole in Shanghai and discussing her mother's cooking and salt intake. Instantly, tears welled in my eyes. \n",
      "\n",
      "\"Your mother just can't be bringing me meals like this all the time,\" I insisted. \"I can hardly do dishes in return.\" \n",
      "\n",
      "\"Oh, no, Lucy.\" Nicole said. \"Mum doesn't like western food. Don't worry about it; she has to cook for the three of them anyway, and she wants to do it.\" \n",
      "\n",
      "The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
      "\n",
      "I am now working on some more Chinese words--it's the least I can do after such display of kindness. \n",
      "\n",
      "\"Thank you\" is, of course, the first one. Somehow, it seems inadequate.\n",
      "\n",
      "Discussion:\n",
      "Q: Who is at the door?\n",
      "A: An elderly Chinese lady and a little boy\n",
      "Q: Is she carrying something?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=3,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            try:\n",
    "                prediction = get_answer_with_entailment(model, \n",
    "                               statement_model,\n",
    "                               small_text,\n",
    "                               dev_dict['data'][index][\"story\"],\n",
    "                               get_discussion_from_text(small_text, last_n=2))\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            if not prediction or prediction == \"unknown\":\n",
    "                continue\n",
    "        \n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                    false_positives.append(prediction)\n",
    "                \n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif entailer.entails(label, prediction):\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                elif entailer.entails(label, prediction):\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    wrong_predictions.append({\n",
    "                        'index': index,\n",
    "                        'number' : number,\n",
    "                        'label': label, \n",
    "                        'prediction': prediction})\n",
    "                    \n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââ| 10/10 [08:35<00:00, 51.56s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6837606837606838,\n",
       " [{'index': 0,\n",
       "   'number': 6,\n",
       "   'label': 'paint herself like them',\n",
       "   'prediction': '()'},\n",
       "  {'index': 0,\n",
       "   'number': 6,\n",
       "   'label': 'she painted herself',\n",
       "   'prediction': '()'},\n",
       "  {'index': 1,\n",
       "   'number': 1,\n",
       "   'label': 'the bottle',\n",
       "   'prediction': 'It was hard and clear'},\n",
       "  {'index': 1,\n",
       "   'number': 1,\n",
       "   'label': 'a bottle',\n",
       "   'prediction': 'It was hard and clear'},\n",
       "  {'index': 1, 'number': 2, 'label': 'Asta', 'prediction': 'a friend'},\n",
       "  {'index': 1, 'number': 2, 'label': 'Sharkie', 'prediction': 'a friend'},\n",
       "  {'index': 1, 'number': 2, 'label': 'Asta', 'prediction': 'a friend'},\n",
       "  {'index': 1,\n",
       "   'number': 10,\n",
       "   'label': 'unknown',\n",
       "   'prediction': 'So did they open the note'},\n",
       "  {'index': 2,\n",
       "   'number': 1,\n",
       "   'label': 'Yes',\n",
       "   'prediction': 'I know what is inside the bag--a thermos with hot soup and a stainless-steel container'},\n",
       "  {'index': 2,\n",
       "   'number': 1,\n",
       "   'label': 'a little boy',\n",
       "   'prediction': 'I know what is inside the bag--a thermos with hot soup and a stainless-steel container'},\n",
       "  {'index': 2,\n",
       "   'number': 1,\n",
       "   'label': 'yes',\n",
       "   'prediction': 'I know what is inside the bag--a thermos with hot soup and a stainless-steel container'},\n",
       "  {'index': 2,\n",
       "   'number': 4,\n",
       "   'label': 'her daughter bought the house next door',\n",
       "   'prediction': 'Nicole'},\n",
       "  {'index': 2,\n",
       "   'number': 7,\n",
       "   'label': 'more nutrients',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake'},\n",
       "  {'index': 2,\n",
       "   'number': 9,\n",
       "   'label': 'needs more nutrients before heart surger',\n",
       "   'prediction': 'I know what is inside'},\n",
       "  {'index': 2,\n",
       "   'number': 9,\n",
       "   'label': 'I am having heart surgery soon, so her mother has decided I need more nutrients',\n",
       "   'prediction': 'I know what is inside'},\n",
       "  {'index': 2,\n",
       "   'number': 9,\n",
       "   'label': 'she has decided the narrator needs more nutrients',\n",
       "   'prediction': 'I know what is inside'},\n",
       "  {'index': 2,\n",
       "   'number': 9,\n",
       "   'label': 'has become an almost-daily practice',\n",
       "   'prediction': 'I know what is inside'},\n",
       "  {'index': 2, 'number': 10, 'label': 'an iPad', 'prediction': 'a thermos'},\n",
       "  {'index': 2,\n",
       "   'number': 11,\n",
       "   'label': 'hot soup, rice, vegetables, chicken, meat, or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice'},\n",
       "  {'index': 2,\n",
       "   'number': 11,\n",
       "   'label': 'soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice'},\n",
       "  {'index': 2,\n",
       "   'number': 11,\n",
       "   'label': 'rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice'},\n",
       "  {'index': 2,\n",
       "   'number': 11,\n",
       "   'label': 'hot soup and a container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake',\n",
       "   'prediction': 'a thermos with hot soup and a stainless-steel container with rice'},\n",
       "  {'index': 3, 'number': 1, 'label': 'Dennis Farina', 'prediction': 'First'},\n",
       "  {'index': 3,\n",
       "   'number': 2,\n",
       "   'label': 'cop-turned-actor',\n",
       "   'prediction': 'dapper'},\n",
       "  {'index': 3,\n",
       "   'number': 2,\n",
       "   'label': 'he was an actor',\n",
       "   'prediction': 'dapper'},\n",
       "  {'index': 3, 'number': 2, 'label': 'Actor', 'prediction': 'dapper'},\n",
       "  {'index': 3,\n",
       "   'number': 10,\n",
       "   'label': 'joined the cast of Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'index': 3,\n",
       "   'number': 10,\n",
       "   'label': 'he joined the cast of  Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'index': 3,\n",
       "   'number': 10,\n",
       "   'label': 'He joined a TV show cast',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'index': 3,\n",
       "   'number': 10,\n",
       "   'label': ', he joined the cast of Law & Order',\n",
       "   'prediction': 'he joined the cast of the long-running Law & Order'},\n",
       "  {'index': 3, 'number': 11, 'label': 'Law & Order', 'prediction': 'A movie'},\n",
       "  {'index': 3, 'number': 11, 'label': 'Law & Order', 'prediction': 'A movie'},\n",
       "  {'index': 3,\n",
       "   'number': 12,\n",
       "   'label': 'Detective Joe Fontana',\n",
       "   'prediction': 'A Dennis Farina'},\n",
       "  {'index': 3,\n",
       "   'number': 14,\n",
       "   'label': 'flashy clothes and an expensive car',\n",
       "   'prediction': 'One?'},\n",
       "  {'index': 3,\n",
       "   'number': 14,\n",
       "   'label': 'An expensive car',\n",
       "   'prediction': 'One?'},\n",
       "  {'index': 3,\n",
       "   'number': 14,\n",
       "   'label': 'an expensive car',\n",
       "   'prediction': 'One?'},\n",
       "  {'index': 4, 'number': 1, 'label': 'no', 'prediction': 'yes'},\n",
       "  {'index': 4, 'number': 1, 'label': 'No', 'prediction': 'yes'},\n",
       "  {'index': 4,\n",
       "   'number': 6,\n",
       "   'label': 'before bedtime',\n",
       "   'prediction': 'An afternoon'},\n",
       "  {'index': 4,\n",
       "   'number': 6,\n",
       "   'label': 'right before bedtime',\n",
       "   'prediction': 'An afternoon'},\n",
       "  {'index': 4,\n",
       "   'number': 6,\n",
       "   'label': 'right before bedtime',\n",
       "   'prediction': 'An afternoon'},\n",
       "  {'index': 4,\n",
       "   'number': 7,\n",
       "   'label': 'no answer',\n",
       "   'prediction': 'she thought something might be wrong'},\n",
       "  {'index': 4,\n",
       "   'number': 7,\n",
       "   'label': 'no one answered',\n",
       "   'prediction': 'she thought something might be wrong'},\n",
       "  {'index': 4, 'number': 11, 'label': 'yes', 'prediction': 'No'},\n",
       "  {'index': 4,\n",
       "   'number': 13,\n",
       "   'label': 'her teacher',\n",
       "   'prediction': 'She told him what happened that day'},\n",
       "  {'index': 4, 'number': 16, 'label': 'dentist', 'prediction': 'A bus stop'},\n",
       "  {'index': 4,\n",
       "   'number': 16,\n",
       "   'label': 'to the dentist',\n",
       "   'prediction': 'A bus stop'},\n",
       "  {'index': 4,\n",
       "   'number': 16,\n",
       "   'label': 'the dentist',\n",
       "   'prediction': 'A bus stop'},\n",
       "  {'index': 5,\n",
       "   'number': 5,\n",
       "   'label': 'Arthur Kill and the Kill Van Kull',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'index': 5,\n",
       "   'number': 5,\n",
       "   'label': 'the Arthur Kill and the Kill Van Kull,',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'index': 5,\n",
       "   'number': 5,\n",
       "   'label': 'the Arthur Kill and the Kill Van Kull',\n",
       "   'prediction': 'Conference House Park'},\n",
       "  {'index': 6,\n",
       "   'number': 8,\n",
       "   'label': 'A violent storm',\n",
       "   'prediction': 'heavy-maybe the washing machine-knocked'},\n",
       "  {'index': 6,\n",
       "   'number': 11,\n",
       "   'label': 'The flashlight',\n",
       "   'prediction': \"the light of his father's flashlight\"},\n",
       "  {'index': 7,\n",
       "   'number': 0,\n",
       "   'label': 'Gary Giordano',\n",
       "   'prediction': 'Answered by FBI agents'},\n",
       "  {'index': 7,\n",
       "   'number': 0,\n",
       "   'label': 'the suspect',\n",
       "   'prediction': 'Answered by FBI agents'},\n",
       "  {'index': 7, 'number': 1, 'label': 'Maryland', 'prediction': 'Gaithersburg'},\n",
       "  {'index': 7, 'number': 2, 'label': 'Montgomery County', 'prediction': 'No'},\n",
       "  {'index': 7, 'number': 2, 'label': 'Montgomery', 'prediction': 'No'},\n",
       "  {'index': 7, 'number': 2, 'label': 'Gaithersburg', 'prediction': 'No'},\n",
       "  {'index': 7,\n",
       "   'number': 5,\n",
       "   'label': 'suspect in the recent disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'index': 7,\n",
       "   'number': 5,\n",
       "   'label': 'he is a suspect in the recent disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'index': 7,\n",
       "   'number': 5,\n",
       "   'label': 'he is the suspect in the disappearance of an American woman',\n",
       "   'prediction': 'he is being held in an Aruban jail'},\n",
       "  {'index': 7,\n",
       "   'number': 8,\n",
       "   'label': 'Solicitor General',\n",
       "   'prediction': 'Taco Stein'},\n",
       "  {'index': 7,\n",
       "   'number': 12,\n",
       "   'label': 'near Baby Beach',\n",
       "   'prediction': 'An unknown'},\n",
       "  {'index': 7,\n",
       "   'number': 12,\n",
       "   'label': 'near baby beach',\n",
       "   'prediction': 'An unknown'},\n",
       "  {'index': 7,\n",
       "   'number': 12,\n",
       "   'label': 'ast seen near Baby Beach',\n",
       "   'prediction': 'An unknown'},\n",
       "  {'index': 7, 'number': 14, 'label': 'Giordano', 'prediction': 'Gardner'},\n",
       "  {'index': 7, 'number': 18, 'label': 'on August 5', 'prediction': 'By whom?'},\n",
       "  {'index': 7, 'number': 18, 'label': 'August 5', 'prediction': 'By whom?'},\n",
       "  {'index': 7,\n",
       "   'number': 19,\n",
       "   'label': '2, Giordano told authorities that he had been snorkeling with Gardner',\n",
       "   'prediction': 'three days after Robyn Gardner was last seen near Baby Beach'},\n",
       "  {'index': 7,\n",
       "   'number': 19,\n",
       "   'label': 'Two',\n",
       "   'prediction': 'three days after Robyn Gardner was last seen near Baby Beach'},\n",
       "  {'index': 8, 'number': 0, 'label': 'Great Britain', 'prediction': 'India'},\n",
       "  {'index': 8,\n",
       "   'number': 2,\n",
       "   'label': '30 feet tall',\n",
       "   'prediction': 'An easy height for tea picking'},\n",
       "  {'index': 8,\n",
       "   'number': 2,\n",
       "   'label': '30 feet tall',\n",
       "   'prediction': 'An easy height for tea picking'},\n",
       "  {'index': 8,\n",
       "   'number': 2,\n",
       "   'label': 'may be 30 feet tall',\n",
       "   'prediction': 'An easy height for tea picking'},\n",
       "  {'index': 8,\n",
       "   'number': 5,\n",
       "   'label': 'Leaves fell into the hot water',\n",
       "   'prediction': 'A tea plant is pruned'},\n",
       "  {'index': 8,\n",
       "   'number': 5,\n",
       "   'label': 'Leaves from a wild tea tree fell into a hot water pot',\n",
       "   'prediction': 'A tea plant is pruned'},\n",
       "  {'index': 8,\n",
       "   'number': 5,\n",
       "   'label': 'by accident',\n",
       "   'prediction': 'A tea plant is pruned'},\n",
       "  {'index': 8,\n",
       "   'number': 5,\n",
       "   'label': 'By accident',\n",
       "   'prediction': 'A tea plant is pruned'},\n",
       "  {'index': 8, 'number': 9, 'label': 'unknown', 'prediction': 'bad digestion'},\n",
       "  {'index': 9, 'number': 1, 'label': 'Germany', 'prediction': 'No'},\n",
       "  {'index': 9,\n",
       "   'number': 3,\n",
       "   'label': 'bloody',\n",
       "   'prediction': 'a patch of sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 4,\n",
       "   'label': 'two bodies propped up',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 4,\n",
       "   'label': 'propped up, back to back',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 4,\n",
       "   'label': 'what appears to be two bodies propped up,',\n",
       "   'prediction': 'soldiers kneeling by a bloody body sprawled over a patch of sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 5,\n",
       "   'label': 'a military vehicle',\n",
       "   'prediction': 'sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 5,\n",
       "   'label': 'in front of a military vehicle',\n",
       "   'prediction': 'sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 5,\n",
       "   'label': 'military vehicle',\n",
       "   'prediction': 'sand and grass'},\n",
       "  {'index': 9,\n",
       "   'number': 6,\n",
       "   'label': 'taking or retaining individual souvenirs or trophies,',\n",
       "   'prediction': '(a) smoking hashish'},\n",
       "  {'index': 9,\n",
       "   'number': 6,\n",
       "   'label': 'taking or retaining individual souvenirs or trophies',\n",
       "   'prediction': '(a) smoking hashish'},\n",
       "  {'index': 9,\n",
       "   'number': 6,\n",
       "   'label': 'souvenirs or trophies',\n",
       "   'prediction': '(a) smoking hashish'},\n",
       "  {'index': 9,\n",
       "   'number': 9,\n",
       "   'label': 'Holmes is charged with the premeditated deaths of three civilians',\n",
       "   'prediction': 'wrongful deaths'},\n",
       "  {'index': 9,\n",
       "   'number': 9,\n",
       "   'label': 'the premeditated deaths of three civilians',\n",
       "   'prediction': 'wrongful deaths'},\n",
       "  {'index': 9,\n",
       "   'number': 9,\n",
       "   'label': 'The premeditated deaths of three civilians, possessing a dismembered human finger, wrongfully possessing photographs of human casualties, and smoking hashish',\n",
       "   'prediction': 'wrongful deaths'}],\n",
       " ['So did they open the note', 'bad digestion'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first 10, last 3 questions:\n",
    "\n",
    "    without replicas: 0.770\n",
    "\n",
    "    15 replicas, 5 repetitions, single question: 0.787\n",
    "\n",
    "    15 replicas, 5 repetitions, using whole dialogue: 0.777\n",
    "    \n",
    "    Using unique beam search (deterministic, only one repetition):\n",
    "    - 10 replicas, 1 repetition, last 3 question for statements: 0.628099173553719\n",
    "    - 10 replicas, 1 repetition, last 2 question for statements: 0.6837606837606838"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot3",
   "language": "python",
   "name": "chatbot3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
