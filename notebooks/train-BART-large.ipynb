{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = json.load(open('../data/qa_train_BART.json', encoding='utf8'))\n",
    "random.shuffle(train_dict)\n",
    "train_dict = train_dict[:10000]\n",
    "dev_dict = json.load(open('../data/qa_dev_BART.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        in_length = item[0].shape[1]\n",
    "        out_length = item[1].shape[1]\n",
    "        length = (in_length, out_length)\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0][0] for item in chunk])\n",
    "        labels = torch.stack([item[1][0] for item in chunk])\n",
    "        batches.append((inputs, labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 16 out of 10000\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_dict:\n",
    "    input_tokens = tokenizer.encode(item['inputs'], return_tensors='pt')\n",
    "    output_tokens = tokenizer.encode(item['labels'], return_tensors='pt')\n",
    "    if input_tokens.shape[1] > _limit:\n",
    "        input_tokens = input_tokens[:, :_limit]\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    if output_tokens.shape[1] > _limit:\n",
    "        output_tokens = output_tokens[:, :_limit]\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append((input_tokens, output_tokens))\n",
    "print(f'Skipped {total_skipped} out of {len(train_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch[0]\n",
    "        labels = batch[1]\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=labels.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)\n",
    "\n",
    "def test(test_model, batches):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        test_model.eval()\n",
    "        inputs = batch\n",
    "        loss = test_model(inputs, labels=inputs)[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 237/9984 [00:40<27:57,  5.81it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.76 GiB total capacity; 8.91 GiB already allocated; 196.00 MiB free; 9.36 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f8d2a19d1e2 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7f8d2a3f364b in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7f8d2a3f4464 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7f8d2a3f4aa1 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f8cdbd3890e in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7f8cda172949 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7f8cda18c777 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7f8d14f28c7d in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7f8d14f28f97 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f8d15033a1a in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::mm_cuda(at::Tensor const&, at::Tensor const&) + 0x6c (0x7f8cdb227ffc in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf22a20 (0x7f8cda161a20 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xa56530 (0x7f8d14895530 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f8d1507d81c in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::mm(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f8d14fce6ab in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ed0a2f (0x7f8d16d0fa2f in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0xa56530 (0x7f8d14895530 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f8d1507d81c in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::Tensor::mm(at::Tensor const&) const + 0x4b (0x7f8d15163cab in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2d11fbb (0x7f8d16b50fbb in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x25f (0x7f8d16b6c7df in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x3375bb7 (0x7f8d171b4bb7 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f8d171b0400 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f8d171b0fa1 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f8d171a9119 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f8d2af4f4ba in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #26: <unknown function> + 0xbd6df (0x7f8d46d746df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #27: <unknown function> + 0x76db (0x7f8d4c9216db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #28: clone + 0x3f (0x7f8d4cc5aa3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4b5c7b343dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     torch.save({'epoch': epoch,\n",
      "\u001b[0;32m<ipython-input-10-194d9b30168e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_model, batches, optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/tacred_rel/.env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 10.76 GiB total capacity; 8.91 GiB already allocated; 196.00 MiB free; 9.36 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f8d2a19d1e2 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7f8d2a3f364b in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7f8d2a3f4464 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7f8d2a3f4aa1 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f8cdbd3890e in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7f8cda172949 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7f8cda18c777 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7f8d14f28c7d in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7f8d14f28f97 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f8d15033a1a in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::mm_cuda(at::Tensor const&, at::Tensor const&) + 0x6c (0x7f8cdb227ffc in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf22a20 (0x7f8cda161a20 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xa56530 (0x7f8d14895530 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f8d1507d81c in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::mm(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f8d14fce6ab in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ed0a2f (0x7f8d16d0fa2f in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0xa56530 (0x7f8d14895530 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f8d1507d81c in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::Tensor::mm(at::Tensor const&) const + 0x4b (0x7f8d15163cab in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2d11fbb (0x7f8d16b50fbb in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x25f (0x7f8d16b6c7df in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x3375bb7 (0x7f8d171b4bb7 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f8d171b0400 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f8d171b0fa1 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f8d171a9119 in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f8d2af4f4ba in /home/alce/src/tacred_rel/.env/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #26: <unknown function> + 0xbd6df (0x7f8d46d746df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #27: <unknown function> + 0x76db (0x7f8d4c9216db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #28: clone + 0x3f (0x7f8d4cc5aa3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_small' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_answer_prompt = '\\nA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_answer_prompt)\n",
    "\n",
    "def get_answer_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_answer_prompt))\n",
    "    return text[pos + len(_answer_prompt):end]\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_number(model, text, number):\n",
    "    prompt = get_text_up_to_question_number(text, number)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 20\n",
    "    tokens_length = tokens.shape[1]\n",
    "    output = model.generate(\n",
    "             tokens,\n",
    "             max_length=tokens_length + _length,\n",
    "             temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_number(output, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model_with_all_answers(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "        total_questions = get_answers_number(text)\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        for number in range(total_questions):\n",
    "            prediction = generate_answer_number(model, text, number)\n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                else:\n",
    "                    wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "            if not it_was_answered:\n",
    "                print('No Answer for number', number)\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, question_number=-1, last_question=True):\n",
    "    text = ''\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][:question_number+1], \n",
    "                                       item['answers'][:question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             #temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers(model, prompt, num_replicas=6):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    \n",
    "    outputs = []\n",
    "    for _ in range(num_replicas):\n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        offset = len(prompt)\n",
    "        start = offset + 1\n",
    "        end = output.find('\\n', start)\n",
    "        outputs.append(output[start:end].split(':')[-1].strip())\n",
    "        \n",
    "    return list(set(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('save_small' + str(1))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=0\n",
    "number = 6\n",
    "generate_answer(model, dev_dict[doc + number]['inputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_dict[:1000]\n",
    "    for item in tqdm(dlist, total=len(dlist)):\n",
    "        small_text = item['inputs']\n",
    "        prediction = generate_answer(model, small_text)\n",
    "        if not prediction:\n",
    "            print('NO PREDICTION!!')\n",
    "            continue\n",
    "        prediction = prediction.replace('.', '').replace('\"', '')\n",
    "        it_was_answered = False\n",
    "        for label in item['labels']:\n",
    "            label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "            if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                false_positives.append(prediction)\n",
    "\n",
    "            if prediction.lower() == label.lower():\n",
    "                correct_answers += 1\n",
    "                it_was_answered = True\n",
    "                break\n",
    "            elif prediction.lower() in label.lower():\n",
    "                correct_answers += 1\n",
    "                it_was_answered = True\n",
    "                break\n",
    "            elif label.lower() in prediction.lower():\n",
    "                correct_answers += 1\n",
    "                it_was_answered = True\n",
    "                break\n",
    "            else:\n",
    "                wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "        total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "false_positives = None\n",
    "wrong_answers = None\n",
    "\n",
    "for index in range(10):\n",
    "    checkpoint = torch.load('save_small' + str(index))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    accuracy, wrong_answers, false_positives = compute_accuracy_of_model(model.cuda())\n",
    "    print('Epoch:', index, ' with accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(answer):\n",
    "    answer = answer.lower()\n",
    "    answer = answer.strip()\n",
    "    answer = answer.replace('.', '')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model_only_y_or_n(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "        total_questions = get_answers_number(text)\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=5,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            prediction = generate_answer(model, small_text)\n",
    "            if clean_answer(prediction) not in ['yes', 'no']:\n",
    "                continue\n",
    "            prediction = prediction.replace('.', '').replace('\"', '')\n",
    "            it_was_answered = False\n",
    "            for label in all_answers[number]:\n",
    "                label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                    false_positives.append(prediction)\n",
    "                \n",
    "                if prediction.lower() == label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif prediction.lower() in label.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                elif label.lower() in prediction.lower():\n",
    "                    correct_answers += 1\n",
    "                    it_was_answered = True\n",
    "                    break\n",
    "                else:\n",
    "                    wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = None\n",
    "wrong_answers = None\n",
    "\n",
    "for index in range(10):\n",
    "    checkpoint = torch.load('save_small' + str(index))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    accuracy, wrong_answers, false_positives = compute_accuracy_of_model_only_y_or_n(model.cuda())\n",
    "    print('Epoch:', index, ' with accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "dispatcher",
   "language": "python",
   "name": "dispatcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
