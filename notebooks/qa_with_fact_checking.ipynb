{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d041322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8380e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcb6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "fact_checking_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "fact_checking_model.cuda()\n",
    "checkpoint = torch.load('save_fever3')\n",
    "fact_checking_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "_ = fact_checking_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346a2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    return text[:text.find(_claim_yn) + len(_claim_yn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34417527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d35a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(fact_checking_model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 1\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = fact_checking_model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    to_return = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    perplexity = float(model(output, labels=output)[0])\n",
    "    return get_answer_from_text(to_return), perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5334aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_answer(fact_checking_model, text):\n",
    "    prompt_y = get_text_up_to_question(text) + 'Y'\n",
    "    prompt_n = get_text_up_to_question(text) + 'N'\n",
    "    tokens_y = tokenizer.encode(prompt_y, return_tensors='pt').cuda()\n",
    "    tokens_n = tokenizer.encode(prompt_n, return_tensors='pt').cuda()\n",
    "    perplexity_y = float(model(tokens_y, labels=tokens_y)[0])\n",
    "    perplexity_n = float(model(tokens_n, labels=tokens_n)[0])\n",
    "    if perplexity_y < perplexity_n:\n",
    "        return 'Y', perplexity_y\n",
    "    return 'N', perplexity_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10144a",
   "metadata": {},
   "source": [
    "# Question Answering part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f16096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_answer_prompt = '\\nA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_answer_prompt)\n",
    "\n",
    "def get_answer_number(text, number):\n",
    "    pos = text.find(_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_answer_prompt))\n",
    "    return text[pos + len(_answer_prompt):end]\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39651446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c0b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_answers(model, prompt, num_replicas=25):\n",
    "    model.train()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            offset = len(prompt)\n",
    "            start = offset + 1\n",
    "            end = text.find('\\n', start)\n",
    "            outputs.append(text[start:end].split(':')[-1].strip())\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963a8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e255816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_y_n_answers(model, prompt, num_replicas=25):\n",
    "    model.train()\n",
    "    outputs_count = {}\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            answer = get_answer_from_text(text)\n",
    "            outputs_count.setdefault(answer, 0)\n",
    "            outputs_count[answer] += 1\n",
    "\n",
    "    total = sum(v for v in outputs_count.values())\n",
    "    return [(k, v / total) for k, v in outputs_count.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32990b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_answer(model, prompt):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    \n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset + 1\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba651de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "checkpoint = torch.load('save_small' + str(6))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56a34f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))\n",
    "dev_list = json.load(open('../data/qa_dev_list.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0dbdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d06c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=0\n",
    "number = 1\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "first_answer = generate_first_answer(model, small_text)\n",
    "answers = generate_multiple_answers(model, small_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efbf978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_from_data_item(item):\n",
    "    return item['story']\n",
    "\n",
    "def get_dialogue_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = ''\n",
    "    text += ' '.join([q['input_text'] + ' ' + a['input_text'] + '.'\n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '?'.join(text.split('?')[:-1]) + '?'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6baad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claim_from_description_and_dialogue(description, dialogue):\n",
    "    if dialogue[-1] == '.':\n",
    "        dialogue = dialogue[:-1]    \n",
    "    text = 'Evidence:\\n'\n",
    "    text += description.replace('\\n\\n', '\\n') + '\\n\\n'\n",
    "    text += 'Claim:\\n'\n",
    "    text += dialogue + '\\n\\n'\n",
    "    text += 'The evidence supports the claim:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eca5cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70060e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "sentence_model = sentence_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db18bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_text(text):\n",
    "    outputs = sentence_model.encode(text)\n",
    "    return outputs\n",
    "\n",
    "def group_similar_answers_and_get_scores(answers):\n",
    "    answers_dict = {}\n",
    "    threshold = 0.7\n",
    "    embeddings = get_embeddings_from_text(answers)\n",
    "    embeddings = np.array([e/np.linalg.norm(e) for e in embeddings])\n",
    "    similarity_matrix = np.matmul(embeddings, embeddings.transpose())\n",
    "    superseded = set()\n",
    "    superseded_from = {}\n",
    "    for i in range(len(answers)):\n",
    "        for j in range(len(answers)):\n",
    "            if i > j:\n",
    "                continue\n",
    "            if i != j and answers[i] == answers[j]:\n",
    "                continue\n",
    "            if similarity_matrix[i][j] > threshold :\n",
    "                answers_dict.setdefault(i, 0)\n",
    "                answers_dict[i] += 1\n",
    "                if i != j:\n",
    "                    superseded.add(j)\n",
    "                    superseded_from.setdefault(i, [])\n",
    "                    superseded_from[i].append(j)\n",
    "\n",
    "    answers_and_scores = [(index, score/len(answers))\n",
    "                          for index, score in answers_dict.items() \n",
    "                          if index not in superseded]\n",
    "    \n",
    "    new_scores_dict = {}\n",
    "    total_score = sum(item[1] for item in answers_and_scores)\n",
    "    for answer_index, score in answers_and_scores:\n",
    "        answer_group = [answers[answer_index]]\n",
    "        if answer_index in superseded_from:\n",
    "            answer_group += [answers[i] for i in superseded_from[answer_index]]\n",
    "        answer_group = tuple(set(answer_group))\n",
    "        if answer_group in new_scores_dict:\n",
    "            new_scores_dict[answer_group] += score / total_score\n",
    "        else:\n",
    "            new_scores_dict[answer_group] = score / total_score\n",
    "    \n",
    "    \n",
    "    return sorted(list(new_scores_dict.items()), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "703be865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(9))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d65630aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statement_from_dialogue(model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             #temperature=0,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa7cc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "number = 6\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1dbef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt(item, max_num_questions=0, question_number=-1, use_answer=None):\n",
    "    text = 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if use_answer:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n' + 'A: ' + use_answer + '\\n'\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "055b0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_answer(description, answers, doc, number):\n",
    "    best_probability = 0\n",
    "    \n",
    "    best_answer = ''\n",
    "    for answer_tuple in answers:\n",
    "        answer_sample = answer_tuple[0][0]\n",
    "        answer_score = answer_tuple[1]\n",
    "        print(answer_score, answer_sample)\n",
    "        statement_prompt = get_statement_prompt(dev_dict['data'][doc], \n",
    "                                 max_num_questions=5,\n",
    "                                 question_number=number,\n",
    "                                 use_answer=answer_sample)\n",
    "        #print(statement_prompt)\n",
    "        statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "        #print(statement)\n",
    "        text = create_claim_from_description_and_dialogue(description, statement)\n",
    "        \n",
    "        y_n_tuples = generate_multiple_y_n_answers(fact_checking_model, text)\n",
    "        print(y_n_tuples)\n",
    "        for y_n, score in y_n_tuples:\n",
    "            if score * answer_score > best_probability and y_n == 'Y':\n",
    "                best_probability = score * answer_score\n",
    "                best_answer = answer_tuple[0]\n",
    "\n",
    "    return best_answer, best_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a797eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['white', 'White', 'White', 'White', 'white', 'white', 'White', 'white', 'orange', 'white', 'white', 'white', 'white', 'White', 'orange', 'white', 'white', 'White', 'white', 'white', 'white', 'orange', 'white', 'orange', 'White']\n",
      "0.6666666666666667 White\n",
      "[('Y', 1.0)]\n",
      "0.33333333333333337 orange\n",
      "[('Y', 1.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('White', 'white'), 0.6666666666666667)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = 0\n",
    "number = 0\n",
    "description = get_description_from_data_item(dev_dict['data'][doc])\n",
    "small_text = get_text_from_data_item(dev_dict['data'][doc], \n",
    "                                     max_num_questions=5, \n",
    "                                     question_number=number,\n",
    "                                     last_question=False)\n",
    "answers = generate_multiple_answers(model, small_text)\n",
    "\n",
    "print(answers)\n",
    "answers = group_similar_answers_and_get_scores(answers)\n",
    "select_answer(description, answers, doc, number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370e942",
   "metadata": {},
   "source": [
    "# Computing accuracy after fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82fe9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model):\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    dlist = dev_list[:10]\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=8,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=False)\n",
    "            predictions = generate_multiple_answers(model, small_text)\n",
    "            predictions = group_similar_answers_and_get_scores(predictions)\n",
    "            #predictions = predictions[0]\n",
    "            predictions = select_answer(description, predictions, index, number)\n",
    "            print(predictions)\n",
    "            for prediction in predictions[0]:\n",
    "                it_was_answered = False\n",
    "                if not prediction:\n",
    "                    prediction = 'unknown'\n",
    "                prediction = prediction.replace('.', '').replace('\"', '')\n",
    "                it_was_answered = False\n",
    "                for label in all_answers[number]:\n",
    "                    label = label.replace('.', '').replace('\"', '')\n",
    "\n",
    "                    if prediction.lower() != 'unknown' and label.lower() == 'unknown':\n",
    "                        false_positives.append(prediction)\n",
    "\n",
    "                    if prediction.lower() == label.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    elif prediction.lower() in label.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    elif label.lower() in prediction.lower():\n",
    "                        correct_answers += 1\n",
    "                        it_was_answered = True\n",
    "                        break\n",
    "                    else:\n",
    "                        wrong_predictions.append({'label': label, 'prediction': prediction})\n",
    "                        \n",
    "                if it_was_answered:\n",
    "                    break\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2676fc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777777 White\n",
      "[('Y', 1.0)]\n",
      "0.22222222222222213 orange\n",
      "[('Y', 1.0)]\n",
      "(('White', 'white'), 0.7777777777777777)\n",
      "0.47058823529411764 in a barn\n",
      "[('Y', 1.0)]\n",
      "0.29411764705882354 a farm house\n",
      "[('Y', 1.0)]\n",
      "0.1764705882352941 above the barn\n",
      "[('Y', 1.0)]\n",
      "0.058823529411764705 a farm\n",
      "[('Y', 1.0)]\n",
      "(('in a barn', 'a barn'), 0.47058823529411764)\n",
      "0.9200000000000002 no\n",
      "[('Y', 1.0)]\n",
      "0.07999999999999999 yes\n",
      "[('Y', 0.96), ('N', 0.04)]\n",
      "(('no',), 0.9200000000000002)\n",
      "1.0 her mom and 5 other sisters\n",
      "[('Y', 1.0)]\n",
      "(('her mom and 5 other sisters', 'her mommy and sisters', 'her mommy and 5 other sisters', 'her mommy and 5 sisters'), 1.0)\n",
      "1.0000000000000002 orange\n",
      "[('Y', 1.0)]\n",
      "(('orange',), 1.0000000000000002)\n",
      "1.0000000000000002 no\n",
      "[('Y', 1.0)]\n",
      "(('no',), 1.0000000000000002)\n",
      "0.75 she used her paint\n",
      "[('Y', 1.0)]\n",
      "0.16666666666666669 rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and droppe\n",
      "[('Y', 1.0)]\n",
      "0.08333333333333334 Rub her face on Cotton's and dropped her into a bucket of water\n",
      "[('Y', 1.0)]\n",
      "(('she used her paint', 'she used it to paint herself', 'she used her paint to paint herself', 'she painted herself', 'painted herself'), 0.75)\n",
      "0.368421052631579 her mommy's\n",
      "[('Y', 1.0)]\n",
      "0.3157894736842105 Cotton's mommy\n",
      "[('Y', 1.0)]\n",
      "0.21052631578947367 the old farmer's\n",
      "[('Y', 1.0)]\n",
      "0.10526315789473684 her mommy and sisters'\n",
      "[('Y', 1.0)]\n",
      "((\"her mommy's\", 'her mommy'), 0.368421052631579)\n",
      "0.5555555555555556 they laughed\n",
      "[('Y', 1.0)]\n",
      "0.22222222222222224 rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\".\n",
      "[('Y', 1.0)]\n",
      "0.11111111111111112 rubbed her face\n",
      "[('Y', 1.0)]\n",
      "0.11111111111111112 started laughing\n",
      "[('Y', 1.0)]\n",
      "(('they laughed', 'laughed'), 0.5555555555555556)\n",
      "1.0 a bucket of water\n",
      "[('Y', 0.96), ('N', 0.04)]\n",
      "(('a bucket of water', 'in a bucket', 'in a big bucket of water', 'in a bucket of water'), 0.96)\n",
      "0.7000000000000001 she licked her face\n",
      "[('Y', 1.0)]\n",
      "0.25 they licked Cotton's face\n",
      "[('Y', 1.0)]\n",
      "0.049999999999999996 she rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\"\n",
      "[('Y', 1.0)]\n",
      "(('she licked her face', 'they licked her face', 'licked her face'), 0.7000000000000001)\n",
      "0.8800000000000001 no\n",
      "[('Y', 1.0)]\n",
      "0.11999999999999998 yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:46<06:58, 46.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Y', 1.0)]\n",
      "(('no',), 0.8800000000000001)\n",
      "1.0000000000000002 Asta\n",
      "[('Y', 1.0)]\n",
      "(('Asta',), 1.0000000000000002)\n",
      "0.4761904761904762 A bottle\n",
      "[('Y', 1.0)]\n",
      "0.1904761904761905 It was hard and clear.\n",
      "[('Y', 1.0)]\n",
      "0.14285714285714285 It was not a bird's belly.\n",
      "[('N', 0.36), ('Y', 0.64)]\n",
      "0.09523809523809525 The bottle floated above them.\n",
      "[('Y', 1.0)]\n",
      "0.09523809523809525 what did they see\n",
      "[('Y', 1.0)]\n",
      "(('A bottle', 'It was a bottle.', 'a bottle', 'It was a bottle', 'bottle', 'A bottle.'), 0.4761904761904762)\n",
      "0.5217391304347825 Sharkie\n",
      "[('Y', 1.0)]\n",
      "0.26086956521739124 Asta's friend\n",
      "[('Y', 1.0)]\n",
      "0.13043478260869562 Asta.\n",
      "[('Y', 1.0)]\n",
      "0.08695652173913042 a friend\n",
      "[('Y', 1.0)]\n",
      "(('Sharkie', 'Sharkie.'), 0.5217391304347825)\n",
      "0.9523809523809523 yes\n",
      "[('Y', 1.0)]\n",
      "0.047619047619047616 no\n",
      "[('Y', 1.0)]\n",
      "(('yes', 'Yes', 'Yes.'), 0.9523809523809523)\n",
      "1.0000000000000002 Yes\n",
      "[('Y', 1.0)]\n",
      "(('Yes',), 1.0000000000000002)\n",
      "0.5652173913043476 a note\n",
      "[('Y', 1.0)]\n",
      "0.434782608695652 A bottle\n",
      "[('Y', 1.0)]\n",
      "(('a note',), 0.5652173913043476)\n",
      "1.0000000000000002 Yes\n",
      "[('Y', 1.0)]\n",
      "(('Yes',), 1.0000000000000002)\n",
      "1.0000000000000002 Asta\n",
      "[('Y', 1.0)]\n",
      "(('Asta',), 1.0000000000000002)\n",
      "0.5517241379310345 took it to the bottom of the ocean\n",
      "[('Y', 0.92), ('N', 0.08)]\n",
      "0.44827586206896547 they took it to Asta's papa\n",
      "[('Y', 1.0)]\n",
      "(('took it to the bottom of the ocean', 'They took it to the bottom of the ocean', 'they took it to the bottom of the ocean'), 0.5075862068965518)\n",
      "1.0000000000000002 Yes\n",
      "[('Y', 1.0)]\n",
      "(('Yes',), 1.0000000000000002)\n",
      "0.9600000000000002 Yes\n",
      "[('Y', 1.0)]\n",
      "0.039999999999999994 No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [01:23<05:26, 40.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Y', 1.0)]\n",
      "(('Yes',), 0.9600000000000002)\n",
      "0.7999999999999998 elderly Chinese lady\n",
      "[('Y', 1.0)]\n",
      "0.11999999999999998 Nicole\n",
      "[('Y', 1.0)]\n",
      "0.039999999999999994 a little boy\n",
      "[('Y', 1.0)]\n",
      "0.039999999999999994 My doorbell\n",
      "[('Y', 1.0)]\n",
      "(('elderly Chinese lady', 'The elderly Chinese lady', 'A Chinese lady', 'a lady', 'the elderly Chinese lady', 'a Chinese lady'), 0.7999999999999998)\n",
      "0.84 yes\n",
      "[('Y', 1.0)]\n",
      "0.16 a paper carrier bag\n",
      "[('Y', 1.0)]\n",
      "(('yes', 'Yes'), 0.84)\n",
      "1.0 a paper carrier bag\n",
      "[('Y', 1.0)]\n",
      "(('a paper carrier bag', 'A paper carrier bag', 'Paper carrier bag'), 1.0)\n",
      "0.6799999999999999 Yes\n",
      "[('Y', 1.0)]\n",
      "0.3199999999999999 No\n",
      "[('Y', 1.0)]\n",
      "(('Yes',), 0.6799999999999999)\n",
      "1.0000000000000002 Nicole\n",
      "[('Y', 1.0)]\n",
      "(('Nicole',), 1.0000000000000002)\n",
      "1.0000000000000002 Shanghai\n",
      "[('Y', 0.52), ('N', 0.48)]\n",
      "(('Shanghai',), 0.5200000000000001)\n",
      "0.8421052631578947 She is her grandmother\n",
      "[('Y', 1.0)]\n",
      "0.07894736842105263 Her mother bought the house next door last October\n",
      "[('Y', 1.0)]\n",
      "0.05263157894736842 She is her grandmother\n",
      "[('Y', 1.0)]\n",
      "0.02631578947368421 Her daughter\n",
      "[('Y', 1.0)]\n",
      "(('She is her grandmother', 'His grandmother', 'She is his grandmother'), 0.8421052631578947)\n",
      "0.411764705882353 a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp\n",
      "[('Y', 0.96), ('N', 0.04)]\n",
      "0.23529411764705882 rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
      "[('Y', 1.0)]\n",
      "0.1764705882352941 rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake.\n",
      "[('Y', 1.0)]\n",
      "0.11764705882352941 hot soup and rice\n",
      "[('Y', 1.0)]\n",
      "0.058823529411764705 a paper carrier bag\n",
      "[('Y', 1.0)]\n",
      "(('a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp', 'hot soup and a stainless-steel container', 'a thermos with hot soup and a rice', 'a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake', 'a thermos with soup and a rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake'), 0.39529411764705885)\n",
      "0.9600000000000002 Yes\n",
      "[('Y', 1.0)]\n",
      "0.039999999999999994 No\n",
      "[('Y', 1.0)]\n",
      "(('Yes',), 0.9600000000000002)\n",
      "0.36 she has heart surgery\n",
      "[('Y', 1.0)]\n",
      "0.24 she brought an iPad\n",
      "[('Y', 1.0)]\n",
      "0.16 to get nutrients\n",
      "[('Y', 1.0)]\n",
      "0.08 she has told her mother that she is having heart surgery soon\n",
      "[('Y', 1.0)]\n",
      "0.04 to bring food\n",
      "[('Y', 1.0)]\n",
      "0.04 she wanted to know if the food was all right\n",
      "[('Y', 1.0)]\n",
      "0.04 her grandmother bought the house\n",
      "[('Y', 1.0)]\n",
      "0.04 she didn't speak English\n",
      "[('Y', 0.96), ('N', 0.04)]\n",
      "(('she has heart surgery', 'heart surgery'), 0.36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:13<08:54, 66.78s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 10.76 GiB total capacity; 7.53 GiB already allocated; 24.69 MiB free; 7.96 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-eb55228213ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_accuracy_of_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-ce700e2ddd89>\u001b[0m in \u001b[0;36mcompute_accuracy_of_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                  \u001b[0mquestion_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                                  last_question=False)\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multiple_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_similar_answers_and_get_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#predictions = predictions[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-da50a9caee78>\u001b[0m in \u001b[0;36mgenerate_multiple_answers\u001b[0;34m(model, prompt, num_replicas)\u001b[0m\n\u001b[1;32m     14\u001b[0m              \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m              \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m              \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_replicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m             )\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m             )\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         )\n\u001b[1;32m    958\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 )\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mpast_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 10.76 GiB total capacity; 7.53 GiB already allocated; 24.69 MiB free; 7.96 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "compute_accuracy_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167c901",
   "metadata": {},
   "source": [
    "## Results with alternative answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b915617",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "#### Using dev_dict 10, 20 multiple answers\n",
    "0.7588652482269503\n",
    "(without fact checking and 20 multiple answers it is 0.7163120567375887)\n",
    "(without fact checking and 25 multiple answers it is 0.7659574468085106)\n",
    "\n",
    "#### Using dev_dict 10, 25 multiple answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05fd385",
   "metadata": {},
   "source": [
    "## Repeat the test with save number 7 <= there was an error with the evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e2865",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "* properly train FEVER. Why is it so good on fever dev data but louse on coqa? (leakage?)\n",
    "\n",
    "* Is the FEVER system skewed to say Y? Should you have a threshold for the \"correct\" answer?\n",
    "\n",
    "* maybe you can add some syntethic corereference to the training data for the statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8fd280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
