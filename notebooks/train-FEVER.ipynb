{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = json.load(open('../data/supporting.json'))\n",
    "refuting_texts = json.load(open('../data/refuting.json'))\n",
    "\n",
    "dev_supporting_texts = json.load(open('../data/dev_supporting.json'))\n",
    "dev_refuting_texts = json.load(open('../data/dev_refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "split = 0.8\n",
    "\n",
    "_prompt = '\\n\\n\\nThe evidence supports the claim:\\n'\n",
    "train_list = [item + _prompt + 'Yes.' for item in supporting_texts]\n",
    "train_list += [item + _prompt + 'Nope.' for item in refuting_texts]\n",
    "random.shuffle(train_list)\n",
    "\n",
    "dev_list = [item + _prompt + 'Yes.' for item in dev_supporting_texts]\n",
    "dev_list += [item + _prompt + 'Nope.' for item in dev_refuting_texts]\n",
    "random.shuffle(dev_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_list, open('../data/train_list.json', 'w'))\n",
    "json.dump(dev_list, open('../data/dev_list.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/train_list.json'))\n",
    "dev_list = json.load(open('../data/dev_list.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Uranium-235 (235U) is an isotope of uranium making up about 0.72 % of natural uranium. Unlike the predominant isotope uranium-238, it is fissile, i.e., it can sustain a fission chain reaction. It is the only fissile isotope that is a primordial nuclide or found in significant quantity in nature. Uranium-235 has a half-life of 703.8 million years. It was discovered in 1935 by Arthur Jeffrey Dempster. Its (fission) nuclear cross section for slow thermal neutrons is about 584.994 barns. For fast neutrons it is on the order of 1 barnArthur Jeffrey Dempster (August 14, 1886 -- March 11, 1950) was a Canadian-American physicist best known for his work in mass spectrometry and his discovery of the uranium isotope 235U.\n",
      "\n",
      "\n",
      "Claim:\n",
      "In 1935, Uranium-235 was discovered.\n",
      "\n",
      "\n",
      "The evidence supports the claim:\n",
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(dev_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cac601863384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_skipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_skipped\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m         )\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         )\n\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/transformers/models/gpt2/tokenization_gpt2.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;34m\"\"\"Tokenize a string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mbpe_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             token = \"\".join(\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_encoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/the_chatbot_experiment/.env/lib/python3.6/site-packages/regex/regex.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags, pos, endpos, overlapped, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     more than one group. Empty matches are included in the result.\"\"\"\n\u001b[1;32m    337\u001b[0m     \u001b[0mpat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_unused\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m def finditer(pattern, string, flags=0, pos=None, endpos=None, overlapped=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append(tokens)\n",
    "print(f'Skipped {total_skipped} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-69e8fa9af1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_batches = train_batches[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [4:00:53<00:00, 15.21it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.8349866022318717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [4:02:03<00:00, 15.14it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.27282461847403583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [3:59:59<00:00, 15.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 0.2118869653484744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [3:59:33<00:00, 15.29it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 0.1911627285202235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [4:04:37<00:00, 14.98it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 0.17449108368149846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [3:58:51<00:00, 15.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 0.1654880905636952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [4:08:54<00:00, 14.72it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 0.15662710911507668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219832/219832 [4:17:08<00:00, 14.25it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 0.15177448879509217\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    #test(model, dev_list[:2000])\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_fever' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    for item in tqdm(data):\n",
    "        expected = get_answer_from_text(item)\n",
    "        predicted = ''\n",
    "        try:\n",
    "            predicted = generate_answer(model, item)\n",
    "        except (IndexError, RuntimeError) as e:\n",
    "            #print(str(e))\n",
    "            #exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            #print(repr(traceback.extract_tb(exc_traceback)))\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if expected == predicted:\n",
    "            tp += 1\n",
    "        if expected == 'N' and predicted == 'Y':\n",
    "            fp += 1\n",
    "        if expected == 'Y' and predicted == 'N':\n",
    "            fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1:', f1)\n",
    "    print('Skipped:', skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    return text[:text.find(_claim_yn) + len(_claim_yn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 1\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_from_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_answer(model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 3\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    score = model(output, labels=output)[0]\n",
    "    out_text = tokenizer.decode(output[0][tokens_length:], skip_special_tokens=True)\n",
    "\n",
    "    return out_text, float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "checkpoint = torch.load(f'save_fever5')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 94/12317 [00:01<03:18, 61.52it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 12317/12317 [03:05<00:00, 66.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9332385175269807\n",
      "Recall: 0.9704219225750326\n",
      "F1: 0.9514670760832479\n",
      "Skipped: 24\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:07<00:00, 65.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.934847728407389\n",
      "Recall: 0.976107732406603\n",
      "F1: 0.9550323019381163\n",
      "Skipped: 24\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:08<00:00, 65.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9458805870580387\n",
      "Recall: 0.9741497767090347\n",
      "F1: 0.9598070739549839\n",
      "Skipped: 24\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:07<00:00, 65.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9344887635790696\n",
      "Recall: 0.9796574806572198\n",
      "F1: 0.9565401918343095\n",
      "Skipped: 24\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:09<00:00, 65.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9274099883855982\n",
      "Recall: 0.9790681380276756\n",
      "F1: 0.9525391956373551\n",
      "Skipped: 24\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:10<00:00, 64.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9371414317784984\n",
      "Recall: 0.976943746207853\n",
      "F1: 0.9566287557290782\n",
      "Skipped: 24\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:07<00:00, 65.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9339583506181034\n",
      "Recall: 0.9791249891275985\n",
      "F1: 0.9560084925690022\n",
      "Skipped: 24\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12317/12317 [03:08<00:00, 65.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9368394774070068\n",
      "Recall: 0.9760707473556441\n",
      "F1: 0.9560528215362405\n",
      "Skipped: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune on adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = json.load(open('../data/adv_supporting.json'))\n",
    "refuting_texts = json.load(open('../data/adv_refuting.json'))\n",
    "\n",
    "dev_supporting_texts = json.load(open('../data/adv_dev_supporting.json'))\n",
    "dev_refuting_texts = json.load(open('../data/adv_dev_refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_list = supporting_texts + refuting_texts\n",
    "random.shuffle(train_list)\n",
    "\n",
    "dev_list = dev_supporting_texts + dev_refuting_texts\n",
    "random.shuffle(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 out of 13724\n"
     ]
    }
   ],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append(tokens)\n",
    "print(f'Skipped {total_skipped} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13724"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch = 5\n",
    "#checkpoint = torch.load(f'save_fever{epoch}')\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:48<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 2.8265762857699737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 2.398946189952684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:41<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 2.0794248728950744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.848062120221666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:39<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 1.6539967937258713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 1.513019484298695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:46<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 1.3904902372768198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:39<00:00, 12.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 1.300064485630059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:34<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 1.2185287391775048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 1.1582863060133197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Loss: 1.101385164749951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 Loss: 1.056454688653185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:45<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Loss: 1.0138452767031563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Loss: 0.987048549780362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Loss: 0.9519627090963975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 Loss: 0.9310495258080663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:33<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Loss: 0.9055487251582932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:40<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Loss: 0.8882209499927384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Loss: 0.8701309680753325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Loss: 0.85805117948585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Loss: 0.8433181485769219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 Loss: 0.833092354444466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:38<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Loss: 0.8230131825909107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Loss: 0.8151353167549658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 Loss: 0.8054880833495393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Loss: 0.7983001110122787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:44<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Loss: 0.7931169445982843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:37<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Loss: 0.7878529269327509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:32<00:00, 12.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Loss: 0.7815568830894757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13724/13724 [18:21<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Loss: 0.7777050177764047\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)\n",
    "    #test(model, dev_list[:2000])\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_fever_adv' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5761947700631199\n",
      "Recall: 0.8565683646112601\n",
      "F1: 0.6889487870619946\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5304054054054054\n",
      "Recall: 0.9515151515151515\n",
      "F1: 0.6811279826464208\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8162031438935913\n",
      "Recall: 0.6343984962406015\n",
      "F1: 0.7139079851930196\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9070422535211268\n",
      "Recall: 0.56\n",
      "F1: 0.6924731182795699\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8591549295774648\n",
      "Recall: 0.6066907775768535\n",
      "F1: 0.7111817700052995\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5585970915312233\n",
      "Recall: 0.9328571428571428\n",
      "F1: 0.6987693953986089\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6425166825548141\n",
      "Recall: 0.8014268727705113\n",
      "F1: 0.7132275132275133\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7395048439181916\n",
      "Recall: 0.7053388090349076\n",
      "F1: 0.722017866526537\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8177514792899409\n",
      "Recall: 0.6506591337099812\n",
      "F1: 0.7246984792868381\n",
      "Skipped: 0\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.875\n",
      "Recall: 0.5932203389830508\n",
      "F1: 0.707070707070707\n",
      "Skipped: 0\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9634146341463414\n",
      "Recall: 0.5302013422818792\n",
      "F1: 0.683982683982684\n",
      "Skipped: 0\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7148803329864725\n",
      "Recall: 0.7292993630573248\n",
      "F1: 0.7220178665265371\n",
      "Skipped: 0\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6907114624505929\n",
      "Recall: 0.7740863787375415\n",
      "F1: 0.7300261096605745\n",
      "Skipped: 0\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7384780278670954\n",
      "Recall: 0.7088477366255144\n",
      "F1: 0.7233595800524933\n",
      "Skipped: 0\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7903780068728522\n",
      "Recall: 0.6679574056147144\n",
      "F1: 0.7240293809024133\n",
      "Skipped: 0\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7052313883299799\n",
      "Recall: 0.7594799566630552\n",
      "F1: 0.7313510693792383\n",
      "Skipped: 0\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6012323943661971\n",
      "Recall: 0.8951507208387942\n",
      "F1: 0.7193259610321222\n",
      "Skipped: 0\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7513397642015005\n",
      "Recall: 0.7123983739837398\n",
      "F1: 0.7313510693792383\n",
      "Skipped: 0\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8284313725490197\n",
      "Recall: 0.6282527881040892\n",
      "F1: 0.7145877378435518\n",
      "Skipped: 0\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8582375478927203\n",
      "Recall: 0.6081447963800904\n",
      "F1: 0.711864406779661\n",
      "Skipped: 0\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7415254237288136\n",
      "Recall: 0.720164609053498\n",
      "F1: 0.7306889352818372\n",
      "Skipped: 0\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.848180677540778\n",
      "Recall: 0.617351598173516\n",
      "F1: 0.7145877378435518\n",
      "Skipped: 0\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7298136645962733\n",
      "Recall: 0.7382198952879581\n",
      "F1: 0.7339927121290993\n",
      "Skipped: 0\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7173252279635258\n",
      "Recall: 0.7556029882604055\n",
      "F1: 0.735966735966736\n",
      "Skipped: 0\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7112676056338029\n",
      "Recall: 0.7610333692142088\n",
      "F1: 0.735309412376495\n",
      "Skipped: 0\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7526997840172787\n",
      "Recall: 0.7061803444782169\n",
      "F1: 0.7286983795086253\n",
      "Skipped: 0\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7643865363735071\n",
      "Recall: 0.7047047047047047\n",
      "F1: 0.7333333333333334\n",
      "Skipped: 0\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8231780167264038\n",
      "Recall: 0.6451310861423221\n",
      "F1: 0.7233595800524933\n",
      "Skipped: 0\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7822671156004489\n",
      "Recall: 0.6819960861056752\n",
      "F1: 0.7286983795086253\n",
      "Skipped: 0\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7274590163934426\n",
      "Recall: 0.7473684210526316\n",
      "F1: 0.7372793354101765\n",
      "Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever_adv{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8253768844221105\n",
      "Recall: 0.6100278551532033\n",
      "F1: 0.7015483182060865\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9275568181818182\n",
      "Recall: 0.5605150214592275\n",
      "F1: 0.6987693953986089\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7562642369020501\n",
      "Recall: 0.6626746506986028\n",
      "F1: 0.7063829787234043\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8538754764930114\n",
      "Recall: 0.6103542234332425\n",
      "F1: 0.7118644067796611\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8784313725490196\n",
      "Recall: 0.5983971504897596\n",
      "F1: 0.711864406779661\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7762237762237763\n",
      "Recall: 0.650390625\n",
      "F1: 0.7077577045696068\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8467243510506799\n",
      "Recall: 0.6272893772893773\n",
      "F1: 0.7206733298264071\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8681177976952625\n",
      "Recall: 0.6091644204851752\n",
      "F1: 0.7159450897571277\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7633333333333333\n",
      "Recall: 0.6849451645064806\n",
      "F1: 0.7220178665265371\n",
      "Skipped: 0\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8235294117647058\n",
      "Recall: 0.6417212347988774\n",
      "F1: 0.7213459516298634\n",
      "Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever_adv{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5278491859468724\n",
      "Recall: 0.9263157894736842\n",
      "F1: 0.6724890829694323\n",
      "Skipped: 0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:26<00:00, 45.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.523645743766122\n",
      "Recall: 0.9199395770392749\n",
      "F1: 0.6673972602739725\n",
      "Skipped: 0\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5274442538593482\n",
      "Recall: 0.924812030075188\n",
      "F1: 0.6717640633533589\n",
      "Skipped: 0\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.521404109589041\n",
      "Recall: 0.9269406392694064\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5222222222222223\n",
      "Recall: 0.9299847792998478\n",
      "F1: 0.668856048166393\n",
      "Skipped: 0\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5134907251264755\n",
      "Recall: 0.9530516431924883\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5165394402035624\n",
      "Recall: 0.9427244582043344\n",
      "F1: 0.6673972602739726\n",
      "Skipped: 0\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 44.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5186125211505922\n",
      "Recall: 0.9474497681607419\n",
      "F1: 0.6703116457080373\n",
      "Skipped: 0\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:27<00:00, 43.91it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5c0ab998521f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-a73c56bfcae5>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    test(model, dev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests with dev from COQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = 5\n",
    "checkpoint = torch.load(f'save_fever{epoch}')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dict = json.load(open('../data/coqa-dev-v1.0.json', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \n",
      "\n",
      "\"What are you doing, Cotton?!\" \n",
      "\n",
      "\"I only wanted to be more like you\". \n",
      "\n",
      "Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \n",
      "\n",
      "\"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" \n",
      "\n",
      "Then Cotton thought, \"I change my mind. I like being special\".\n"
     ]
    }
   ],
   "source": [
    "print(dev_dict['data'][0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = dev_dict['data'][0]['story'].replace('\\n\\n', '\\n').replace('\\n', '')\n",
    "\n",
    "text = f\"\"\"\n",
    "Evidence:\n",
    "{story}\n",
    "\n",
    "Claim:\n",
    "What color was Cotton? Blue.\n",
    "\n",
    "The evidence supports the claim:\n",
    "\"\"\"[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. But Cotton wasn't alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton's mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer's orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \"What are you doing, Cotton?!\" \"I only wanted to be more like you\". Cotton's mommy rubbed her face on Cotton's and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton's mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton's fur was all all dry. \"Don't ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn't want that!\" Then Cotton thought, \"I change my mind. I like being special\".\n",
      "\n",
      "Claim:\n",
      "What color was Cotton? Blue.\n",
      "\n",
      "The evidence supports the claim:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Yes.\\n\\n', 7.669204235076904)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_full_answer(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_data_item(item, max_num_questions=0, question_number=-1, last_question=True):\n",
    "    text = 'In the text below two people are discussing a story.\\n\\n'\n",
    "    text += 'Story:\\n' + item['story'] + '\\n\\n'\n",
    "    text += 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if not last_question:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "number = 0\n",
    "small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                     max_num_questions=8,\n",
    "                                     question_number=number,\n",
    "                                     last_question=True)\n",
    "last_question = small_text.split('\\n')[-1].replace('A: ', '')\n",
    "discussion_anchor = 'Discussion:\\n'\n",
    "discussion = small_text[small_text.find(discussion_anchor) + len(discussion_anchor):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What color was Cotton?\n",
      "A: white\n"
     ]
    }
   ],
   "source": [
    "print(discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "statement_model.cuda()\n",
    "checkpoint = torch.load('save_statement' + str(2))\n",
    "statement_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_prompt(item, max_num_questions=0, question_number=-1, use_answer=None):\n",
    "    text = 'Discussion:\\n'\n",
    "    text += '\\n'.join(['Q: ' + q['input_text'] \n",
    "                       + '\\nA: ' + a['input_text'] \n",
    "                       for q, a in zip(item['questions'][max(0,question_number-max_num_questions):question_number+1], \n",
    "                                       item['answers'][max(0,question_number-max_num_questions):question_number+1]) \n",
    "                      ])\n",
    "    if use_answer:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) + '\\n' + 'A: ' + use_answer + '\\n'\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statement_from_dialogue(statement_model, prompt):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 50\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length > 1024:\n",
    "        return ''\n",
    "    output = statement_model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    offset = len(prompt)\n",
    "    start = offset\n",
    "    end = output.find('\\n', start)\n",
    "    return output[start:end].split(':')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_from_data_item(item):\n",
    "    return item['story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = get_description_from_data_item(dev_dict['data'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_prompt = get_statement_prompt(dev_dict['data'][index], \n",
    "                         max_num_questions=5,\n",
    "                         question_number=number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: What color was Cotton?\n",
      "A: white\n",
      "Statement:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(statement_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claim_from_description_and_dialogue(description, dialogue):\n",
    "    if not dialogue:\n",
    "        return ''\n",
    "    if dialogue[-1] == '.':\n",
    "        dialogue = dialogue[:-1]    \n",
    "    text = 'Evidence:\\n'\n",
    "    text += description.replace('\\n\\n', '\\n') + '\\n\\n'\n",
    "    text += 'Claim:\\n'\n",
    "    text += dialogue + '\\n\\n'\n",
    "    text += 'The evidence supports the claim:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "text = create_claim_from_description_and_dialogue(description, statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evidence:\\nOnce upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a nice warm place above the barn where all of the farmer\\'s horses slept. But Cotton wasn\\'t alone in her little home above the barn, oh no. She shared her hay bed with her mommy and 5 other sisters. All of her sisters were cute and fluffy, like Cotton. But she was the only white one in the bunch. The rest of her sisters were all orange with beautiful white tiger stripes like Cotton\\'s mommy. Being different made Cotton quite sad. She often wished she looked like the rest of her family. So one day, when Cotton found a can of the old farmer\\'s orange paint, she used it to paint herself like them. When her mommy and sisters found her they started laughing. \\n\"What are you doing, Cotton?!\" \\n\"I only wanted to be more like you\". \\nCotton\\'s mommy rubbed her face on Cotton\\'s and said \"Oh Cotton, but your fur is so pretty and special, like you. We would never want you to be any other way\". And with that, Cotton\\'s mommy picked her up and dropped her into a big bucket of water. When Cotton came out she was herself again. Her sisters licked her face until Cotton\\'s fur was all all dry. \\n\"Don\\'t ever do that again, Cotton!\" they all cried. \"Next time you might mess up that pretty white fur of yours and we wouldn\\'t want that!\" \\nThen Cotton thought, \"I change my mind. I like being special\".\\n\\nClaim:\\nCotton was white\\n\\nThe evidence supports the claim:\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "sentence_model = sentence_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_text(text):\n",
    "    outputs = sentence_model.encode(text)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_answers(dev_dict, dev_index):\n",
    "    answers = [[item['input_text'] for item in dev_dict['data'][dev_index]['answers']]]\n",
    "    answers += [[item['input_text'] for item in dev_dict['data'][dev_index]['additional_answers'][str(index)]] for index in range(3)]\n",
    "    return [list(set([answers[j][i] for j in range(len(answers))])) for i in range(len(answers[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_of_model(model, dlist):\n",
    "    correct_predictions = []\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        all_answers = get_all_answers(dev_dict, index)\n",
    "        total_questions = len(all_answers)        \n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = get_text_from_data_item(dev_dict['data'][index], \n",
    "                                                 max_num_questions=8,\n",
    "                                                 question_number=number,\n",
    "                                                 last_question=True)\n",
    "            last_question = small_text.split('\\n')[-1].replace('A: ', '')\n",
    "            description = get_description_from_data_item(dev_dict['data'][index])\n",
    "            statement_prompt = get_statement_prompt(dev_dict['data'][index], \n",
    "                         max_num_questions=5,\n",
    "                         question_number=number)\n",
    "            statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "            text = create_claim_from_description_and_dialogue(description, statement)\n",
    "            try:\n",
    "                y_n_from_fever = generate_answer(model, text)\n",
    "            except RuntimeError:\n",
    "                continue\n",
    "            \n",
    "            label = last_question\n",
    "            \n",
    "\n",
    "            if y_n_from_fever == 'Y' or y_n_from_fever == 'N' and label[:2].lower() == 'no':\n",
    "                correct_answers += 1\n",
    "\n",
    "                correct_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'label': label,\n",
    "                })                    \n",
    "            else:\n",
    "                wrong_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'label': label,\n",
    "                })                 \n",
    "\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:54<00:00,  5.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285714285714286\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:49<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9235588972431078\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:39<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9223057644110275\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9273182957393483\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931077694235589\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9423558897243107\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:38<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398496240601504\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:41<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9461152882205514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "wrong_predictions = []\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    accuracy, wrong_prediction, _, _ = compute_accuracy_of_model(model, dev_list[:50])\n",
    "    accuracies.append(accuracy)\n",
    "    wrong_predictions.append(wrong_prediction)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'number': 10,\n",
       "  'statement': 'The other cats licked her face when Cotton emerged from the bucket of water.',\n",
       "  'label': 'licked her face'},\n",
       " {'index': 1,\n",
       "  'number': 8,\n",
       "  'statement': 'They did not know what the note was.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 2, 'number': 1, 'statement': 'Yes.', 'label': 'Yes'},\n",
       " {'index': 2, 'number': 3, 'statement': 'I know her.', 'label': 'Yes'},\n",
       " {'index': 4,\n",
       "  'number': 5,\n",
       "  'statement': 'Kendra does not want to miss story time.',\n",
       "  'label': 'story time'},\n",
       " {'index': 5,\n",
       "  'number': 5,\n",
       "  'statement': 'Arthur Kill and the Kill Van Kull separate New York from new jersey.',\n",
       "  'label': 'Arthur Kill and the Kill Van Kull'},\n",
       " {'index': 7, 'number': 17, 'statement': 'He is 50.', 'label': '50'},\n",
       " {'index': 10, 'number': 8, 'statement': 'Mayweather is 38.', 'label': '38'},\n",
       " {'index': 11,\n",
       "  'number': 7,\n",
       "  'statement': 'Frederick G. Kilgour is not currently enrolled at the University.',\n",
       "  'label': 'He is not'},\n",
       " {'index': 14,\n",
       "  'number': 16,\n",
       "  'statement': \"Franz didn't stay in touch because he assumed Hans was dead.\",\n",
       "  'label': 'He assumed Hans was dead.'},\n",
       " {'index': 16,\n",
       "  'number': 12,\n",
       "  'statement': 'Ted regretted not paying more attention to Spotty.',\n",
       "  'label': 'yes'},\n",
       " {'index': 17,\n",
       "  'number': 4,\n",
       "  'statement': \"The dog wasn't.\",\n",
       "  'label': \"He wasn't\"},\n",
       " {'index': 20,\n",
       "  'number': 7,\n",
       "  'statement': 'Jenny helped unpack.',\n",
       "  'label': 'yes'},\n",
       " {'index': 21,\n",
       "  'number': 4,\n",
       "  'statement': 'Natasha did not follow their instructions.',\n",
       "  'label': 'She did not.'},\n",
       " {'index': 21, 'number': 12, 'statement': 'No.', 'label': 'Yes.'},\n",
       " {'index': 21,\n",
       "  'number': 15,\n",
       "  'statement': 'Sonors are boys or girls.',\n",
       "  'label': 'Sons'},\n",
       " {'index': 22,\n",
       "  'number': 6,\n",
       "  'statement': 'Peter did not want to be tired.',\n",
       "  'label': 'tired'},\n",
       " {'index': 23, 'number': 1, 'statement': 'the dog is ugly.', 'label': 'false'},\n",
       " {'index': 24,\n",
       "  'number': 10,\n",
       "  'statement': 'No, she changed her opinion.',\n",
       "  'label': 'yes'},\n",
       " {'index': 25,\n",
       "  'number': 1,\n",
       "  'statement': \"Sir Earl should not get too close to Archie's traces.\",\n",
       "  'label': \"Archie's traces.\"},\n",
       " {'index': 26,\n",
       "  'number': 6,\n",
       "  'statement': 'No commanders were retained.',\n",
       "  'label': 'yes'},\n",
       " {'index': 26,\n",
       "  'number': 16,\n",
       "  'statement': 'Lugo has no children.',\n",
       "  'label': 'yes'},\n",
       " {'index': 28,\n",
       "  'number': 8,\n",
       "  'statement': 'London commuter belt is not a place.',\n",
       "  'label': 'London commuter belt'},\n",
       " {'index': 29,\n",
       "  'number': 16,\n",
       "  'statement': 'hilip did not ask him not to tell her husband something.',\n",
       "  'label': 'yes'},\n",
       " {'index': 31,\n",
       "  'number': 6,\n",
       "  'statement': 'Marsha lives in a plastic bag.',\n",
       "  'label': 'A plastic bag.'},\n",
       " {'index': 31,\n",
       "  'number': 7,\n",
       "  'statement': 'Marsha stays fresh.',\n",
       "  'label': 'Yes.'},\n",
       " {'index': 31,\n",
       "  'number': 8,\n",
       "  'statement': \"Marsha's mom told her to soak him in water every few days.\",\n",
       "  'label': \"Marsha's mom told her to soak him in water every few days.\"},\n",
       " {'index': 32,\n",
       "  'number': 1,\n",
       "  'statement': 'The only secular book Anne saw while at whitehall was an odd volume of Parthenissa.',\n",
       "  'label': 'an odd volume of Parthenissa'},\n",
       " {'index': 32,\n",
       "  'number': 14,\n",
       "  'statement': 'eter Bridgeman did not have any pursuit.',\n",
       "  'label': 'only ones that were to her advantage'},\n",
       " {'index': 33,\n",
       "  'number': 11,\n",
       "  'statement': \"William couldn't speak well because he was so full of wrath.\",\n",
       "  'label': 'he was so full of wrath he could not speak'},\n",
       " {'index': 34,\n",
       "  'number': 8,\n",
       "  'statement': 'Chip and cake and candy are avoided.',\n",
       "  'label': 'chips and cake and candy'},\n",
       " {'index': 35,\n",
       "  'number': 16,\n",
       "  'statement': 'Martin reads the manuscript to himself.',\n",
       "  'label': 'Martin'},\n",
       " {'index': 35,\n",
       "  'number': 19,\n",
       "  'statement': \"Martin believes Maria may not love it because it's too strong for the magazines.\",\n",
       "  'label': \"It's too strong for the magazines\"},\n",
       " {'index': 36,\n",
       "  'number': 4,\n",
       "  'statement': 'And Barcelona.',\n",
       "  'label': 'Barcelona'},\n",
       " {'index': 37,\n",
       "  'number': 5,\n",
       "  'statement': 'The low estimate of the money they were spending was five dollars per week.',\n",
       "  'label': 'five dollars per week'},\n",
       " {'index': 37,\n",
       "  'number': 13,\n",
       "  'statement': 'Doing that would be a bad thing.',\n",
       "  'label': 'yes'},\n",
       " {'index': 38,\n",
       "  'number': 1,\n",
       "  'statement': 'Ryan and Adam are trying to be NBA players.',\n",
       "  'label': 'NBA players'},\n",
       " {'index': 40,\n",
       "  'number': 6,\n",
       "  'statement': \"Josephine didn't turn up.\",\n",
       "  'label': 'Josephine'},\n",
       " {'index': 42,\n",
       "  'number': 11,\n",
       "  'statement': 'They find no sign that he has been there.',\n",
       "  'label': 'yes'},\n",
       " {'index': 43,\n",
       "  'number': 1,\n",
       "  'statement': 'Unknowable is the one word best described as unknown.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 44,\n",
       "  'number': 7,\n",
       "  'statement': 'Wnated liberty was not cooperating.',\n",
       "  'label': 'wnated liberty'},\n",
       " {'index': 44,\n",
       "  'number': 8,\n",
       "  'statement': 'Paul did not get it.',\n",
       "  'label': 'unknown'},\n",
       " {'index': 45,\n",
       "  'number': 4,\n",
       "  'statement': 'Bonn is in the northernmost zone.',\n",
       "  'label': 'southernmost'},\n",
       " {'index': 45,\n",
       "  'number': 16,\n",
       "  'statement': 'Bonn served as the seat of government, but not the capitol from 1990 to 1999.',\n",
       "  'label': 'Bonn served as the seat of government, but not the capitol'},\n",
       " {'index': 46,\n",
       "  'number': 9,\n",
       "  'statement': 'Lodi was not their only hope.',\n",
       "  'label': 'He was their only hope.'},\n",
       " {'index': 49,\n",
       "  'number': 11,\n",
       "  'statement': 'Harry did not sell the telegraph.',\n",
       "  'label': 'Harry'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRY WITHOUT ADDITIONAL NEGATIONS IN TRAINING DATA!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_list = json.load(open('../data/adversarial_claims_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the text below two people are discussing a story.\n",
      "\n",
      "Story:\n",
      "My doorbell rings. On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
      "\n",
      "I know this lady. It is not her first visit. She is the boy's grandmother, and her daughter bought the house next door last October. \n",
      "\n",
      "Her daughter, Nicole, speaks fluent English. But she is now in Shanghai, and her parents are here with the little boy. Nicole has obviously told her mother that I am having heart surgery soon, so her mother has decided I need more nutrients. \n",
      "\n",
      "I know what is inside the bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake. This has become an almost-daily practice. \n",
      "\n",
      "Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello. Once, she brought an iPad as well as the food. She pointed to the screen, which displayed a message from her daughter telling me that her mother wanted to know if the food was all right and whether it was too salty. I am not used to iPads, so she indicated I should go with her to her house. Then, she handed the iPad to her husband and almost immediately I found myself looking at Nicole in Shanghai and discussing her mother's cooking and salt intake. Instantly, tears welled in my eyes. \n",
      "\n",
      "\"Your mother just can't be bringing me meals like this all the time,\" I insisted. \"I can hardly do dishes in return.\" \n",
      "\n",
      "\"Oh, no, Lucy.\" Nicole said. \"Mum doesn't like western food. Don't worry about it; she has to cook for the three of them anyway, and she wants to do it.\" \n",
      "\n",
      "The doorbell keeps ringing and there is the familiar brown paper carrier bag, handed smilingly to me. \n",
      "\n",
      "I am now working on some more Chinese words--it's the least I can do after such display of kindness. \n",
      "\n",
      "\"Thank you\" is, of course, the first one. Somehow, it seems inadequate.\n",
      "\n",
      "Discussion:\n",
      "Q: How is she related to the boy?\n",
      "WA: He is his grandmother\n",
      "CA: mother.\n",
      "Q: What is in the bag?\n",
      "WA: rice, vegetables and either chicken, meat or shrimp\n",
      "CA: food.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(adversarial_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "_question_prompt = '\\nQ: '\n",
    "_correct_answer_prompt = '\\nCA: '\n",
    "_wrong_answer_prompt = '\\nWA: '\n",
    "    \n",
    "def get_text_up_to_question_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_answer_prompt, pos + 1)\n",
    "    return text[0:pos + 1]\n",
    "    \n",
    "def get_answers_number(text):\n",
    "    return text.count(_correct_answer_prompt)\n",
    "\n",
    "def get_correct_answer_number(text, number):\n",
    "    pos = text.find(_correct_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_correct_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_correct_answer_prompt))\n",
    "    return text[pos + len(_correct_answer_prompt):end]\n",
    "\n",
    "def get_wrong_answer_number(text, number):\n",
    "    pos = text.find(_wrong_answer_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_wrong_answer_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_wrong_answer_prompt))\n",
    "    return text[pos + len(_wrong_answer_prompt):end]\n",
    "\n",
    "\n",
    "def get_question_number(text, number):\n",
    "    pos = text.find(_question_prompt)\n",
    "    for _ in range(number):\n",
    "        pos = text.find(_question_prompt, pos + 1)\n",
    "    end = text.find('\\n', pos + len(_question_prompt))\n",
    "    return text[pos + len(_question_prompt):end]\n",
    "\n",
    "def get_description_from_text(text):\n",
    "    start_prompt = 'Story:'\n",
    "    end_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):text.find(end_prompt)]\n",
    "\n",
    "def get_discussion_from_text(text):\n",
    "    start_prompt = 'Discussion:'\n",
    "    return text[text.find(start_prompt) + len(start_prompt):].strip()\n",
    "\n",
    "def get_statement_prompt_from_text(full_text, number, max_questions=5):\n",
    "    text = 'Discussion:\\n'\n",
    "    questions_and_answers_list = get_discussion_from_text(full_text).split('\\n')\n",
    "    start = max(0, (number + 1 - max_questions) * 3)\n",
    "    end = (number + 1) * 3\n",
    "    questions_and_answers_list = questions_and_answers_list[start:end]\n",
    "    text += '\\n'.join(questions_and_answers_list)\n",
    "    text += '\\nStatement:\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q: Whose paint was it?\\nWA: Cotton's mommy\\nCA: the farmer.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_discussion_from_text(adversarial_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion:\n",
      "Q: How is she related to the boy?\n",
      "WA: He is his grandmother\n",
      "CA: mother.\n",
      "Q: What is in the bag?\n",
      "WA: rice, vegetables and either chicken, meat or shrimp\n",
      "CA: food.\n",
      "Statement:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_statement_prompt_from_text(adversarial_list[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the farmer.\n"
     ]
    }
   ],
   "source": [
    "print(get_correct_answer_number(adversarial_list[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotton's mommy\n"
     ]
    }
   ],
   "source": [
    "print(get_wrong_answer_number(adversarial_list[0], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4870"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adversarial_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([get_answers_number(item) for item in adversarial_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answers_number(adversarial_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_claim_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_y_n_answers(model, prompt, num_replicas=25):\n",
    "    model.train()\n",
    "    outputs_count = {}\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        tokens = tokens.repeat(num_replicas,1)\n",
    "        _length = 50\n",
    "        tokens_length = tokens.shape[1]\n",
    "        if tokens_length + _length > 1024:\n",
    "            return ''\n",
    "\n",
    "        \n",
    "        output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length,\n",
    "             pad_token_id=50256\n",
    "        )\n",
    "        for index in range(num_replicas):\n",
    "            text = tokenizer.decode(output[index, :], skip_special_tokens=True)\n",
    "            answer = get_answer_from_text(text)\n",
    "            outputs_count.setdefault(answer, 0)\n",
    "            outputs_count[answer] += 1\n",
    "\n",
    "    total = sum(v for v in outputs_count.values())\n",
    "    return [(k, v / total) for k, v in outputs_count.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_anchor = 'Discussion:\\n'\n",
    "\n",
    "def compute_precision_of_model(model, dlist):\n",
    "    correct_predictions = []\n",
    "    total_number_of_questions = 0\n",
    "    correct_answers = 0\n",
    "    wrong_predictions = []\n",
    "\n",
    "    false_positives = []\n",
    "    for index, text in tqdm(enumerate(dlist), total=len(dlist)):\n",
    "\n",
    "        total_questions = get_answers_number(text)\n",
    "        \n",
    "        for number in range(total_questions):\n",
    "            small_text = text\n",
    "            description = get_description_from_text(text)\n",
    "            statement_prompt = get_statement_prompt_from_text(text, number)\n",
    "            statement = generate_statement_from_dialogue(statement_model, statement_prompt)\n",
    "            claim = create_claim_from_description_and_dialogue(description, statement)\n",
    "            #try:\n",
    "            y_n_with_score = generate_multiple_y_n_answers(model, claim)\n",
    "            #except RuntimeError:\n",
    "            #    continue\n",
    "            \n",
    "\n",
    "            #if y_n_with_score[0][0] == 'N':\n",
    "            if len(y_n_with_score) == 2:\n",
    "                correct_answers += 1\n",
    "\n",
    "                correct_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'yn': y_n_with_score,\n",
    "                })\n",
    "            else:\n",
    "                wrong_predictions.append({\n",
    "                    'index': index,\n",
    "                    'number': number,\n",
    "                    'statement': statement,\n",
    "                    'yn': y_n_with_score,\n",
    "                })\n",
    "\n",
    "\n",
    "            total_number_of_questions += 1\n",
    "    print(total_number_of_questions)\n",
    "    return correct_answers/total_number_of_questions, wrong_predictions, false_positives, correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.03508771929824561\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:07<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.08771929824561403\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.12280701754385964\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.14035087719298245\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.10526315789473684\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.03508771929824561\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.10526315789473684\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [01:08<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "0.07017543859649122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "wrong_predictions = []\n",
    "for epoch in range(0, num_epochs):\n",
    "    checkpoint = torch.load(f'save_fever{epoch}')\n",
    "    config = GPT2Config(attn_pdrop=0.01, resid_pdrop=0.01, embd_pdrop=0.01)\n",
    "    model = GPT2LMHeadModel(config).from_pretrained('gpt2').cuda()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    _ = model.eval()\n",
    "    print(f'Epoch {epoch}')\n",
    "    accuracy, wrong_prediction, _, _ = compute_precision_of_model(model, adversarial_list)\n",
    "    accuracies.append(accuracy)\n",
    "    wrong_predictions.append(wrong_prediction)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'number': 0,\n",
       "  'statement': 'The library for history, law, philosophy, science and theology. is the library for.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 0,\n",
       "  'number': 1,\n",
       "  'statement': '150,000 books survived the Pre Lateran period.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 0,\n",
       "  'statement': 'Venters called Lassiter a dark bay.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 1,\n",
       "  'statement': 'The man who had led Milly Erne to Cottonwoods was oppressing her.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 2,\n",
       "  'number': 2,\n",
       "  'statement': 'Venters was hoping she could keep the need of a helper from happening to her.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 3,\n",
       "  'number': 0,\n",
       "  'statement': 'He saves the Abominable Snow Monster during a snow storm.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 4,\n",
       "  'number': 0,\n",
       "  'statement': 'Francesco noticed it.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 4,\n",
       "  'number': 1,\n",
       "  'statement': \"The garrison was in Francesco's group.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 6,\n",
       "  'number': 0,\n",
       "  'statement': 'BY WHET THEY IDENTIFY that BUS, they knowingly participated in the planned hazing activity over several years.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 6,\n",
       "  'number': 1,\n",
       "  'statement': 'Their MAIN FOCUS is to fix it.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 7,\n",
       "  'number': 0,\n",
       "  'statement': 'The driver should have called the bus driver.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 0,\n",
       "  'statement': 'It began in the early 1990s.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 1,\n",
       "  'statement': 'It began easier to compare prices in online.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 2,\n",
       "  'statement': \"A reason online is cheaper is it's easier to compare prices.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 8,\n",
       "  'number': 3,\n",
       "  'statement': \"Because they don't offer a service like we do. Why is it easier to compare prices.\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 0,\n",
       "  'statement': 'The energy source for an incandescent bulb is electric current.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 1,\n",
       "  'statement': 'In a halogen bulb, by feeding-through terminals or wires embedded in the glass.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 10,\n",
       "  'number': 2,\n",
       "  'statement': 'They convert 16 lumens per watt to light.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 11,\n",
       "  'number': 0,\n",
       "  'statement': 'We would have diagnosed him with today by diagnosing him with learning to read and write.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 12,\n",
       "  'number': 0,\n",
       "  'statement': 'The this period began when the beginning of farming.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 13,\n",
       "  'number': 0,\n",
       "  'statement': 'People might have kept quail in captivity to feed them. Because they are to feed them.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 13,\n",
       "  'number': 2,\n",
       "  'statement': 'The term poultry is defined as small animal.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 15,\n",
       "  'number': 0,\n",
       "  'statement': 'This divorce fits a certain trope.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 16,\n",
       "  'number': 0,\n",
       "  'statement': 'L into the pool.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 16,\n",
       "  'number': 1,\n",
       "  'statement': 'He got closer to the nk of the pool.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 18,\n",
       "  'number': 0,\n",
       "  'statement': 'Its values are mirrored by a culture.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 18,\n",
       "  'number': 1,\n",
       "  'statement': 'He thought it was sudden glory arising from sudden thought of feeling far better than others.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 19,\n",
       "  'number': 0,\n",
       "  'statement': 'A child was beat.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 19,\n",
       "  'number': 1,\n",
       "  'statement': \"His condition was `` severely beaten '' with `` fists and feet. ''. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 21,\n",
       "  'number': 0,\n",
       "  'statement': 'About that and that and that.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 21,\n",
       "  'number': 1,\n",
       "  'statement': 'It made the giant feel like he was asking me about the beans and their roots.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 22,\n",
       "  'number': 0,\n",
       "  'statement': 'A specific identy emerged in the Celtic Britons.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 23,\n",
       "  'number': 0,\n",
       "  'statement': 'The first form of brackets in written English was a bracket.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 24,\n",
       "  'number': 0,\n",
       "  'statement': 'The recovery helped.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 24,\n",
       "  'number': 1,\n",
       "  'statement': 'The recovery helped to get it introduced.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 25,\n",
       "  'number': 0,\n",
       "  'statement': 'Lilly drew a picture of her best friend Lilly.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 25,\n",
       "  'number': 1,\n",
       "  'statement': 'Lilly drew this time by her mother.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 26,\n",
       "  'number': 1,\n",
       "  'statement': \"Rochester got the `` Places Rated Almanac '' in 2007. ''s. Rated Almanac. '' in 2007. ''s. was. '' Places Rated Almanac. '' in 2007. ''s. was. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 27,\n",
       "  'number': 0,\n",
       "  'statement': 'There have been 17,400 people in the last few years.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 29,\n",
       "  'number': 0,\n",
       "  'statement': \"The use of the word `` water '' on is used to help people who fall into the water. '' is used on to help people who fall into the water. '' is used on to help people who fall into the water. '' is used o\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 30,\n",
       "  'number': 1,\n",
       "  'statement': 'If he makes a mess in his diaper, he is worried.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 30,\n",
       "  'number': 2,\n",
       "  'statement': 'They got a rattle.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 31,\n",
       "  'number': 0,\n",
       "  'statement': \"In the beaver's expression when he answered, `` Well, now that you have found out that you cann't do that, you are going to do that. '' was in the beaver's expression when he answered. ''\",\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 32,\n",
       "  'number': 0,\n",
       "  'statement': 'He made it to the Porta Pinti today.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 33,\n",
       "  'number': 0,\n",
       "  'statement': 'For three hours, the game is played for three hours.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 34,\n",
       "  'number': 0,\n",
       "  'statement': 'He was searching vainly.',\n",
       "  'yn': [('Y', 1.0)]},\n",
       " {'index': 34,\n",
       "  'number': 1,\n",
       "  'statement': \"D'Arbino went to seek help from Count Fabio d'Ascoli.\",\n",
       "  'yn': [('Y', 1.0)]}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/fractalego/fact-checker/commit/a3185c8c177d8866908ea46c6b40abe9c7afddcb'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "epoch = 5\n",
    "checkpoint = torch.load(f'save_fever{epoch}')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.push_to_hub(\"fractalego/fact-checking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/fractalego/fact-checker/commit/ef06b4530a000f7671efed80a4440e752f2da351'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.push_to_hub(\"fractalego/fact-checking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
