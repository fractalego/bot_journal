{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_texts = json.load(open('../data/supporting.json'))\n",
    "refuting_texts = json.load(open('../data/refuting.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "He composed the scores to the television series Lost, Alias and Fringe, the video ' game series Medal of Honor and Call of Duty and many films such as The Incredibles, Ratatouille, Up, Mission: Impossible - Ghost Protocol, Dawn of the Planet of the Apes, Jurassic World, Inside Out, Star Trek Beyond, Doctor Strange, Rogue One and Spider-Man: Homecoming.\n",
      "\n",
      "Claim:\n",
      "Michael Giacchino composed the score for Doctor Strange.\n"
     ]
    }
   ],
   "source": [
    "print(supporting_texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "split = 0.8\n",
    "\n",
    "_prompt = '\\n\\nThe evidence supports the claim:\\n'\n",
    "all_list = [item + _prompt + 'Y' for item in supporting_texts]\n",
    "all_list += [item + _prompt + 'N' for item in refuting_texts]\n",
    "random.shuffle(all_list)\n",
    "train_list = all_list[:int(len(all_list) * split)]\n",
    "dev_list = all_list[int(len(all_list) * split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(train_list, open('../data/train_list.json', 'w'))\n",
    "#json.dump(dev_list, open('../data/dev_list.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = json.load(open('../data/train_list.json'))\n",
    "dev_list = json.load(open('../data/dev_list.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence:\n",
      "Shot in a mockumentary format, it follows the contestants in a beauty pageant called the Sarah Rose Cosmetics Mount Rose American Teen Princess Pageant, held in the small fictional town of Mount Rose, Minnesota, in which various contestants die in suspicious ways.\n",
      "\n",
      "Claim:\n",
      "Drop Dead Gorgeous follows the contestants in the Sarah Rose Cosmetics Mount Rose American Child Princess Pageant.\n",
      "\n",
      "The evidence supports the claim:\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "print(train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def batchify(data, n):\n",
    "    len_dict = {}\n",
    "    for item in data:\n",
    "        length = item.shape[1]\n",
    "        try:\n",
    "            len_dict[length].append(item)\n",
    "        except:\n",
    "            len_dict[length] = [item]\n",
    "\n",
    "    batch_chunks = []\n",
    "    for k in len_dict.keys():\n",
    "        vectors = len_dict[k]\n",
    "        batch_chunks += chunks(vectors, n)\n",
    "\n",
    "    batches = []\n",
    "    for chunk in batch_chunks:\n",
    "        inputs = torch.stack([item[0] for item in chunk])\n",
    "        batches.append((inputs))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_limit = 1024\n",
    "data = []\n",
    "total_skipped = 0\n",
    "for item in train_list:\n",
    "    tokens = tokenizer.encode(item, return_tensors='pt')\n",
    "    if tokens.shape[1] > _limit:\n",
    "        total_skipped += 1\n",
    "        continue\n",
    "    data.append(tokens)\n",
    "print(f'Skipped {total_skipped} out of {len(train_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = batchify(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_model, batches, optimizer, criterion):\n",
    "    total_loss = 0.\n",
    "    for i, batch in tqdm(enumerate(batches), total=len(batches)):\n",
    "        model.train()\n",
    "        inputs = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_model(inputs.cuda(), labels=inputs.cuda())[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(train_model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "random.shuffle(train_batches)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.8)\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_batches)\n",
    "    loss = train(model, train_batches, optimizer, criterion)z\n",
    "    #test(model, dev_list[:2000])\n",
    "    print('Epoch:', epoch, 'Loss:', loss)\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict()},\n",
    "                'save_fever' + str(epoch))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.cuda()\n",
    "checkpoint = torch.load('save_fever2')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import traceback\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    for item in tqdm(data):\n",
    "        expected = get_answer_from_text(item)\n",
    "        predicted = ''\n",
    "        try:\n",
    "            predicted = generate_answer(model, item)\n",
    "        except (IndexError, RuntimeError) as e:\n",
    "            print(str(e))\n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            print(repr(traceback.extract_tb(exc_traceback)))\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        if expected == predicted:\n",
    "            tp += 1\n",
    "        if expected == 'N' and predicted == 'Y':\n",
    "            fp += 1\n",
    "        if expected == 'Y' and predicted == 'N':\n",
    "            fn += 1\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', precision)\n",
    "    print('F1:', precision)\n",
    "    print('Skipped:', skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_up_to_question(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    return text[:text.find(_claim_yn) + len(_claim_yn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_text(text):\n",
    "    _claim_yn = 'The evidence supports the claim:\\n'\n",
    "    pos = text.find(_claim_yn) + len(_claim_yn)\n",
    "    return text[pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, text):\n",
    "    prompt = get_text_up_to_question(text)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    _length = 1\n",
    "    tokens_length = tokens.shape[1]\n",
    "    if tokens_length + _length >= 1024:\n",
    "        raise RuntimeError('Text is longer than 1024')\n",
    "    output = model.generate(\n",
    "             tokens.cuda(),\n",
    "             max_length=tokens_length + _length, \n",
    "             pad_token_id=50256\n",
    "    )\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return get_answer_from_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer_from_text(dev_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(model, dev_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 102/19048 [00:01<04:20, 72.79it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1528 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 118/19048 [00:01<04:30, 70.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 407/19048 [00:05<04:24, 70.43it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1593 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  2%|▏         | 424/19048 [00:05<04:14, 73.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 741/19048 [00:10<04:28, 68.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1194 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  4%|▍         | 750/19048 [00:10<04:14, 71.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1813/19048 [00:25<03:52, 74.01it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 10%|▉         | 1830/19048 [00:26<03:51, 74.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2697/19048 [00:38<03:57, 68.89it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 14%|█▍        | 2706/19048 [00:39<03:46, 72.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2925/19048 [00:42<03:49, 70.28it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 15%|█▌        | 2941/19048 [00:42<03:46, 70.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3372/19048 [00:48<03:54, 66.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3966 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 18%|█▊        | 3388/19048 [00:48<03:44, 69.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4160/19048 [00:59<03:21, 74.06it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1400 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 22%|██▏       | 4177/19048 [01:00<03:18, 74.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 4696/19048 [01:07<03:32, 67.46it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 25%|██▍       | 4705/19048 [01:07<03:20, 71.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5478/19048 [01:18<03:12, 70.51it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 29%|██▉       | 5486/19048 [01:18<03:09, 71.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6305/19048 [01:30<03:07, 68.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1288 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 33%|███▎      | 6322/19048 [01:30<02:52, 73.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 6623/19048 [01:34<03:06, 66.50it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2306 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 35%|███▍      | 6640/19048 [01:35<02:57, 69.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 6815/19048 [01:37<02:39, 76.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 36%|███▌      | 6831/19048 [01:37<02:37, 77.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 7238/19048 [01:43<02:37, 74.80it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 38%|███▊      | 7255/19048 [01:43<02:35, 75.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 7286/19048 [01:44<03:03, 63.98it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 38%|███▊      | 7304/19048 [01:44<02:44, 71.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7807/19048 [01:51<02:36, 72.00it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2348 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 41%|████      | 7824/19048 [01:51<02:28, 75.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 8289/19048 [01:58<02:35, 69.27it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 44%|████▎     | 8304/19048 [01:58<02:33, 70.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 10291/19048 [02:27<01:56, 75.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1198 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 54%|█████▍    | 10307/19048 [02:27<02:03, 70.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 10364/19048 [02:28<02:07, 68.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 54%|█████▍    | 10373/19048 [02:28<02:00, 71.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 10517/19048 [02:30<02:00, 70.52it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1727 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 55%|█████▌    | 10533/19048 [02:30<01:56, 73.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 10986/19048 [02:37<02:14, 59.77it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1929 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 58%|█████▊    | 11001/19048 [02:37<02:02, 65.75it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n",
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 11405/19048 [02:43<01:52, 67.79it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1604 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 60%|█████▉    | 11420/19048 [02:44<01:53, 67.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11532/19048 [02:45<01:58, 63.66it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 61%|██████    | 11546/19048 [02:46<02:02, 61.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 12572/19048 [03:01<01:42, 63.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1142 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 66%|██████▌   | 12589/19048 [03:01<01:34, 68.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 12998/19048 [03:07<01:31, 66.39it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 68%|██████▊   | 13014/19048 [03:07<01:28, 67.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 13418/19048 [03:13<01:21, 69.32it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 71%|███████   | 13436/19048 [03:13<01:14, 75.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 13496/19048 [03:14<01:31, 60.98it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1461 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 71%|███████   | 13512/19048 [03:15<01:21, 67.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13656/19048 [03:17<01:16, 70.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1193 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 72%|███████▏  | 13672/19048 [03:17<01:13, 72.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13759/19048 [03:18<01:23, 63.08it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 72%|███████▏  | 13774/19048 [03:18<01:21, 64.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 14109/19048 [03:23<01:10, 69.90it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 74%|███████▍  | 14126/19048 [03:23<01:05, 74.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 14615/19048 [03:30<01:03, 69.51it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 77%|███████▋  | 14631/19048 [03:31<01:01, 71.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14840/19048 [03:34<01:00, 69.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1954 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 78%|███████▊  | 14856/19048 [03:34<01:02, 67.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 15483/19048 [03:43<00:49, 71.83it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 81%|████████▏ | 15499/19048 [03:43<00:48, 73.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 15549/19048 [03:44<00:47, 73.41it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 82%|████████▏ | 15565/19048 [03:44<00:48, 71.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 15672/19048 [03:45<00:44, 76.07it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2769 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 82%|████████▏ | 15680/19048 [03:46<00:45, 74.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15843/19048 [03:48<00:47, 67.40it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1385 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 83%|████████▎ | 15859/19048 [03:48<00:45, 69.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 16474/19048 [03:57<00:38, 66.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 16490/19048 [03:58<00:37, 68.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 16570/19048 [03:59<00:40, 60.95it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 16584/19048 [03:59<00:40, 60.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16750/19048 [04:02<00:34, 66.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2016 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 88%|████████▊ | 16764/19048 [04:02<00:36, 62.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 17054/19048 [04:06<00:28, 70.01it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1548 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 90%|████████▉ | 17062/19048 [04:06<00:28, 69.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18224/19048 [04:23<00:12, 65.64it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 96%|█████████▌| 18241/19048 [04:23<00:11, 71.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18302/19048 [04:24<00:11, 67.68it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1306 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 96%|█████████▌| 18318/19048 [04:24<00:10, 71.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 18436/19048 [04:26<00:08, 72.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2182 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 97%|█████████▋| 18452/19048 [04:26<00:08, 72.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 18908/19048 [04:32<00:02, 69.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1484 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 99%|█████████▉| 18926/19048 [04:33<00:01, 74.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is longer than 1024\n",
      "[<FrameSummary file <ipython-input-97-ccfa008fa64c>, line 16 in test>, <FrameSummary file <ipython-input-100-fe745795e8be>, line 7 in generate_answer>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19048/19048 [04:35<00:00, 69.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.962508115126596\n",
      "Recall: 0.962508115126596\n",
      "F1: 0.962508115126596\n",
      "Skipped: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 2:\n",
    "Precision: 0.962508115126596\n",
    "Recall: 0.962508115126596\n",
    "F1: 0.962508115126596\n",
    "Skipped: 45\n",
    "\n",
    "Epoch 3:\n",
    "Precision: 0.9684621220331635\n",
    "Recall: 0.9684621220331635\n",
    "F1: 0.9684621220331635\n",
    "Skipped: 45\n",
    "\n",
    "Epoch 4:\n",
    "Precision: 0.967658052982285\n",
    "Recall: 0.967658052982285\n",
    "F1: 0.967658052982285\n",
    "Skipped: 45\n",
    "\n",
    "\n",
    "Epoch 5:\n",
    "Precision: 0.9523809523809523\n",
    "Recall: 0.9523809523809523\n",
    "F1: 0.9523809523809523\n",
    "Skipped: 18939\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.mnightly-2021-02-12-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:mnightly-2021-02-12-debian-10-test"
  },
  "kernelspec": {
   "display_name": "dispatcher",
   "language": "python",
   "name": "dispatcher"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
